[{"content":"","date":"17 November 2025","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"NAF AutoCon4 - Austin, TX","type":"talks"},{"content":"","date":"17 November 2025","externalUrl":null,"permalink":"/talks/","section":"Talks \u0026 Media","summary":"","title":"Talks \u0026 Media","type":"talks"},{"content":"","date":"17 November 2025","externalUrl":null,"permalink":"/","section":"William Collins","summary":"","title":"William Collins","type":"page"},{"content":"","date":"23 October 2025","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"Megaport Connect Summit","type":"talks"},{"content":"","date":"30 September 2025","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"AI for Network Leaders Summit - New York, NY","type":"talks"},{"content":"","date":"18 November 2024","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"NAF AutoCon2 - Denver, CO","type":"talks"},{"content":"","date":"15 November 2024","externalUrl":null,"permalink":"/categories/ai/","section":"Categories","summary":"","title":"Ai","type":"categories"},{"content":"As AI dominates tech headlines and corporate strategies in 2024, an important distinction is being blurred - the difference between developing AI versus consuming AI services. This mischaracterization risks confusing the market and overselling capabilities, but that isn\u0026rsquo;t anything new right? Take zero-trust, cloud computing, or even take a look back at the early-2000s with the web revolution. Urs Baumann tossed out a great questions in the Network Automation Forum - Slack recently, and I thought it would make for a good blog (or maybe venting session depending on how you look at it). Remember, these are opinions.\nIntro The Big Question # Question I think this is an excellent question. The moment Urs posted this in the chat, it made me think about medicine. There\u0026rsquo;s a clear difference between researchers that are advancing medical science and those practicing medicine. A family doctor doesn\u0026rsquo;t claim to be \u0026ldquo;doing pharmaceutical research\u0026rdquo; when prescribing medication. Let\u0026rsquo;s dig down a little further.\nBuilding AI - (A Small Crowd) # Core AI development represents a deeply interconnected ecosystem where each component builds upon and enables others. This is the story of our lives in tech, right? Training large language models requires novel neural architectures, which themselves emerge from fundamental machine learning research. This research, in turn, depends on insights gained from model training attempts and optimization techniques. The optimization methods then get refined. If you are doing AI, then you probably fall into one of the four cycles:\nFoundational Models \u0026amp; Training Neural Architecture Development Machine Learning Research Optimization Techniques What does this mean? If I had to summarize this, I would say: If you are consuming APIs or even fine-tuning models, you are most likely (not always) operating independently of the aforementioned core development cycles. This means you are consuming AI services. Consider the evolution of web services: Many companies claiming to \u0026ldquo;do web services\u0026rdquo; were typically implementing standardized protocols that were developed and maintained by a group of core contributors. While not a perfect 1:1 example, the same principles apply.\nImplementing AI - (Most of Us) # Most companies today use databases without developing database engines, or build on the public cloud services without building data centers. What about AI then? Isn\u0026rsquo;t it the same thing to say, companies are implementing AI without developing the underlying models or architectures? BINGO! What I\u0026rsquo;m seeing in the market today is a layered approach of implementation that looks something like:\nBasic API integration of existing LLMs Domain-specific apps (content generation / data analysis) RAG or fine-tuning pre-trained models So, you have a ChatBot! ChatBots, document analysis, data processing, content generation, and even code assistance are all, in my opinion, examples of AI consumption. These implementations mirror other technological adoptions throughout computing history (the above database example).\nRecognizing The Critical Nature of This Distinction # The distinction between AI development and AI implementation isn\u0026rsquo;t merely academic - the implications bleed into the industry, investors, and end-users. When tech companies for example, blur this line in their marketing, it creates unrealistic expectations about their capabilities. The consequences of this across sales, product, and engineering teams are self-evident.\nThe Feature VS. Product Debate # Another opinion I hold strongly is, most implementations of AI today are features rather than products. They enhance existing solutions rather than create fundamentally new ones. When AI serves as an enhancement to existing software, it\u0026rsquo;s clearly a feature. An example might be, building a ChatBot that is trained on product documentation, support tickets, and some measure of product data. This would be a feature that ultimately enhances customer experience and provides new ways to interact with and use a given product.\nLooking Forward # As AI technology matures, it makes sense that we would see further separation between core development and implementation. What category do you fall into? There\u0026rsquo;s no shame in brining value to your customers by being an excellent AI implementer - but accuracy in how we describe our relationship with AI technology matters. Moreover, being honest about how we describe and market the products we build, is becoming increasingly important and will serve to build integrity and trust. In conclusion, let\u0026rsquo;s celebrate both the innovators advancing core AI technology and those skillfully implementing it to solve real-world problems. But let\u0026rsquo;s also maintain clarity about the distinction between these roles.\n","date":"15 November 2024","externalUrl":null,"permalink":"/posts/2024/are-you-building-ai-or-just-using-it/","section":"Posts","summary":"As AI dominates tech headlines and corporate strategies in 2024, an important distinction is being blurred - the difference between developing AI versus consuming AI services. This mischaracterization risks confusing the market and overselling capabilities, but that isn’t anything new right? Take zero-trust, cloud computing, or even take a look back at the early-2000s with the web revolution. Urs Baumann tossed out a great questions in the Network Automation Forum - Slack recently, and I thought it would make for a good blog (or maybe venting session depending on how you look at it). Remember, these are opinions.\n","title":"Are You Building AI Or Just Using It","type":"posts"},{"content":"","date":"15 November 2024","externalUrl":null,"permalink":"/categories/business/","section":"Categories","summary":"","title":"Business","type":"categories"},{"content":"","date":"15 November 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"15 November 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"Llm","type":"tags"},{"content":"","date":"15 November 2024","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine-Learning","type":"tags"},{"content":"","date":"15 November 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"15 November 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" ","date":"6 November 2024","externalUrl":null,"permalink":"/talks/2024/day-two-devops-256/","section":"Talks \u0026 Media","summary":" ","title":"PacketPushers Day Two DevOps: 256 - Alkira's Universal Transit","type":"talks"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"ONUG Fall - New York, NY","type":"talks"},{"content":"","date":"1 October 2024","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"(VA)NUG Inaugural Keynote - Northern Virginia","type":"talks"},{"content":"","date":"5 September 2024","externalUrl":null,"permalink":"/categories/networking/","section":"Categories","summary":"","title":"Networking","type":"categories"},{"content":"","date":"5 September 2024","externalUrl":null,"permalink":"/tags/nvidia/","section":"Tags","summary":"","title":"Nvidia","type":"tags"},{"content":"","date":"5 September 2024","externalUrl":null,"permalink":"/tags/rdma/","section":"Tags","summary":"","title":"Rdma","type":"tags"},{"content":"","date":"5 September 2024","externalUrl":null,"permalink":"/tags/tesla/","section":"Tags","summary":"","title":"Tesla","type":"tags"},{"content":"The foundation of high-performance computing that powers artificial intelligence lies not just in powerful GPUs, but in the intricate web of connections between them. As models grow exponentially in size and complexity, the networking infrastructure that facilitates their training has become a critical bottleneck. This has sparked a fascinating race to develop networking solutions tailored for AI-centric workloads, with tech giants like NVIDIA who are on an infrastructure trailblazing marathon.\nIn big tech, unique requirements drive cool (and sometimes custom) innovations. Never one to follow the crowd, Tesla has developed its own networking protocol tailored for AI workloads: TTPoE (Tesla Transport Protocol over Ethernet). As presented at Hot Chips 2024, TTPoE takes a fundamentally different approach to the high-performance networking problem. You can find the source material slides here.\nIntro The Lossless Imperative # At the heart of this networking revolution is the concept of lossless vs. lossy networks. Traditional Ethernet networks are lossy by nature - packets can be dropped when congestion occurs. While this is acceptable for many applications, AI workloads are exceptionally sensitive to data loss. If a datagram is dropped, GPUs may become idle while waiting to resynchronize. Moreover, GPUs need to be synchronized at the start and end of a run. When a run ends, the processed data often needs to be distributed across GPUs.\nThis is where lossless networking protocols like InfiniBand and RoCE (RDMA over Converged Ethernet) enter the picture. These technologies implement flow control mechanisms to prevent packet loss, ensuring reliable data transmission even under heavy load. However, achieving lossless behavior comes with its own set of trade-offs, including increased complexity and potential for congestion spreading.\nOptimizing for Efficiency + ROI Can you imagine running \u0026gt; billion dollars worth of GPUs and having synchronization issues stemming from packet loss on the network? Talk about a gigantic waste of computational resources. If you are training at this scale, network efficiency becomes massively critical. Ethernet is inherently lossy. Atop Ethernet, protocols like IP and TCP/IP introduce additional layers where loss can occur (think buffering, queues, etc.)\nNVIDIA\u0026rsquo;s Mellanox Masterstroke # In 2020, NVIDIA completed its $7 billion acquisition of Mellanox, a move that seemed puzzling to some at the time. In retrospect, it was a stroke of genius. Mellanox was the leading provider of InfiniBand and high-performance Ethernet solutions - precisely the technologies needed to unlock the full potential of NVIDIA\u0026rsquo;s GPUs for AI workloads.\nThis acquisition gave NVIDIA end-to-end control over the AI computing stack, from GPUs to networking fabric. It allowed them to optimize the entire pipeline, resulting in solutions like the NVIDIA DGX SuperPOD that can scale to thousands of GPUs with minimal performance loss. The foresight demonstrated in this acquisition has positioned NVIDIA as the dominant force in AI infrastructure. If you want to hear the whole story straight from Jensen Huang, check out this episode of the Acquired Podcast.\nRoCE: Bridging Worlds # While InfiniBand offers unparalleled performance for all your AI workload needs, its proprietary nature and specialized hardware requirements can be a barrier for many organizations. Enter RoCE (RDMA over Converged Ethernet), which aims to bring the benefits of RDMA (Remote Direct Memory Access) to standard Ethernet networks.\nRoCE implements many of the same lossless principles as InfiniBand but does so over familiar Ethernet infrastructure. This offers a compelling middle ground - performance approaching that of InfiniBand, but with greater flexibility and potential cost savings. However, achieving truly lossless behavior with RoCE requires careful network configuration and can be challenging to scale in large deployments.\nThe Middle Ground Wait, your whole business model doesn\u0026rsquo;t revolve around AI/ML? That sounds like most of the companies out there! This means you are probably seeking a middle ground. This is where RoCE comes into play. Since just about every organization out there already runs on Ethernet powered infrastructure, RoCE offers a path forward without the necessity to completely overhaul your existing network.\nSo it doesn\u0026rsquo;t match InfiniBand\u0026rsquo;s peak performance? If the cost saving is substantial, the deployment familiar, and RoCE and traditional ethernet can coexist, then why adopt the complexity of InfiniBand? If you do however require the absolute highest performance and can justify the investment in a specialized network, InfiniBand remains the top choice.\nHow is Tesla\u0026rsquo;s TTPoE Different? # Instead of implementing complex lossless mechanisms, TTPoE adapts Ethernet by replacing TCP with a custom transport layer with the goal of delivering microsecond-scale latency while allowing for simple hardware offload. How does it do this? By simplifying connection handling, removing wait states present in TCP, and taking a more direct approach to congestion control.\nDojo OSI Layers TCP Wait States This is an oversimplification, but for those who are not deep in the network engineering discipline, let\u0026rsquo;s try and simplify: TCP wait states happen during the process of establishing or terminating a connection between two devices over a network. When this happens, TCP pauses during the connection process and sequences data packets to ensure ordered delivery - which is why these pauses are necessary. When packet loss occurs, it triggers TCP\u0026rsquo;s recovery mechanisms, leading to more wait states and retransmissions. When dealing with large volume, this problem snowballs quickly.\nTCP Got It Right - Just Do It In Hardware # TCP Got it right - just do it in hardware is a direct quote from Tesla\u0026rsquo;s presentation and speaks to TTPoE\u0026rsquo;s key differentiator: it\u0026rsquo;s implemented in hardware. Tesla designed a custom MAC (Media Access Control) hardware block that sits between the chip and standard Ethernet hardware. This block, described as acting like a shared cache, handles the TTPoE protocol entirely in hardware, making it transparent to software.\nThis aids in the protocol facilitating efficient data transmission across standard Ethernet networks. Unlike lossless RDMA networks that require specialized switches, TTPoE functions over conventional Layer-2 transport. In contrast to both traditional TCP/UDP protocols and lossless RDMA solutions, TTPoE takes a unique approach by anticipating potential packet loss and incorporating built-in retry mechanisms. This design philosophy sets it apart in the landscape of network protocols.\nDumb-NIC and Potential Performance Gains # TTPoE is implemented on what Tesla calls a \u0026ldquo;Dumb-NIC\u0026rdquo; - a cost-effective network interface card designed for commodity driven scale. This approach allows Tesla to efficiently scale the number of host nodes feeding their Dojo supercomputer, which powers their autonomous driving training workloads.\nAt Hot Chips 2024, Tesla suggested that TTPoE could offer lower one-way write latency over switches, including NVLink. This claim, if proven in real-world deployments, could have significant implications for the performance of distributed AI training workloads.\nUltraEthernet Consortium and Public Offering # Tesla also announced its participation in the UltraEthernet Consortium (UEC). This step indicates Tesla\u0026rsquo;s intention to promote TTPoE as a potential new standard beyond just their own operations. By offering TTPoE publicly, Tesla is positioning its technology as a possible solution for broader AI and high-performance computing applications. Sharing is caring!\nConclusion # The emergence of these diverse networking approaches - InfiniBand, RoCE, and TTPoE - highlights a crucial point: there is no one-size-fits-all solution for AI infrastructure or even tech more broadly. Each approach has its strengths and trade-offs, making them suitable for different scenarios.\nInfiniBand remains the gold standard for ultimate performance in controlled environments. RoCE offers a bridge to lossless networking for organizations heavily invested in Ethernet infrastructure. Tesla\u0026rsquo;s TTPoE, while still unproven at scale, demonstrates the potential for new ideas and innovation tailored for specific unique purposes.\nAs AI continues to push the boundaries of computing, we can expect further innovations in networking technology. The key takeaway is the importance of aligning infrastructure choices with specific workload requirements and organizational constraints. The future of AI will likely be built not on a single networking standard, but on a diverse ecosystem of solutions, each optimized for particular use cases. Big thanks to Pete Lumbis for taking the time to peer review!\n","date":"5 September 2024","externalUrl":null,"permalink":"/posts/2024/tesla-adapts-ethernet-for-with-modified-transport-for-dojo/","section":"Posts","summary":"The foundation of high-performance computing that powers artificial intelligence lies not just in powerful GPUs, but in the intricate web of connections between them. As models grow exponentially in size and complexity, the networking infrastructure that facilitates their training has become a critical bottleneck. This has sparked a fascinating race to develop networking solutions tailored for AI-centric workloads, with tech giants like NVIDIA who are on an infrastructure trailblazing marathon.\n","title":"Tesla Adapts Ethernet with Modified Transport Layer for Dojo","type":"posts"},{"content":"","date":"5 September 2024","externalUrl":null,"permalink":"/tags/ttpoe/","section":"Tags","summary":"","title":"Ttpoe","type":"tags"},{"content":"","date":"5 September 2024","externalUrl":null,"permalink":"/tags/ultraethernet/","section":"Tags","summary":"","title":"Ultraethernet","type":"tags"},{"content":"","date":"29 August 2024","externalUrl":null,"permalink":"/tags/startups/","section":"Tags","summary":"","title":"Startups","type":"tags"},{"content":"In my previous blog on Dilution of Ownership, I explored how startup funding rounds impact equity. Now, let\u0026rsquo;s dive into a hot-button, highly-debated, and dramatically misunderstood matter of policy - taxing unrealized capital gains (or the wealth tax). Could this policy drastically alter the startup landscape? Would there be a tangible impact on founders, investors, and the innovation ecosystem as a whole?\nIntro The Allure of Taxing Paper Wealth # On the surface, taxing unrealized gains - (the increase in value of an asset that hasn\u0026rsquo;t been sold yet) - might seem like a fair way to ensure the ultra-wealthy pay their \u0026ldquo;fair share.\u0026rdquo; After all, why should founders sitting on billions in paper wealth escape taxation while their companies grow exponentially? It\u0026rsquo;s never a pleasant experience observing a person or entity that has made it so far, getting outsized benefits and incentives. However, this simplistic view overlooks the complex realities of growing innovative companies and is worth examining the trade-offs and potential consequences.\nThe Founder\u0026rsquo;s Dilemma # Imagine our friend Taylor from the previous post. Her AI-powered music authentication platform has taken off, and her stake in the company is now worth $500 million on paper. Under a regime taxing unrealized gains, Taylor could face a massive tax bill without having ever seen a dime of actual profit. This creates several problems:\nForced Liquidation: To pay the tax, Taylor might need to sell a significant portion of her shares, potentially losing control of her company. Misaligned Incentives: The pressure to constantly generate cash for taxes could push founders to prioritize short-term profits over long-term innovation and growth. Reduced Risk-Taking: The prospect of being taxed on paper gains might discourage entrepreneurs from taking big swings on potentially world-changing ideas. Innovation at Risk When founders are forced to focus on short-term liquidity instead of long-term vision, groundbreaking innovations that require years of development and uncertain outcomes become much less attractive propositions.\nThe Ripple Effect on the Startup Ecosystem # The impacts of taxing unrealized gains extend far beyond individual founders:\n1. Chilling Effect on Investment # Venture capitalists and angel investors might become more hesitant to back early-stage startups, knowing that taxes could significantly erode their potential returns before any exit event.\n2. Delayed or Canceled IPOs # The prospect of triggering a massive tax bill upon going public could cause founders to delay or even cancel plans for an IPO, keeping innovative companies private for longer and reducing opportunities for public investment.\n3. Brain Drain # Top talent might be less inclined to join startups if the potential upside of equity compensation is diminished by taxes on paper gains.\n4. Global Competitiveness # In a global economy, entrepreneurs might choose to build their companies in jurisdictions with more favorable tax treatment, potentially causing the U.S. to lose its edge as the darling of innovation.\nThe Broader Market Impact # Beyond the startup world, taxing unrealized gains could have far-reaching consequences for the broader economy:\nMarket Volatility: Forced selling to cover tax bills could lead to increased market volatility, especially for less liquid assets. Reduced Capital Formation: If investing becomes less attractive due to taxes on paper gains, it could lead to reduced capital formation and slower economic growth. Administrative Nightmare: Determining the fair market value of illiquid assets for tax purposes would be a complex and potentially arbitrary process. The Government\u0026rsquo;s Perspective: Revenue and Spending # While proponents of taxing unrealized gains often tout the potential for increased government revenue, it\u0026rsquo;s crucial to examine the broader fiscal implications:\nProjected Revenue # Estimates of the revenue generated from taxing unrealized gains vary widely. A 2021 proposal targeting billionaires was projected to raise approximately $557 billion over ten years, according to the Joint Committee on Taxation. However, these projections are subject to significant uncertainty due to market fluctuations and potential behavioral changes among those affected.\nAllocation of Funds # $557 billion over 10 years is a lot of green. Although never definitive, the intended use of this additional revenue often spans across:\nFunding social programs Infrastructure investment Debt Reduction Climate change initiatives However, history suggests increased revenue only sometimes translates to these specific outcomes.\nIs this really about Trust? Sometimes, it\u0026rsquo;s easy to get caught up in the outcome and avoid the details of how you might get there. I was talking to a good friend (a resident of San Jose) a few weeks ago. They shared their impression that California keeps increasing taxes but doesn\u0026rsquo;t use the additional revenue as projected by state officials. The lack of promised services in the face of additional taxes erodes trust. This observation is true at the federal level as well.\nThe Spending Dilemma # There\u0026rsquo;s a legitimate concern that any increase in tax revenue could be offset by increased government spending, potentially nullifying the fiscal benefits:\nHistorical Precedent: Analysis of U.S. fiscal policy shows that increases in tax revenue are often accompanied by increases in government spending. Fiscal Illusion: This phenomenon, where the public perceives the cost of government to be less than it actually is, can lead to increased demand for government services when revenue increases. Political Incentives: Policymakers may feel pressure to spend new revenue rather than use it for deficit reduction or long-term fiscal sustainability. Revenue vs. Efficiency While taxing unrealized gains might generate additional revenue in the short term, could the potential negative impacts on economic growth and innovation actually reduce overall tax receipts in the long run?\nConsider credit card debt. Many Americans are neck-deep in heavy-interest debt; many of us live above our means. When we get a raise or a higher-paying job, we increase our spending habits instead of keeping them level. Rather than use that additional income to pay off debt, we often increase our debt by adding monthly payments. This same phenomenon can be observed with government spending.\nBang for the Buck? # Given the potential downsides we\u0026rsquo;ve discussed - reduced innovation, market volatility, and administrative complexities - it\u0026rsquo;s worth questioning whether taxing unrealized gains would benefit society. The revenue generated might be substantial on paper, but if it comes at the cost of stifling the next generation of world-changing companies, the trade-off could be far from worth it.\nMoreover, the complexity of implementing and enforcing such a tax could significantly reduce generated revenue. The IRS would likely need substantial additional resources to administer this new system effectively.\nAs we consider these fiscal implications, it becomes clear that taxing unrealized gains is not just about generating revenue but would also alter the fundamental structure of our economy and the incentives that spur innovation and growth.\nA Delicate Balance # Suppose the multiverse exists outside of Marvel; no version of me would be a policymaker in any potential universe. Ensuring a fair tax system seems impossible and policymakers must tread carefully when considering changes that could fundamentally alter the incentives for innovation and entrepreneurship. Most things in life have unintended consequences. The bigger the problem, usually, the more adverse the consequences. The big question is, do the unintended consequences outweigh the short-term revenue gains? I\u0026rsquo;m glad I don\u0026rsquo;t have to make that call!\nFood for Thought Instead of focusing on taxing paper wealth, perhaps we should explore ways to incentivize long-term value creation, sustainable business practices, and a broader distribution of innovation\u0026rsquo;s benefits throughout society.\nConclusion # The debate around taxing unrealized gains highlights the delicate balance between generating government revenue and maintaining a vibrant, innovative economy. As we\u0026rsquo;ve seen, the potential downsides of such a policy could have far-reaching and long-lasting impacts on the startup ecosystem and beyond.\nThe future of innovation depends on finding more nuanced solutions that foster economic fairness and continued technological progress. What are your thoughts on this complex issue? How can we balance the need for a fair tax system with the imperative to encourage risk-taking and innovation?\n","date":"29 August 2024","externalUrl":null,"permalink":"/posts/2024/taxing-unrealized-gains/","section":"Posts","summary":"In my previous blog on Dilution of Ownership, I explored how startup funding rounds impact equity. Now, let’s dive into a hot-button, highly-debated, and dramatically misunderstood matter of policy - taxing unrealized capital gains (or the wealth tax). Could this policy drastically alter the startup landscape? Would there be a tangible impact on founders, investors, and the innovation ecosystem as a whole?\n","title":"The Ripple Effect: Could Taxing Unrealized Gains Stifle Innovation?","type":"posts"},{"content":"","date":"29 August 2024","externalUrl":null,"permalink":"/tags/venture-capital/","section":"Tags","summary":"","title":"Venture-Capital","type":"tags"},{"content":"There is a lot of confusion surrounding the dilution of ownership, especially in the tech startup space. What does this mean in layman\u0026rsquo;s terms? Who does it impact? And most importantly, why you should care if you plan to work for a startup. This is a topic I have discussed with many new (and even some seasoned) engineers over the past few years. I recently had this very discussion with a senior engineer (and close friend) who is leaving big enterprise for startup land. He suggested I write a blog in the plain speak that I used with him. Little does he know (until now) that this is as deep as I go on this topic!\nIntro A Few Thoughts on Funding # Building a startup isn\u0026rsquo;t easy and neither is scaling it. Founders are on the hook for securing capital and balancing the tricky scale of timing, valuation, and dilution of ownership. Venture Capital, especially in tech, plays a huge role in fueling innovation and accelerating growth. Why is this? While there are probably many answers to this, I think the primary reason is a result of how fast technology changes. This rapid change and additional contenders getting introduced in a particular market accelerate things and create additional competitive pressures.\nTaylor Builds a Great Tech Startup # Let\u0026rsquo;s dream up a hypothetical scenario! Meet Taylor, a world-class engineer turned entrepreneur. Taylor cooks up a fantastic idea for a platform that uses AI to verify the authenticity and originality of music, allowing one to distinguish human-created music from AI-generated content. Taylor puts together a top-notch founding team with some ex-engineers and has connections in the VC space to strike while the iron is hot. At the time of founding, the initial ownership is divided amongst Taylor, the founding team, and any investors. Let\u0026rsquo;s say there are 100,000 shares in total, and Taylor owns 40,000 shares. This means Taylor has 40% ownership.\nThe Dilution Effect # Taylor\u0026rsquo;s startup is established, the market fit is great, and operations are in place: it\u0026rsquo;s growth time! This means hiring sales, engineering, or other talent to scale the business. Taylor decides to raise an additional round of funding to make this happen. Additional shares get issued that the investors can then purchase. Let\u0026rsquo;s say, for the sake of example, Taylor\u0026rsquo;s startup issues an additional 50K shares in a Series: B round, bringing the total number of shares to 150,000. The dilution effect is apparent: Taylor retains the same amount of shares (40,000), but her ownership is now 26% as a percentage of the new total.\nWho gets Impacted? # Dilution can impact the founders who built the company, the investors who supplied funding, and any employees holding stock options or equity. A fundraising round can take the form of a down-round (valuation decreases vs. previous round), at par, or up-round (valuation increases vs. previous round). The outcome here largely determines how each class of shareholder is impacted moving forward.\nInvestors: As more shares get issued, the percentage of ownership that early investors hold in the company decreases, which may impact decision-making power and influence based on round dynamics. Founders: With founders, although the company’s value may increase, their slice of the pie continues to shrink. This can potentially minimize the control and influence a founder has on the direction of the company, but if they are strategically essential to the business, the ownership percentage is just one dimension of influence. Employees: Like our other two groups, the share of ownership for employees is diminished, thus potentially impacting employee morale. If the value of the shares don\u0026rsquo;t meet expected growth for employees, it may be addressed through increase salaries and additional shares. In advanced private stage companies, strong performers also typically get annual stock refresh grants. The Offset Valuation and dilution are interconnected realities. Valuation directly impacts future dilution. Founders aim to maximize valuation while minimizing dilution to offset impact. You can think of this as owning a smaller piece of a bigger pie, meaning the value of your shares will usually increase at the same time your equity is diluted.\nWhy Should you Care? # When working for a startup, the base salary is often less than the market average. The silver lining is the equity or stock options component of the compensation package, the performance of which heavily influences the financial upside for you as an employee. In working for a large enterprise, the focus is geared towards immediate financial security and long-term retirement planning. Usually, this means looking at things like base salary, healthcare benefits, and 401K match. With a startup, however, there are many other considerations potential employees should understand.\nRisk Tolerance: Since you will likely be taking a hit to base salary, take the time to research and understand if the equity potential outweighs that salary Vesting Schedule: When will you earn your shares? Be mindful of the vesting schedule. I just did a quick Google search, and a typical vesting schedule is four years with a 1-year cliff Timing: Earlier-stage startups carry more risk; While you will likely get more equity, you will have the trade-off of uncertainty Tax Implications: Understand what it means to exercise stock options and the potentially significant impact on your taxes. The following article from Carta explains this in detail Summary # As technical practitioners, you probably like to stay in your technology lane. When going to work for a startup, you will likely receive an offer that has a three-pronged compensation package, including base, equity, and bonus. You owe it to yourself to understand what growth stage the startup is in, the company\u0026rsquo;s runway, and how current and future fundraising plans might impact your equity over time. As always, consult with a financial planner and/or tax advisor before making any big decision.\n","date":"22 August 2024","externalUrl":null,"permalink":"/posts/2024/understanding-dilution-of-ownership/","section":"Posts","summary":"There is a lot of confusion surrounding the dilution of ownership, especially in the tech startup space. What does this mean in layman’s terms? Who does it impact? And most importantly, why you should care if you plan to work for a startup. This is a topic I have discussed with many new (and even some seasoned) engineers over the past few years. I recently had this very discussion with a senior engineer (and close friend) who is leaving big enterprise for startup land. He suggested I write a blog in the plain speak that I used with him. Little does he know (until now) that this is as deep as I go on this topic!\n","title":"Understanding Dilution of Ownership","type":"posts"},{"content":"","date":"15 June 2024","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"The Future is Back - Portland, OR","type":"talks"},{"content":"","date":"15 February 2024","externalUrl":null,"permalink":"/categories/automation/","section":"Categories","summary":"","title":"Automation","type":"categories"},{"content":"","date":"15 February 2024","externalUrl":null,"permalink":"/tags/chatgpt/","section":"Tags","summary":"","title":"Chatgpt","type":"tags"},{"content":"","date":"15 February 2024","externalUrl":null,"permalink":"/tags/gcp/","section":"Tags","summary":"","title":"Gcp","type":"tags"},{"content":"Sometimes, especially if you aren\u0026rsquo;t a developer by trade, you can get stuck on something small that will find you banging your forehead on your desk (figuratively, of course). Most of the time, it is easy enough to find an answer online or even from ChatGPT. Other times, you may not be so lucky. The other day, I fell victim to a TWE or time wasting event that I thought was worth writing about. Fasten your seatbelt!\nIntro The Problem # GCP SDK provides libraries and tools for interacting with Google\u0026rsquo;s cloud products and services. Pick your poison with whatever programming language you choose! On this particular Fun Day, I was writing a quick script to build VPCs and subnets from .yaml files and then expose a destroy call to wipe everything out when particular testing was completed. I\u0026rsquo;ve built something similar for AWS SDK for Go. On this particular day, I ran into the following error:\nError Digging into the Code # I knew I had to set AutoCreateSubnetworks: false, in the code as it disables GCP from automagically creating subnets for me. Of course, I want to do that myself! And this is where the confusion started \u0026ndash; I did have this set, and I had omitted the \u0026lsquo;IPv4Range\u0026rsquo; at the VPC level despite what the error said.\n1 2 3 4 5 6 7 8 9 10 func createVPCAndSubnets(ctx context.Context, service *compute.Service, vpcConfig *VPCConfig) { fmt.Printf(\u0026#34;Creating VPC: %s\\n\u0026#34;, vpcConfig.Name) op, err := service.Networks.Insert(vpcConfig.Project, \u0026amp;compute.Network{ Name: vpcConfig.Name, AutoCreateSubnetworks: false, }).Do() if err != nil { log.Fatalf(\u0026#34;Failed to create VPC: %s, error: %s\u0026#34;, vpcConfig.Name, err) } Solving the Problem # I searched on DuckDuckGo, my search engine of choice, and came up empty. In my desperation, I turned to a bucket-of-bolts I don\u0026rsquo;t fully understand who\u0026rsquo;s supposed to know all the answers, ChatGPT. I pasted in the error message and code. The AI Overlord then patted me on the back (again figuratively) and told me to set AutoCreateSubnetworks: false and remove IPv4Range from my code since that is for legacy networks. The problem is that IPv4Range wasn\u0026rsquo;t defined anywhere in my code and AutoCreateSubnetworks was already set to false.\nGoogle Wins # As I sat pondering reality, space, and time, I decided to just google search it and came across this excellent answer on Stack Overflow. Apparently, the AutoCreateSubnetworks field is not included in the request payload unless you define it explicitly with ForceSendFields: []string{\u0026ldquo;AutoCreateSubnetworks\u0026rdquo;},. All I needed was this extra line:\n1 2 3 4 5 6 7 8 9 10 11 func createVPCAndSubnets(ctx context.Context, service *compute.Service, vpcConfig *VPCConfig) { fmt.Printf(\u0026#34;Creating VPC: %s\\n\u0026#34;, vpcConfig.Name) op, err := service.Networks.Insert(vpcConfig.Project, \u0026amp;compute.Network{ Name: vpcConfig.Name, AutoCreateSubnetworks: false, ForceSendFields: []string{\u0026#34;AutoCreateSubnetworks\u0026#34;}, // I was missing! }).Do() if err != nil { log.Fatalf(\u0026#34;Failed to create VPC: %s, error: %s\u0026#34;, vpcConfig.Name, err) } Conclusion # Artificial Intelligence is a society-altering technology. Is it coming for our jobs anytime soon? I don\u0026rsquo;t think so. Will it help enhance and augment our workflows? Yes, Absolutely! However, situations like this remind me that there is no substitute for people who possess deep skills and understanding in particular areas of technology. This holds especially TRUE when something fails or isn\u0026rsquo;t exhibiting the correct behavior. Happy coding!\n","date":"15 February 2024","externalUrl":null,"permalink":"/posts/2024/gcp-sdk-fun/","section":"Posts","summary":"Sometimes, especially if you aren’t a developer by trade, you can get stuck on something small that will find you banging your forehead on your desk (figuratively, of course). Most of the time, it is easy enough to find an answer online or even from ChatGPT. Other times, you may not be so lucky. The other day, I fell victim to a TWE or time wasting event that I thought was worth writing about. Fasten your seatbelt!\n","title":"GCP SDK Fun","type":"posts"},{"content":"","date":"13 November 2023","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"NAF AutoCon0 - Denver, CO","type":"talks"},{"content":"","date":"6 July 2023","externalUrl":null,"permalink":"/tags/alkira/","section":"Tags","summary":"","title":"Alkira","type":"tags"},{"content":"","date":"6 July 2023","externalUrl":null,"permalink":"/categories/cloud/","section":"Categories","summary":"","title":"Cloud","type":"categories"},{"content":"","date":"6 July 2023","externalUrl":null,"permalink":"/tags/infrastructure-as-code/","section":"Tags","summary":"","title":"Infrastructure-as-Code","type":"tags"},{"content":"","date":"6 July 2023","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"For many moons, importing existing infrastructure (that is to say, infrastructure running outside of Terraform state), has not been a trivial task. Historically, Terraform did not generate any configuration. You would have to write the infrastructure-as-code in a manner that reflects how it was deployed. Then, to make matters not easier, you would fetch the \u0026lsquo;ol shovel and dig out the unique resource identifiers to feed through the command line. Handling a single resource in this manner is pretty simple. Wrangling 20+ resources like this is not. Last month, Terraform v1.5.0 was released, offering the ability to use import blocks. Let\u0026rsquo;s test this new feature on my favorite infrastructure provider, Alkira.\nIntro Why is this Useful? # This feature shifts import from a CLI driven approach to configuration-driven and plannable actions for adopting existing resources. Here are the key takeaways:\nConfiguration-Driven: You can now declare imports within your Terraform configuration files using an import block, making the process more streamlined and part of the initial planning.\nPlannable Action: Terraform treats importing as part of a standard plan. Running terraform plan will show a summary of the resources that Terraform intends to import, along with other planned changes.\nPreservation of existing CLI command: The existing terraform import CLI command remains unchanged and can still be used separately.\nSupport for Generating Configuration for Imported Resources: This feature, used in conjunction with the import block, enables templating of configuration when importing resources. A new flag -generate-config-out=PATH is added to terraform plan. When this flag is set, Terraform generates an HCL configuration for any resource included in an import block that doesn\u0026rsquo;t already have an associated configuration, writing it to a new file at the specified PATH.\nA Common Scenario # In this scenario, I\u0026rsquo;ll build an AWS VPC and connect it to Alkira using the alkira_connector_aws_vpc resource. This is a pretty common scenario I see with our customers. They begin a proof-of-concept for a particular use case and do most of the testing via Alkira\u0026rsquo;s excellent user interface. Instead of building a new environment for production however, a lot of times, they will want to take the proof-of-concept to production. From here, they need to import what has already been built into the appropriate Terraform State file.\nBuilding some Infrastructure # I\u0026rsquo;m going to mock-up the infrastructure we will import using Terraform.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # Some things already exist - fetch the IDs data \u0026#34;alkira_group\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;aws\u0026#34; } data \u0026#34;alkira_segment\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;my-org\u0026#34; } data \u0026#34;alkira_credential\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;aws\u0026#34; } /* Provision VPC https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc */ resource \u0026#34;aws_vpc\u0026#34; \u0026#34;this\u0026#34; { cidr_block = \u0026#34;10.1.0.0/16\u0026#34; tags = { Name = \u0026#34;this-vpc\u0026#34; } } /* Provision a single /24 subnet for VPC https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/subnet */ resource \u0026#34;aws_subnet\u0026#34; \u0026#34;this\u0026#34; { vpc_id = aws_vpc.this.id cidr_block = cidrsubnet(aws_vpc.this.cidr_block, 8, 0) availability_zone = \u0026#34;${var.aws_region}a\u0026#34; tags = { Name = \u0026#34;this-subnet\u0026#34; } } /* Attach VPC to Alkira https://registry.terraform.io/providers/alkiranet/alkira/latest/docs/resources/connector_aws_vpc */ resource \u0026#34;alkira_connector_aws_vpc\u0026#34; \u0026#34;this\u0026#34; { # AWS details aws_account_id = var.aws_account_id_p aws_region = var.aws_region vpc_id = aws_vpc.this.id vpc_cidr = [aws_vpc.this.cidr_block] # Alkira details name = \u0026#34;this-connector\u0026#34; cxp = upper(var.aws_region) credential_id = data.alkira_credential.this.id group = data.alkira_group.this.name segment_id = data.alkira_segment.this.id size = \u0026#34;SMALL\u0026#34; depends_on = [ aws_vpc.this, aws_subnet.this ] } # Output for connector_id output \u0026#34;connector_id\u0026#34; { value = alkira_connector_aws_vpc.this.id } Importing that Infrastructure Somewhere Else # Now that we have some infrastructure to work with, along with the resource identifiers, let\u0026rsquo;s put import blocks to the test. First, we create an import block for the Alkira connector in a file called imports.tf in a separate directory. I defined the connector_id returned from the previous configuration into a new variable called var.connector_id:\n1 2 3 4 import { to = alkira_connector_aws_vpc.this id = var.connector_id } Next, we initialize this separate directory containing the new Terraform configuration. Once everything is initialized, we can run -generate-config-out=main.tf which generates the following:\nImport Conclusion # This is a great feature that saves time. If I had to guess, as more functionality and polish is added, you\u0026rsquo;ll see modules popping up that leverage import blocks and provide a simplified way to import larger swaths of infrastructure. Some tools exist out there to do this today. Last year, I wrote about one such tool - Azure Terrafy. The difference is that since this is now integrated with the Terraform configuration and planning process, it can keep all of the logic in the HashiCorp ecosystem. No messing around with fetching binaries or needing to do any third-party tricks.\n","date":"6 July 2023","externalUrl":null,"permalink":"/posts/2023/using-terraform-import-blocks-with-alkira/","section":"Posts","summary":"For many moons, importing existing infrastructure (that is to say, infrastructure running outside of Terraform state), has not been a trivial task. Historically, Terraform did not generate any configuration. You would have to write the infrastructure-as-code in a manner that reflects how it was deployed. Then, to make matters not easier, you would fetch the ‘ol shovel and dig out the unique resource identifiers to feed through the command line. Handling a single resource in this manner is pretty simple. Wrangling 20+ resources like this is not. Last month, Terraform v1.5.0 was released, offering the ability to use import blocks. Let’s test this new feature on my favorite infrastructure provider, Alkira.\n","title":"Using Terraform Import Blocks with Alkira","type":"posts"},{"content":"","date":"20 June 2023","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"Aws","type":"tags"},{"content":"What fits somewhere in between re:Invent and Community Day events? That would be the AWS Summits! This year, I got to experience a double dose of fun by representing Alkira at our booth and presenting at the AWS Community Developer Lounge. I may be biased, but I believe the Alkira team is the best in the world.\nIt was a blast getting to talk about cloud networking with event attendees while getting a glimpse of how the public sector is adapting to change. And, for my first time presenting at a Summit, I thought it fitting to do a live demo. Why not pull in some advanced concepts and do it all live with semi-flaky internet and see how it goes? Leeroy Jenkins would be proud.\nIntro Highlights # My favorite highlights included:\nTeam Alkira # Whether it was the many engaging conversations we had with event attendees at the booth, the unplanned team-building exercise of scooting through the D.C. bike lanes or seeing the Washington Nationals get smashed, I wouldn\u0026rsquo;t pick any other team to do it with. What made this event great was the fantastic questions we got from event attendees. As it turns out, great questions lead to great conversations!\nTeam Alkira Keynote # At the keynote, we got to hear from the CIA\u0026rsquo;s first CTO, Nand Mulchandani. Keeping your ears open will teach you something new every day. For me, it was that researchers at the CIA created the lithium-iodine battery. I guess this shouldn\u0026rsquo;t come as a surprise, given the need for long-lasting battery power and the nature of surveillance.\nModular Data Center # Back in February AWS announced their Modular Data Center for U.S. Department of Defense Joint Warfighting Cloud Capability. This only available to government customers under the JWCC Contract and is currently supported in the AWS GovCloud (US-West) and (US-East) regions.\nModular DC Why is this cool? You could rely on limited infrastructure. You could also procure, build, and provision infrastructure yourself. Why not just deploy a self-contained and modular data center? In isolated environments, it can securely store, analyze, and interpret petabytes of data in real-time. I got to walk through one of these at the summit and my nerd senses were tingling.\nSnowblade Announcement # Want compute and storage amongst other hybrid services in remote locations, including Denied, Disrupted, Intermittent, and Limited (DDIL) environments? If you are a (JWCC) contract customer, take a look at AWS Snowblade. This tech extends AWS infrastructure to the tactical edge and meets U.S. Military Ruggedization Standards (MIL-STD-810H). Snowblade is available in the AWS GovCloud (US-West) region.\nPresenting in the Dev Lounge # Through the amazing AWS Community Builders program, I got to present in the Dev Lounge. Many folks wanting to enter tech seem to go the software engineering route by default. Serverless and AI/ML are also newer and much more shiny than networking. Throughout my time in tech, there appears to be a waning interest in network engineering.\nOne of my goals in the community is to show that networking is equally exciting in the cloud, especially when paired with awesome tools like Terraform. In this Dev Chat, I started with some theory and real-world problems and then ran a live demonstration on how you could solve some of those problems by combining the power of AWS Transit Gateway and Terraform. The slides and code I ran for the demo can be found here.\nDev Chat Remember the Foundation Properly designed and executed network and security is the solid foundation on which many of the newer and trendy technologies run atop. If you drive a Ferrari, you can reach your destination quickly (I wish I had a Ferrari). If the highway isn\u0026rsquo;t in place, however, and you were driving on rugged terrain, can you imagine what the experience would be like? Imagine also if an optimal interconnection of highways didn\u0026rsquo;t exist between you and your destination.\nConclusion # This AWS Summit was a whirlwind of learning, engagement, and networking - literally and figuratively! Representing Alkira and having the opportunity to educate attendees on our product led to many deeper discussions around the state of networking in the cloud. It was a blast to discuss cloud networking, interact with various individuals, and offer a different perspective on a field often perceived as a thing I want to get out of the way.\n","date":"20 June 2023","externalUrl":null,"permalink":"/posts/2023/aws-dc-summit-recap/","section":"Posts","summary":"What fits somewhere in between re:Invent and Community Day events? That would be the AWS Summits! This year, I got to experience a double dose of fun by representing Alkira at our booth and presenting at the AWS Community Developer Lounge. I may be biased, but I believe the Alkira team is the best in the world.\n","title":"AWS DC Summit - Recap","type":"posts"},{"content":"","date":"20 June 2023","externalUrl":null,"permalink":"/categories/community/","section":"Categories","summary":"","title":"Community","type":"categories"},{"content":"","date":"15 June 2023","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"AWS Community Day - Chicago, IL","type":"talks"},{"content":"","date":"7 June 2023","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"AWS Summit - Washington, D.C.","type":"talks"},{"content":"","date":"17 May 2023","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"ONUG Spring - Dallas, TX","type":"talks"},{"content":"","date":"16 May 2023","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"Worldwide Software Architecture Summit'23","type":"talks"},{"content":"I recently wrote a blog about generating Terraform with ChatGPT. In that experiment, I was using GPT-3.5. It didn\u0026rsquo;t take much to trip up the galactic AI in many of the experiments I did with Terraform and beyond. According to OpenAI GPT-4 soars past its predecessor in its advanced reasoning capabilities. Let\u0026rsquo;s use this model to build code in Python and Go. How will the super titan fair in the Sysadmin arena? Let\u0026rsquo;s take on some tedious backend work head-on with the biggest brain around and see what happens.\nIntro Creating a Narrative # In the vast expanse of the Digital Frontier, there were the sysadmins. These Server Sheriffs were the guardians of data prairies. Notorious for their unwavering dedication to their craft, Server Sheriffs ensured that systems always stayed up (and up to date). When I did a lot of sysadmin work, AD-HOC scripting was serious business.\nLet\u0026rsquo;s not kid ourselves. AD-HOC scripting is STILL serious business. One common task was logging into a ton of systems and running a set of commands. For this little experiment, I launched three VMs running Debian 11.3 in the lab - (VMware ESXi hypervisor).\nStarting Simple # In my experience testing ChatGPT so far, I have found a lot of success in asking a simple question first and then refining the output with more thoughtful questions once I have an idea of what it is throwing back at me. Let\u0026rsquo;s start with the following:\nQuestion: 1 # Write a python3 script that logs into a debian server and runs a command Question 1 Observations The first pass was a success! Even though I\u0026rsquo;m a cowboy, I\u0026rsquo;m hard-coded to not hard-code passwords in files. Before I ran the script, I copied it out of ChatGPT, and tweaked the password var to use getpass. I also filled in the username, server IP, and command. To that point, I would rather not use passwords at all. And logging into a single server is hardly useful. Logging into many servers is. Let\u0026rsquo;s ask ChatGPT to help us out here.\nMore Servers, No Passwords # At this point, I put on a fierce glare, look the AI straight in the eyes, and murmur, \u0026ldquo;Is that all you got, you bucket of bolts?\u0026rdquo; For the second pass I created a key pair with ssh-keygen and SCP\u0026rsquo;d it to each of the Debian VMs. And with that, I ask my next question:\nQuestion: 2 # Use this script to loop through multiple servers with key based authentication Question 2 Another Small Step This question only required a little of an update to the script. Merely updating the function input to key_filename, adding a variable for the key, and constructing a For Loop for the cattle, I mean servers. I went ahead and switched out the command to apt-get update. Anyone could build this script, right? After testing, I glanced over and saw ChatGPT\u0026rsquo;s spurs glinting like silver in the blazing sun.\nArguments And Files # One common task I would do in my sysadmin days was, adding multiple commands in a file and passing that file into the script as a command line argument. This is something I started doing many moons ago first with bash scripting, then Perl, and onwards to Python. This was a great approach to automation before APIs took over the world.\nQuestion: 3 # Use this script to run multiple commands passed in from a file as a command line argument Question 3 Winner Winner I added some commands in commands.txt in my local dir and passed it in with my command. Not surprisingly at this point, everything was successful. In the middle of the town, as the church bell struck high-noon, I took a sideways glance at ChatGPT and tipped my hat. ChatGPT glanced back at me as a tumbleweed passed quietly between us. The AI Titan then murmured, \u0026ldquo;Would you like me to translate that script to GO?\u0026rdquo;\nTranslating Python to Go # A long time ago, I worked on a project that took several enormous Perl scripts and attempted to transpose them to Python. These scripts were hacked together like nothing I\u0026rsquo;d ever seen. We quickly realized the best path forward was starting from scratch. I have been spending more time in Go lately, so let\u0026rsquo;s see if our AI Superpower is multilingual.\nQuestion: 4 # Create this script in Go Question 4 Color me Impressed In some of my testing, I went back to some fairly dated Perl scripts I had lying around and had ChatGPT translate them to some of the newer languages the cool kids are using now. I then opened a new chat and had this rattletrap convert the new scripts back to Perl and thought, so that is what really efficient Perl would have looked like!\nConclusion # Asking ChatGPT questions is kind of like building an MVP (minimum viable product). You don\u0026rsquo;t ask for the finished product on the first pass. That initial request forms the baseline, e.g., I want a script that can connect to this and run that. You, as the consumer, can then do some testing and then provide feedback by means of asking additional questions. This makes me think of a customer feedback loop. In this case, though, you are the customer and, to some extent, the builder. In my next round of experiments, I\u0026rsquo;ll be setting my aim on networking. Perhaps some OSPF and BGP?\n","date":"23 April 2023","externalUrl":null,"permalink":"/posts/2023/configuring-systems-with-chatgpt-python-and-go/","section":"Posts","summary":"I recently wrote a blog about generating Terraform with ChatGPT. In that experiment, I was using GPT-3.5. It didn’t take much to trip up the galactic AI in many of the experiments I did with Terraform and beyond. According to OpenAI GPT-4 soars past its predecessor in its advanced reasoning capabilities. Let’s use this model to build code in Python and Go. How will the super titan fair in the Sysadmin arena? Let’s take on some tedious backend work head-on with the biggest brain around and see what happens.\n","title":"Configuring Systems With ChatGPT, Python, and Go","type":"posts"},{"content":"Usually, when it comes to technology, my grandmother doesn\u0026rsquo;t know much because she doesn\u0026rsquo;t care. What is the cloud? How to install a new browser on her laptop? What is 2FA? I might be speaking French to her as I discuss these things. Yet, she knows what ChatGPT is. This shows the vast amount of publicity, hype, and polarization that has ensued since November 2022. I tend to avoid AI fear-mongering and focus more on, how could a tool like this help enhance my daily grind? Can ChatGPT write Terraform as elegantly as a poem written from the perspective of Samuel L. Jackson in Pulp Fiction? Let\u0026rsquo;s take it for a spin on AWS using infrastructure-as-code.\nIntro Create a VPC and Two Subnets # Let\u0026rsquo;s ease in gently. Just like when my wife says she is making cookies, I tell myself I will keep things simple and only have one. My first request for ChatGPT is a simple one: Create a VPC and two subnets in AWS using Terraform.\nVPC + Two Subnets Observations That was pretty seamless. I provided little detail in my first request, but ChatGPT filled in all the required arguments added tags for the Name of each resource, and even put my subnets in separate availability zones.\nDoes it Work? # Whenever the cookies come out of the oven, I am overcome with a sense of responsibility to test them out before anyone else eats any (maybe I run this test 3 or 4 times depending on the cookie). Like those cookies, ChatGPT isn\u0026rsquo;t getting any favors from me. Let\u0026rsquo;s test:\nTest the Terraform Updating our Request # I\u0026rsquo;m always down for a good conversation when I\u0026rsquo;m eating cookies. Let\u0026rsquo;s talk a little more to ChatGPT and request an update to our Name tags for our resources:\nAdditional Requests Captivating Conversation Asking ChatGPT to update Name tags was successful. Not only did I get the updated code, but comments were added to the code file to show what changes were made. This conversational style is very intuitive. If I don\u0026rsquo;t get what I need, or maybe the output clues me into something additional I need to add, I need but ask. This generative pretrained transformer is starting to win my heart over, just like cookies do.\nGoing Beyond Basics # At some point (usually around Christmas / New Year\u0026rsquo;s), I realize I have been eating way too many cookies. The cookies have failed me. Let\u0026rsquo;s add more logic to see if ChatGPT fails me too. Since ChatGPT offered up handling availability zones without me asking, let\u0026rsquo;s see what happens if I throw the count Meta-Argument in the mix?\nAdding Logic Too Many Cookies Anyone who has spent time writing Terraform would likely have spotted the problem before running the code. The \u0026lsquo;count\u0026rsquo; Meta-Argument in Terraform works using an incrementing counter. With ChatGPT knowing all the \u0026ldquo;answers\u0026rdquo; and providing the availability zones as part of the configuration, it also decided to increment them in the same manner used with cidr_block and Name tag. Using this logic, it produced us-east-11, us-east-12, and us-east-13 which are not availability zones in AWS, thus causing my configuration to crash and burn. Time to go on a diet?\nConclusion # It is hard not to be impressed. The value goes beyond simply providing lines of code. With each update to my request, ChatGPT provided a clear explanation as to why it modified the logic in the manner that it did. Since I began experimenting in January, I have found many ways to get ChatGPT to produce wrong (sometimes laughable) code or configuration. Network gear, general-purpose programming, and infrastructure-as-code are complex things, though. Numerous complications, variables, versioning, and interpretation require adjusted expectations.\nKnowing how to phrase the question that frames what you need is half the battle. It will supply you with as much detail in the configuration as you request and provide placeholders for the rest. In my testing, I have learned quite a few things that have ended up in my day-to-day workflows, which is valuable to me. The idea that this is only the beginning doesn\u0026rsquo;t scare me in the way that it scares many others I have talked to. ChatGPT isn\u0026rsquo;t taking my job. If there is anything that does scare me, it is technologists at the beginning of their journey that will miss out on the valuable details of working their way through complex problems. That experience is worth its weight in gold.\n","date":"28 March 2023","externalUrl":null,"permalink":"/posts/2023/can-chatgpt-terraform-simple-networking-in-aws/","section":"Posts","summary":"Usually, when it comes to technology, my grandmother doesn’t know much because she doesn’t care. What is the cloud? How to install a new browser on her laptop? What is 2FA? I might be speaking French to her as I discuss these things. Yet, she knows what ChatGPT is. This shows the vast amount of publicity, hype, and polarization that has ensued since November 2022. I tend to avoid AI fear-mongering and focus more on, how could a tool like this help enhance my daily grind? Can ChatGPT write Terraform as elegantly as a poem written from the perspective of Samuel L. Jackson in Pulp Fiction? Let’s take it for a spin on AWS using infrastructure-as-code.\n","title":"Can ChatGPT Terraform Simple Networking In AWS?","type":"posts"},{"content":"","date":"3 February 2023","externalUrl":null,"permalink":"/series/evolution-of-aws-site-to-site-vpn/","section":"Series","summary":"","title":"Evolution of AWS Site-to-Site VPN","type":"series"},{"content":"In Part 1, we talked about the origins of the Site-to-Site VPN Service in AWS. As consumers began to scale in the early days, they faced tunnel sprawl, performance constraints, and the need for a simplified design. AWS responded with Transit Gateway. How did Transit Gateway simplify architecture leading to smoother operations, better network performance, and a scalable blueprint for the future network?\nIntro Pre Transit Gateway # Security teams in the early days would often balk at the idea of using VPC peering without having a centralized transit hub (where the hybrid connectivity was landed). Since VPC couldn\u0026rsquo;t do any advanced packet forwarding natively, many designs would do transitive routing on the customer gateway. Traffic patterns looked something like this:\nLegacy Transit Observations In this design, the tunnels exist from the customer gateway all the way to the VPC. In this one-to-one relationship, there is no intelligent way to manage the complexity of incremental connections as you grow. Traffic also exits AWS even when the destination is another VPC which is inefficient. Also, since one tunnel is actively forwarding traffic at a time, you are limited to ~ 1.25 Gbps.\nTransit VPC # To solve some of the shortcomings with VPC peering and tunneling to each VPC directly, Transit VPC was born. This solution deploys a hub-and-spoke design that reminds me of my data center networking days when we connected sites to data centers with all the glory of active/standby and boxes everywhere. Want to connect another network? Deploy another few boxes! Sometimes this is unavoidable, but any opportunity I get not to manage additional appliances or agents, I take it.\nWhen I prototyped this design for the first time, we used Cisco CSRs deployed in a Transit VPC. This acted as the hub, and each spoke VPC\u0026rsquo;s VGW had two tunnels to the CSRs with BGP running over IPsec. Since VGW doesn\u0026rsquo;t support ECMP, this gives us active/standby out of the box. This design does perform transitive routing in the hub which keeps spoke-to-spoke traffic in the cloud.\nTransit VPC Appliance Sprawl In this design, you are responsible for provisioning the appliances (across multiple availability zones in a given VPC). This means you have to do the routine software upgrades along with the emergency firefighting when new CVEs are uncovered. Also, running this design at scale is problematic. As the number of VPCs grow, the number of appliances grow. Imagine the above diagram with 60 VPCs attached.\nEnter Transit Gateway # AWS Transit Gateway (TGW) works as a managed distributed router, enabling you to attach multiple VPCs in a hub + spoke architecture. These VPCs have reachability to each other (with the TGW doing the transitive routing). All traffic remains on the global AWS backbone. The service is regionally scoped; however, you can route between transit gateways in different AWS regions using Inter-Region Peering.\nYou connect various network types to TGW with the appropriate attachments. The various attachment types can be found here. Since we are talking about the evolution of site-to-site VPN design, we would use the TGW VPN Attachments to terminate our VPNs and TGW VPC Attachments for connecting VPCs.\nSimplified Connectivity # Keeping things simple has a lot of benefits. This includes reducing the number of tunnels you have to manage while getting more from them. Sometimes you find that less is more, especially knowing that someone has to do operations!\nLudwig Mies van der Rohe Ludwig was a German-American architect who adopted the motto less is more to describe the aesthetic of minimalist architecture. In the network, a design should be as simple as possible while meeting requirements.\nWhen using Virtual Private Gateway (VGW), we know the tunnel spans from the customer gateway to redundant public endpoints in different AZs. With this new design, tunnels would land directly on the transit gateway. This also enables us to leverage a single VPN connection for all of our VPCs back to on-premises. We can also use ECMP to aggregate the bandwidth of both tunnels in the connection.\nTransit Gateway Accelerated Connections # When using transit gateway, you can enable acceleration when adding the TGW attachment. An accelerated connection uses AWS Global Accelerator to route traffic from your on-premises network to the closest AWS edge location. Your traffic is then optimized once it lands on AWS\u0026rsquo;s global network. Let\u0026rsquo;s examine using acceleration with TGW versus no acceleration with VGW:\nAccelerated VPN When to Accelerate # If the sites being connected are close to a given AWS region where your resources exist, you will see similar performance between both of these options. As distance increases, additional hops over the public internet are introduced, which can increase latency and impact reliability.\nThis is where acceleration shines. It works by building accelerators that enable you to attach redundant anycast addresses from the edge network. These addresses act as your entrypoint to the VPN tunnel endpoints and then proxies packets at the edge to applications running in a given AWS region.\nHow much improvement? AWS boasts up to 60% better performance for internet traffic when using Global Accelerator. You can learn more about how this is accomplished along with the criteria for measurement here.\nConclusion # Understanding your organization\u0026rsquo;s goals and how the network needs to support them is no trivial task. This is where \u0026ldquo;it depends\u0026rdquo; rears its proverbial ugly head. In the world of cloud networking, complexity, data transfer, and compounding cost, it pays to think through the design and weigh the trade-offs. One thing is certain \u0026ndash; AWS\u0026rsquo;s site-to-site VPN and other adjacent services have evolved, providing the means to construct high-performing networks with a global scale and amazing user experience.\n","date":"3 February 2023","externalUrl":null,"permalink":"/posts/2023/evolution-of-aws-site-to-site-vpn-part-2/","section":"Posts","summary":"In Part 1, we talked about the origins of the Site-to-Site VPN Service in AWS. As consumers began to scale in the early days, they faced tunnel sprawl, performance constraints, and the need for a simplified design. AWS responded with Transit Gateway. How did Transit Gateway simplify architecture leading to smoother operations, better network performance, and a scalable blueprint for the future network?\n","title":"Evolution of AWS Site-to-Site VPN - Part 2","type":"posts"},{"content":"","date":"3 February 2023","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"3 February 2023","externalUrl":null,"permalink":"/tags/site-to-site-vpn/","section":"Tags","summary":"","title":"Site-to-Site-Vpn","type":"tags"},{"content":"","date":"20 December 2022","externalUrl":null,"permalink":"/tags/aws-community-builders/","section":"Tags","summary":"","title":"Aws-Community-Builders","type":"tags"},{"content":"At the end of each year, I take some time for self-reflection. Looking back through 2022 gave me a colossal reminder of how vital teams, communities, and leaders are. Take hockey for instance. You have certain athletes, often referred to as generational players that carry unmatched individual talent and get selected first overall in the NHL Draft.\nThese players are weekly highlight reels, yet, without the right system, coaches, and team, they will never lift the Stanley Cup. This year, I had the opportunity to learn from great leaders, work with amazing teams, engage with valuable communities, and visit exciting places. Here is a recap of my 2022.\nHighlights Startup Life With Alkira # My favorite part of working for a startup like Alkira is, making life easier for our customers. Having amazing founders that promote a culture that exemplifies a team of teams, has a way of creating opportunities that translate directly to the product that, in turn, serves our customers. I\u0026rsquo;m thrilled to be working with the brightest minds in networking as we take a lot of momentum into 2023.\nTech Evangelism # I participated in some incredible community and partner events in 2022. Travel is back in full swing post-pandemic, and it was great to see happy smiling faces again. I got to jointly present with the ONUG Collaborative on the Network Cloud - Whitepaper we finished earlier this year, and also the state of Network Cloud. NYC is nice and all, but you can\u0026rsquo;t beat the weather in Santa Clara which is where Tech Field Day was hosted. The weather was terrific, the delegates were outstanding, and it was a blast connecting with everyone.\nPartner Events # Many large organizations juggle various architectures that were deployed at different times when specific technologies were available. This is why partner integrations are critical for businesses looking to evolve while still managing some of their older design patterns. Often, those older applications are the ones generating a lot of the revenue.\nI got to co-present with quite a few of our partners this year, including HashiCorp, Fortinet, and Infoblox. These events are significant because the better together story is grounded in reality. Each of these integrations is there because it solves a considerable customer problem.\nMidwest Community # Connecting with the community closer to my backyard is always great! AWS Community Day - MidWest was hosted in the Buckeye State, and the sessions were fantastic. I talked about network complexity and digital transformation. I also somehow landed in Kansas City for the BBQ, and stayed for Cloud Security Alliance - KC Chapter. This was a fun bunch with tons of questions and engagement.\nAWS Community Builders # Passion culminates in sharing and brings contagious energy to many professionals at every stage of their journey. I\u0026rsquo;m super excited about participating in the AWS Community Builder program in the Network C\u0026amp;D category. So far, the engagement has been great. I have been able to learn from seasoned experts and also grow new technologists. This is what makes community such a powerful machine.\nCommunity Builders Join Us! If you are curious about joining the program, you can find out more here. During my first month in the program, I got expert assistance with some complicated Terraform I was writing, authored a blog on the AWS Community Builders DEV org, and peer-reviewed some blogs for other builders. Community is a fantastic way to grow your network!\nLinkedIn Learning # Sometime in late 2021, the passionate and dynamic Heather Hurley reached out to me to discuss making LinkedIn Learning Content. One gap that was particularly interesting to me was networking skills transitioning to the cloud-era. The need for content that went beyond certifications and approached the problem with real-life scenarios and experiences was needed. Working with the LinkedIn Learning team was a blast, and in September, we released Hybrid Multicloud Networking - Practical Concepts.\nLinkedIn Learning Want to make content? Are you passionate about your craft? Want to bring it to life and help others along their journey? You can apply to become a LinkedIn Learning Instructor here. If you have experience creating content on your own, then doing it with LinkedIn will be a pleasant surprise. Not having to do your own post-processing is winning!.\nWriting About Startups # In 2022, I decided to pick a few up-and-coming startups to write about that I think will be valuable in the future. This year it was Lightlytics, Infracost, and ZeroTier - (I guess ZeroTier isn\u0026rsquo;t technically a startup anymore, but Mesh VPNs are cool). I plan to continue that trend this year.\nConclusion # Happy Holidays to everyone! Hopefully, many of you can take time with your family and recharge. If you are near the Hartford, CT area, join me at the first (CT)NUG of 2023 on January 19th at Thomas Hooker Brewery. If you are not in Hartford, check out (US)NUA for upcoming events close to your area! Hearing professionals in the community talk from experience and share their collective wisdom is magic and offers some good perspective you can\u0026rsquo;t get over Zoom. See you there!\n","date":"20 December 2022","externalUrl":null,"permalink":"/posts/2022/looking-back-at-2022/","section":"Posts","summary":"At the end of each year, I take some time for self-reflection. Looking back through 2022 gave me a colossal reminder of how vital teams, communities, and leaders are. Take hockey for instance. You have certain athletes, often referred to as generational players that carry unmatched individual talent and get selected first overall in the NHL Draft.\n","title":"Looking Back At 2022","type":"posts"},{"content":"As far as tech conferences are concerned, it\u0026rsquo;s hard to find one as exciting as AWS re:Invent. Whether it\u0026rsquo;s anticipation for new product announcements or connecting in person with the community, there is something electrifying about being at ground zero. And if you can make the trip, you will get a lot of great exercise too! I hit close to 100K steps or approx. 43 miles according to my Fitbit. What were some of my favorite highlights from re:Invent 2022?\nIntro Community # Jason Dunn put on a spectacular event for the AWS Community Builders. Intros were made to various members of the AWS team, community managers, topic leaders, developer advocates, and DevRel leadership. Even the legend, Jeff Barr, was there. The food was hot, the SWAG was hotter, and the conversation and networking was off the chain. These types of events are where major opportunities happen. Want to become a Community Builder? Check out our page.\nAttendance # There were over 60K in attendance this year, and all the sessions I attended were excellent. You can find most of the events already available on the AWS Events YouTube channel.\nTimelapse Network Announcements # My favorite category, Networking \u0026amp; Content Delivery, saw a few new product launches. Let\u0026rsquo;s dig in!\nVPC Lattice # Want consistent network policy and traffic management across instances, containers, and serverless? A new full managed service called VPC Lattice is now in preview. This feels like a service-mesh \u0026ldquo;lite\u0026rdquo; aimed at scaling service-to-service connections while incorporating some zero-trust. Having visibility into service-to-service interactions is important.\nVPC Lattice Pricing # VPC Lattice Pricing is broken down into three components that ultimately decide how much that final bill will increase.\nPer hour charge for each running service (that runs on instances, containers, or serverless). The price will differ per region, but for US East it is $0.0250/hr Per GB charge for each gigabyte of data running through each service. Again, the price for US East is $0.0250/GB Requests made to to each service are priced at a $0.10 per 1 million requests rate. You begin getting charged once you exceed the always free tier Preview Waitlist Services are heavily restricted and controlled during Preview periods. VPC Lattice is only available in US West (Oregon) as of right now. You can get on the waitlist here. Preview is available for up to 5 AWS accounts at a time.\nVerified Access # VPNs are getting a lot of hate these days, and ZTNA products are getting a lot of love. I\u0026rsquo;m not surprised to see AWS release Verified Access for secure access to corporate applications. Like the bulk of ZTNA products on the market today, Verified Access uses conditions based on identity data and device posture for application access.\nVerified Access Pricing # Verified Access is broken down into two components that make up the final bill.\nApplication Hours is an hourly charge for associated applications, which comes in at $0.27/hr for 1-148800 app-hours. If you surpass 148800 app-hours, this is reduced to $0.20/hr. Each partial application hour is rounded up and billed for the whole hour. GB of data processed is a $0.02 per GB charge that gets processed for all data flowing between users and applications while using the service. Services in Preview Visit Service Terms to learn more about the terms and conditions for all release types. Remember, Preview releases are not intended for production! You can check here to see if new services are available in a given region.\nOther Big Announcements # This year seemed bigger than ever, and if I had to pick one category that commanded a lot of attention, it was data management. Nobody says this as elegantly as the CEO:\nAdam Selipsky To unlock the full power, the full value of data, we need to make it easy for the right people and applications to find, access and share the right data when they need it — and to keep data safe and secure.\nDataZone # DataZone aggregates data sources, sets up a data catalog of sorts, and allows you to define a taxonomy. You can then govern access to data in one place. Let\u0026rsquo;s face it, data is heavy, hard to manage, and time-consuming to make sense of. Anything that streamlines and simplifies administration is winning.\nSecurity Lake # Wherever there is data, security is not far behind. What happens when you combine data lakes and security? You get a purpose-built data lake for security-related data and name it Security Lake. On the surface, this looks pretty valuable as it appears to aggregate data from the cloud and on-premises security infrastructure and solutions and normalize it with the Open Cybersecurity Schema Framework (OCSF). In this long history of security and data, gathering tons of data was never a problem. Normalizing, understanding, and contriving value is.\nCloudWatch - Internet Monitor # CloudWatch got a new feature called Internet Monitor which enables you to continually monitor internet availability and performance metrics. The monitoring happens through your VPC, CloudFront distributions, and Workspaces directories. The goal is to arm operations with insight into how internet issues impact the performance of applications hosted in AWS and end-users accessing those applications.\nConclusion # As always, re:Invent did not disappoint. There was a lot of post-pandemic excitement that was pretty contagious, and it was great to catch up with folks I hadn\u0026rsquo;t seen in a while or had never met in real-life. When you get that many builders together, you know some magic is bound to happen.\n","date":"6 December 2022","externalUrl":null,"permalink":"/posts/2022/aws-reinvent-2022-recap/","section":"Posts","summary":"As far as tech conferences are concerned, it’s hard to find one as exciting as AWS re:Invent. Whether it’s anticipation for new product announcements or connecting in person with the community, there is something electrifying about being at ground zero. And if you can make the trip, you will get a lot of great exercise too! I hit close to 100K steps or approx. 43 miles according to my Fitbit. What were some of my favorite highlights from re:Invent 2022?\n","title":"AWS re:Invent 2022 - Recap","type":"posts"},{"content":"The necessity for protocols to keep communication secure has been around since the dawn of the internet. The first ever VPN was jointly developed by a vendor consortium (which included Microsoft) in 1996, and came in the form of Point-to-Point Tunneling Protocol. Although many are skeptical about the value of VPNs in 2022 and beyond, customer consumption of cloud provider VPN services have paved the way for additional features and exponential scale.\nWhat impact do innovations like Transit Gateway and Accelerated VPN Connections have on design complexity, network performance, and operations? In this blog, I will look back at the first time I deployed a site-to-site VPN and then examine what is possible today when thinking through network design. This is going to be two parts.\nMy First Experience # Site-to-Site VPN in AWS is a fully managed and highly available service. This comes in the form of two endpoints on the AWS side (public IP addresses in different AZs per one VPN connection). This service is charged hourly per connection (plus data transfer charges). My first experience connecting on-premises to AWS looked something like this:\nFirst VPN This was long before the public cloud had disrupted enterprise infrastructure. In this case, we had a single VPC that required connectivity back to on-premises. Since the application being developed wasn\u0026rsquo;t production, we set up a single VPN connection to a single physical router in the Data Center. This was easy work since, as network engineers, we pushed VPNs like weights. And we had an existing B2B process in place, so our paperwork + process was ready. Somewhere along the way, though, this application went into production.\nMaking it Highly Available # Pre Transit Gateway, you would typically see a Virtual Private Gateway and VPN connection from each VPC back to on-premises devices. If you follow best practices and want the highest availability possible in this scenario, you will also have redundant devices on-premises. This means you would end up with 2x VPN connections, which totals 4x VPN tunnels per VPC. Once the application went past QA, to meet our requirements for Tier 0 infrastructure, we needed high availability.\nSingle VPC Traffic Forwarding One question I could always count on getting from developers when we would onboard this design was: \u0026ldquo;Since we have four tunnels here, why can\u0026rsquo;t we forward traffic on all of them?\u0026rdquo; The simple answer is that you can override defaults and forward traffic across all tunnels to AWS from on-premises; however, the VGW will always select a single tunnel for return traffic.\nGrowth, Scale, and Operations # At some point, Digital Transformation (you may have heard of it) caused crazy fast growth. And with that growth came more VPCs. And with those VPCs came the necessity for connectivity back to on-premises. Say we add just two additional VPCs using the above methodology, and we end up with 6x VPN connections, bringing us to 12x VPN tunnels to manage.\nMulti VPC How much growth? The above example is single region with only 3 VPCs. The reality for many organizations using AWS is much larger. What would this design look like if we had 50 VPCs? What happens when we need to include multi-region for DR capabilities? How does it impact operations when an application has performance issues, and the network is getting blamed?\nConclusion # As we onboard more VPCs, it becomes apparent that the above design does not easily scale. Bandwidth is limited, tunnels increase exponentially, and operations will have limited success in diagnosing network problems (or proving the network is not the problem). In Part 2 we will dive into how Transit Gateway and Accelerated VPN Connections take this from an operational misstep to an enterprise success story.\n","date":"22 November 2022","externalUrl":null,"permalink":"/posts/2022/evolution-of-aws-site-to-site-vpn-part-1/","section":"Posts","summary":"The necessity for protocols to keep communication secure has been around since the dawn of the internet. The first ever VPN was jointly developed by a vendor consortium (which included Microsoft) in 1996, and came in the form of Point-to-Point Tunneling Protocol. Although many are skeptical about the value of VPNs in 2022 and beyond, customer consumption of cloud provider VPN services have paved the way for additional features and exponential scale.\n","title":"Evolution of AWS Site-to-Site VPN - Part 1","type":"posts"},{"content":"Blowing out cloud spend is an easy thing to do. This McKinsey Report notes that 80% of enterprises consider managing cloud spend a challenge. I recently presented at the Cloud Security Alliance in Kansas City and had the opportunity to network with some tremendous DevOps and Security professionals. One excellent side conversation somehow transitioned to a deep discussion on better ways to understand cost implications in the era of infrastructure-as-code. Shouldn\u0026rsquo;t cost be someone else\u0026rsquo;s problem?\nIntro Cost is a Shared Responsibility # As many organizations continue shifting workloads to the cloud, the cost impacts the bottom line. The responsibility of cost-management now transcends the CIO and accounting straight down to individual engineers. If this sounds scary, fear not. It is an incredible opportunity in the making.\nWhere do Engineers Work? # Engineers do not work in spreadsheets, nor do they work with accounting software. Most software engineers work day-to-day in version control. Furthermore, the centralized teams managing cloud infrastructure more broadly live in this world as well. Version Control often employs an approval process before a pull request is merged, and infrastructure is provisioned. What if you could see the cost impact right here? Yes, right where you, the engineer live? This is what Infracost does.\nPrerequisites # First, follow the instructions found here to download and authenticate Infracost. This includes creating an org inside the platform, which is where you can fetch the API key. Then we need a quick way to spin up some small AWS instances and then quickly dial them up to more expensive options.\nAWS Configuration # I\u0026rsquo;ll be using the following Terraform configuration to build the AWS infrastructure. I\u0026rsquo;ll keep the dynamic portions of the configuration in a separate locals block so we can easily adjust for testing.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-2\u0026#34; } data \u0026#34;aws_ami\u0026#34; \u0026#34;ubuntu\u0026#34; { most_recent = true filter { name = \u0026#34;name\u0026#34; values = [local.instance.image] } owners = local.instance.owners filter { name = \u0026#34;virtualization-type\u0026#34; values = [\u0026#34;hvm\u0026#34;] } } resource \u0026#34;aws_vpc\u0026#34; \u0026#34;vpc\u0026#34; { cidr_block = \u0026#34;10.1.0.0/16\u0026#34; tags = { Name = \u0026#34;vpc-east1\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;subnet\u0026#34; { count = length(local.subnet_names) vpc_id = aws_vpc.vpc.id cidr_block = local.subnet_prefixes[count.index] tags = { Name = local.subnet_names[count.index] } } resource \u0026#34;aws_network_interface\u0026#34; \u0026#34;interface\u0026#34; { count = length(local.subnet_names) subnet_id = aws_subnet.subnet.*.id[count.index] tags = { Name = \u0026#34;primary-network-interface\u0026#34; } } resource \u0026#34;aws_instance\u0026#34; \u0026#34;instance\u0026#34; { ami = \u0026#34;${data.aws_ami.ubuntu.id}\u0026#34; count = length(local.instance_names) instance_type = local.instance.type network_interface { network_interface_id = aws_network_interface.interface.*.id[count.index] device_index = 0 } tags = { Name = local.instance_names[count.index] } credit_specification { cpu_credits = \u0026#34;unlimited\u0026#34; } } Let\u0026rsquo;s Test via CLI # First, let\u0026rsquo;s do a terraform plan against the following criteria:\n1 2 3 4 5 6 7 8 9 10 11 12 13 locals { instance = { type = \u0026#34;t2.micro\u0026#34; image = \u0026#34;ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\u0026#34; owners = [\u0026#34;099720109477\u0026#34;] } subnet_names = [\u0026#34;eeny\u0026#34;] instance_names = [\u0026#34;catch\u0026#34;] subnet_prefixes = [\u0026#34;10.1.1.0/24\u0026#34;] } Running a Cost Estimate # Now, let\u0026rsquo;s run a cost estimate with Infracost and provision an instance:\nRun Estimate Once this is done, we can check out the results on the Infracost portal:\nValidate Estimate Running a Cost Diff # Bigger and more expensive is better, right? Let\u0026rsquo;s update our configuration with a few changes. Let\u0026rsquo;s switch that t2.micro to an m5.24xlarge, and let\u0026rsquo;s see how much it would cost to provision four of them:\n1 2 3 4 5 6 7 8 9 10 11 12 13 locals { instance = { type = \u0026#34;m5.24xlarge\u0026#34; image = \u0026#34;ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\u0026#34; owners = [\u0026#34;099720109477\u0026#34;] } subnet_names = [\u0026#34;eeny\u0026#34;, \u0026#34;meeny\u0026#34;, \u0026#34;miny\u0026#34;, \u0026#34;moe\u0026#34;] instance_names = [\u0026#34;catch\u0026#34;, \u0026#34;tiger\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;toe\u0026#34;] subnet_prefixes = [\u0026#34;10.1.1.0/24\u0026#34;, \u0026#34;10.1.2.0/24\u0026#34;, \u0026#34;10.1.3.0/24\u0026#34;, \u0026#34;10.1.4.0/24\u0026#34;] } This time, we will tweak the command to generate a diff:\nRun Diff Once we navigate back to the portal, we can see the cost change. I have gone from a monthly cost of $9.27 up to a panic-inducing $13,449. Maybe I don\u0026rsquo;t need the m5.24xlarge instances for this testing!\nValidate Diff Testing with GitHub Actions # In the spirit of shifting-left, let\u0026rsquo;s have a go with GitHub Actions running against every pull request. A lot of spending happens outside production, so this is an excellent way to control spending right at the source. If something doesn\u0026rsquo;t get deployed, it can\u0026rsquo;t cost you anything.\nGitHub Actions Workflow # By default, it will execute the typical Terraform init and plan. Then it will run terraform show -json plan.tfplan and save the output to plan.json. Then, Infracost can run its calculations. This will be populated in the conversation log along with everything else being tested as part of the pipeline.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 --- on: pull_request: paths: - \u0026#39;**.tf\u0026#39; - \u0026#39;**.tfvars\u0026#39; - \u0026#39;**.tfvars.json\u0026#39; jobs: infracost: runs-on: ubuntu-latest name: Show Infracost diff steps: - name: Check out repository uses: actions/checkout@v2 - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }} aws-secret-access-key: ${{ secrets.AWS_SECRET_KEY }} aws-region: us-east-2 - name: \u0026#34;Install terraform\u0026#34; uses: hashicorp/setup-terraform@v1 - name: \u0026#34;Terraform init\u0026#34; id: init run: terraform init working-directory: . - name: \u0026#34;Terraform plan\u0026#34; id: plan run: terraform plan -out plan.tfplan working-directory: . - name: \u0026#34;Terraform show\u0026#34; id: show run: terraform show -json plan.tfplan working-directory: . - name: \u0026#34;Save Plan JSON\u0026#34; run: echo \u0026#39;${{ steps.show.outputs.stdout }}\u0026#39; \u0026gt; plan.json - name: Run infracost diff uses: infracost/infracost-gh-action@master env: INFRACOST_API_KEY: ${{ secrets.INFRACOST_API_KEY }} GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: entrypoint: /scripts/ci/diff.sh path: . usage_file: infracost-usage.yml ... Creating a Pull Request # Once we create any pull request, the workflow will run and populate the cost details. At this point in the workflow, multiple approvals can be added. Three sets of eyes are better than one when spending so much with the click of a button! You can find the supported CICD platforms in the Infracost documentation.\nCICD Conclusion # Understanding TCO in the cloud is a deep topic that spans a whole organization. It can be easy to keep provisioning EC2 instances when you are disconnected from the cost. The ability to see the cost in the pipeline is a fantastic way to practice due diligence on the technical side of responsibility.\nArming engineers with the right tooling and knowledge will help drive cost-conscientious decisions. Small steps like this, along with driving continuous governance strategically, are significant steps in getting control of cloud spend.\n","date":"4 October 2022","externalUrl":null,"permalink":"/posts/2022/calculating-cost-like-a-devops-boss-with-infracost-and-aws/","section":"Posts","summary":"Blowing out cloud spend is an easy thing to do. This McKinsey Report notes that 80% of enterprises consider managing cloud spend a challenge. I recently presented at the Cloud Security Alliance in Kansas City and had the opportunity to network with some tremendous DevOps and Security professionals. One excellent side conversation somehow transitioned to a deep discussion on better ways to understand cost implications in the era of infrastructure-as-code. Shouldn’t cost be someone else’s problem?\n","title":"Calculating Cost Like a DevOps Boss with Infracost and AWS","type":"posts"},{"content":"","date":"4 October 2022","externalUrl":null,"permalink":"/tags/finops/","section":"Tags","summary":"","title":"Finops","type":"tags"},{"content":"","date":"4 October 2022","externalUrl":null,"permalink":"/tags/github-actions/","section":"Tags","summary":"","title":"Github-Actions","type":"tags"},{"content":"","date":"4 October 2022","externalUrl":null,"permalink":"/tags/infracost/","section":"Tags","summary":"","title":"Infracost","type":"tags"},{"content":"Optional attributes for object type constraints is almost here! I\u0026rsquo;ve been waiting for this feature to come along for a while. I have tested it extensively in -alpha, and I can confidently confirm that it is a game changer. This feature is long in the making, being discussed as far back as this thread in 2018. Today, it is now in beta, so the official release could be any day now. Let\u0026rsquo;s demonstrate how this is useful and build some common AWS infrastructure.\nWhy it is Useful # Before this feature, you could resort to using tricks to make arguments in object variables optional. This usually included providing a null value for optional parameters and then doing some fancy lookup or conditional like so:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 variable \u0026#34;some_var\u0026#34; { type = object({ name = string description = string } default = { name = \u0026#34;william\u0026#34;, description = null }) } locals { description = lookup(var.some_var, \u0026#34;description\u0026#34;, \u0026#34;no_description\u0026#34;) == \u0026#34;no_description\u0026#34; ? null : var.some_var } Now, we can make that individual attribute optional without any hackery involved. The first thing to know here is that when setting optional(string) without a default value as shown below, the default value is null.\n1 2 3 4 5 6 variable \u0026#34;some_var\u0026#34; { type = object({ name = string description = optional(string) }) } While having the default value automatically set to null is helpful, it only solves half of the problem. What happens if we have a scenario where we still want to provide a value in the logic, even if one isn\u0026rsquo;t supplied at runtime? Then we can set a default value with a second argument like this:\n1 2 3 4 5 6 variable \u0026#34;some_var\u0026#34; { type = object({ name = string description = optional(string, \u0026#34;Can\u0026#39;t stand Monopoly\u0026#34;) }) } Building some Infrastructure in AWS # This feature comes in handy when building complex data types. Let\u0026rsquo;s look at something as simple as building an AWS VPC. Your organization could be using VPC IPAM but may still need the option to pass in custom CIDRs at runtime. Or, other standard defaults may need to be set if not provided at runtime. Take the following example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 variable \u0026#34;vpc\u0026#34; { description = \u0026#34;Default VPC object\u0026#34; type = object({ name = string cidr_block = optional(string) ipv4_ipam_pool_id = optional(string) ipv4_netmask_length = optional(number) enable_dns_support = optional(bool, true) enable_dns_hostnames = optional(bool, false) instance_tenancy = optional(string, \u0026#34;default\u0026#34;) }) } If only the cidr_block attribute is provided at runtime, then the IPAM attributes will be nullified. This simplifies our resource configuration as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 resource \u0026#34;aws_vpc\u0026#34; \u0026#34;vpc\u0026#34; { cidr_block = var.vpc.cidr_block ipv4_ipam_pool_id = var.vpc.ipv4_ipam_pool_id ipv4_netmask_length = var.vpc.ipv4_netmask_length enable_dns_support = var.vpc.enable_dns_support enable_dns_hostnames = var.vpc.enable_dns_hostnames instance_tenancy = var.vpc.instance_tenancy tags = { Name = var.vpc.name } } Test it Yourself! # Want to start testing? Grab v1.3.0-beta1 and setup your versions.tf like this:\n1 2 3 4 terraform { required_version = \u0026#34;\u0026gt;= 1.3.0\u0026#34; experiments = [module_variable_optional_attrs] } Conclusion # AWS VPC is a simple example. This feature really shines when building reusable infrastructure-as-code for Network Firewall or even Network ACLs. Anything that simplifies something and reduces or eliminates any hacks required to reach a logical outcome is super valuable. Great work finally driving this one home HashiCorp.\n","date":"6 September 2022","externalUrl":null,"permalink":"/posts/2022/the-best-terraform-feature-yet/","section":"Posts","summary":"Optional attributes for object type constraints is almost here! I’ve been waiting for this feature to come along for a while. I have tested it extensively in -alpha, and I can confidently confirm that it is a game changer. This feature is long in the making, being discussed as far back as this thread in 2018. Today, it is now in beta, so the official release could be any day now. Let’s demonstrate how this is useful and build some common AWS infrastructure.\n","title":"The Best Terraform Feature Yet?","type":"posts"},{"content":"","date":"6 September 2022","externalUrl":null,"permalink":"/categories/tools/","section":"Categories","summary":"","title":"Tools","type":"categories"},{"content":"","date":"3 August 2022","externalUrl":null,"permalink":"/tags/fortinet/","section":"Tags","summary":"","title":"Fortinet","type":"tags"},{"content":"","date":"3 August 2022","externalUrl":null,"permalink":"/categories/security/","section":"Categories","summary":"","title":"Security","type":"categories"},{"content":"There is a reason why enterprises prefer the best-of-breed approach to connect and secure their network and intellectual property. Alkira announced its integration with Fortinet at AWS re:Inforce in July, and this is a perfect example of the best in action. As anyone that reads my blog knows, I have an automation first approach to everything. Alkira\u0026rsquo;s Terraform Provider is Fortinet ready, so let\u0026rsquo;s take it for a spin!\nIntro Key Features # This partnership comes packed with great features, including the seamless integration of FortiManager (which orchestrates the Fortinet Security Fabric), extending existing firewall zones into and across clouds with auto-mapping of zones-to-groups, and weathering traffic surges with auto-scaling.\nAlkira and Firewall State Coming from the Data Center world, configuring firewalls for high-availability has its challenges. Since firewalls are stateful, if traffic ingresses firewall-a and egresses firewall-b, you break state since firewall-b has no session. With Alkira, FortiGate instances run active/active and traffic symmetry is handled natively. Consistent hashing is inserted into the forwarding layer to detect failures and transition to available instances.\nThe Plan # For this exercise, I decided to include a mix of Hybrid Multi-Cloud and Multi-Region to play with, and why not cross continents too? I already have an SD-WAN fabric extended into East US and Central EU regions along with an IPSEC tunnel. For cloud, I have several networks connected from AWS and Azure with Multi-Cloud Internet Exit. This gives us plenty of options for selectively forwarding traffic to the FortiGates for east/west, north/south, and internet bound traffic.\nExisting Topology # Topology Scoping Criteria # First, let\u0026rsquo;s scope out the basics of what we want to deploy:\nFortiGates running FortiOS: 7.0.3 deployed in all regions Register with on-premises FortiManager Minimum: 2 / Maximum: 4 auto-scaling configuration Extend existing on-premises FortiGate zones across all clouds Policy Criteria # Second, let\u0026rsquo;s define what traffic we want to steer through the FortiGates and what should be left alone.\nDeny any-to-any by default Non-Prod can talk to Non-Prod directly but must pass through a FortiGate when talking to Prod Non-Prod can egress to the internet directly, but Prod must first pass through a FortiGate Partner can talk to migration cloud networks only but must first pass through a FortiGate Corporate can talk to all cloud networks but must first pass through a FortiGate The partner requirement above is pretty standard. As organizations look to modernize using the public cloud, often, they will work with a preferred partner to give them a strong start and avoid mistakes in the beginning that would otherwise set them back. These partners generally have access to the environments in scope for modernization only.\nWhy not forward everything to Firewalls? It may be tempting to forward all traffic through NGFWs. Meeting compliance requirements, especially when protected data is involved, can be challenging. However, it does not make sense when examining cloud principles and cost. Identify traffic required to transit firewalls if possible for compliance or other meaningful reasons. If you have sandbox cloud networks that have no access to data or any other intellectual property, does it make sense to forward all that traffic through a firewall?\nLet\u0026rsquo;s Build! # We could create separate resource blocks for each FortiGate we want to deploy, but that would be unsightly. For this example, let\u0026rsquo;s use [count](https:// to create multiple services and dynamic blocks to handle the nested schema for instances.\nSome Locals # We need to deploy the service twice since we are working across two regions. Let\u0026rsquo;s define names for the service, the regions we are deploying for, instance configurations for when auto-scaling occurs, and some values for policies. I have a separate variables.tf file with var type: list(map(string)) that I have the instance names and serials defined in, so I\u0026rsquo;m just looping through those here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 locals { // Need some names firewalls = [\u0026#34;us-forti\u0026#34;, \u0026#34;eu-forti\u0026#34;] // Need some regions regions = [\u0026#34;US-EAST-2\u0026#34;, \u0026#34;GERMANYWESTCENTRAL-AZURE-1\u0026#34;] // Need some instances (for when auto-scaling occurs) instances = { for instance in var.instances : try(\u0026#34;${instance.name}/${instance.serial}\u0026#34;) =\u0026gt; instance } // Define groups for policy from_groups = [\u0026#34;corp\u0026#34;] to_groups = [\u0026#34;nonprod\u0026#34;, \u0026#34;prod\u0026#34;, \u0026#34;migration\u0026#34;] // Filter into separate sets from_grp_ids = [ for v in data.alkira_group_connector.from_groups : v.id ] to_grp_ids = [ for v in data.alkira_group_connector.to_groups : v.id ] } Fortinet Configuration # Since we already have segments and groups provisioned, we can reference them in our main.tf file. This configuration will provision a Fortinet service per region, connect back to our FortiManager instance on-premises, map our existing firewall zones to Alkira groups for multi-cloud segmentation, and auto-scale to handle load elastically.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 // We already have a segment, let\u0026#39;s use it data \u0026#34;alkira_segment\u0026#34; \u0026#34;business\u0026#34; { name = \u0026#34;business\u0026#34; } resource \u0026#34;alkira_service_fortinet\u0026#34; \u0026#34;service\u0026#34; { count = length(local.firewalls) // Provision for each region with FortiOS 7.0.3 version = \u0026#34;7.0.3\u0026#34; name = local.firewalls[count.index] cxp = local.regions[count.index] // Configure auto-scaling + size min_instance_count = 2 max_instance_count = 4 auto_scale = \u0026#34;ON\u0026#34; size = \u0026#34;LARGE\u0026#34; // Licensing + credentials license_type = \u0026#34;PAY_AS_YOU_GO\u0026#34; management_server_ip = var.mgmt_server credential_id = alkira_credential_fortinet.auth.id // Segment + tunnel proto management_server_segment = data.alkira_segment.business.name segment_ids = [data.alkira_segment.business.id] tunnel_protocol = \u0026#34;IPSEC\u0026#34; // Handle nested schema for instances dynamic \u0026#34;instances\u0026#34; { for_each = { for instance in local.instances : instance.name =\u0026gt; instance } content { name = instances.value.name serial_number = instances.value.serial credential_id = alkira_credential_fortinet_instance.auth.id } } // Handle nested schema for segment_options dynamic \u0026#34;segment_options\u0026#34; { for_each = var.segment_options content { zone_name = segment_options.value.zone_name segment_id = data.alkira_segment.business.id groups = segment_options.value.groups } } } Policy Configuration # Now, we can selectively steer traffic to the FortiGates. Policy Resources can take a list of ids for source and destination groups. This is why I defined the to_groups and from_groups in local variables. We can simply loop through each set, return the IDs, and provide them as a single value to the policy resource.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 data \u0026#34;alkira_group_connector\u0026#34; \u0026#34;from_groups\u0026#34; { for_each = toset(var.from_groups) name = each.value } data \u0026#34;alkira_group_connector\u0026#34; \u0026#34;to_groups\u0026#34; { for_each = toset(var.to_groups) name = each.value } resource \u0026#34;alkira_policy\u0026#34; \u0026#34;policy\u0026#34; { name = var.name enabled = var.enabled from_groups = local.from_grp_ids to_groups = local.to_grp_ids segment_ids = [data.alkira_segment.segment.id] rule_list_id = data.alkira_policy_rule_list.forti.id } Validation # Let\u0026rsquo;s validate our policies to make sure they are meeting requirements. Alkira\u0026rsquo;s ability to visualize policy comes in handy when reviewing multi-cloud policy, traffic flows, and network security more broadly. Having the ability to integrate this into the DevOps Toolchain makes for a great experience.\nValidation Conclusion # Ever heard of that single product that solved all your organization\u0026rsquo;s network and security problems? Me neither! Valuable integrations like this one solve real problems. With the explosion of cloud services, it pays to zoom out and think about the long game. Building a strategy for your organization to support the legacy applications you can\u0026rsquo;t migrate, the applications on deck for migration to the cloud, and greenfield applications across multiple clouds, is a winning strategy. Alkira and Fortinet are two products that can help shift your focus to outcomes.\n","date":"3 August 2022","externalUrl":null,"permalink":"/posts/2022/terraforming-alkira-and-fortinet-is-multicloud-bliss/","section":"Posts","summary":"There is a reason why enterprises prefer the best-of-breed approach to connect and secure their network and intellectual property. Alkira announced its integration with Fortinet at AWS re:Inforce in July, and this is a perfect example of the best in action. As anyone that reads my blog knows, I have an automation first approach to everything. Alkira’s Terraform Provider is Fortinet ready, so let’s take it for a spin!\n","title":"Terraforming Alkira and Fortinet is Multicloud Bliss","type":"posts"},{"content":"I had the opportunity to present at the 4th annual AWS Community Day for the Midwest in June. This event was planned, organized, and delivered by AWS user group leaders and was an absolute blast. I got to catch up with a few remarkable individuals I haven\u0026rsquo;t talked to since pre-pandemic, and I got to meet many new people and listen to their stories of transformation in their respective enterprises.\nIntro Small Package, Major Home Run # AWS re:Invent is incredible, but it is a little like drinking from the firehose. So much to do, so many people to see, so many sessions, and a crowd like you\u0026rsquo;ve never seen. Community Day is smaller, intimate, and focused. CSCC, Mitchell Hall was used for the venue, which is an excellent facility for event hosting, and the Community Leaders did a fantastic job with organizing everything.\nMy Talk # My talk was centered around my real-life experiences with navigating network complexity as businesses dive into the digital transformation deep-end. You can grab the slides here. One thing that did surprise me was how many other professionals in attendance were going through precisely what I had gone through in their respective organizations. Legacy infrastructure and company culture are no respecter of persons. If you want to take a listen, here is the link: Other Great Sessions # Diagnosing Childhood Cancer with AWS Step Functions presented by Grant Lammi, was one of the major standout sessions for me. His talk focused on the difficulty of diagnosing and understanding why children have the cancers they do, what treatment is available, and how to get that treatment to them quicker. This involved a massive operation of sequencing the DNA of children throughout the United States. All the other sessions I attended were fantastic.\nConclusion # It was refreshing attending an event in-person again. There is a contagious energy that these events bring, and I found myself going home motivated with lots of new ideas. Everyone is in a different stage of their journey, but no matter where you are, you will find someone who emboldens you or others you can encourage. That, my friends, is community.\n","date":"25 July 2022","externalUrl":null,"permalink":"/posts/2022/aws-community-day-midwest/","section":"Posts","summary":"I had the opportunity to present at the 4th annual AWS Community Day for the Midwest in June. This event was planned, organized, and delivered by AWS user group leaders and was an absolute blast. I got to catch up with a few remarkable individuals I haven’t talked to since pre-pandemic, and I got to meet many new people and listen to their stories of transformation in their respective enterprises.\n","title":"AWS Community Day - Midwest","type":"posts"},{"content":"Zero Trust is all the rage lately, and traditional VPNs are getting a lot of scrutiny since they essentially add and remove encryption at the firewall. This means bad actors can skip off into the sunset (laterally) and gain access to those legacy systems with less effort. Another challenge with using a traditional VPN is scaling with the dramatic shift to hybrid work. ZeroTier is an interesting solution that claims to combine the capabilities of VPN and SD-WAN, among other things. Let\u0026rsquo;s take it for a spin.\nHow hard could Zero Trust be? Most tech these days supports HTTPS. How hard could Zero Trust be? Anyone that has managed certificates in a large enterprise can answer this question. It is more complicated than it may appear to be on the surface. And as you begin removing that complication, you start trusting identity providers and certificate authorities more broadly. One question that must be asked is, if you are using any number of Zero Trust solutions hitting the market, are they Zero Trust when you trust them to own everything?\nIntro Scenario # I know many people (myself included) that want to access devices on their home network while they are away. A solid example here is security cameras or maybe a lab environment that you can test with while traveling. I wouldn\u0026rsquo;t want to make my home cameras reachable directly from the internet.\nOne way you could solve this problem is by hosting an OpenVPN server on your network, so when you are away, you can fire up the mobile app, connect, and check out what is going on around your property. How would these scenarios work with ZeroTier? Let\u0026rsquo;s lab it up!\nFirst, What is ZeroTier? # ZeroTier is a Mesh VPN solution. Mesh VPNs facilitate connectivity directly between any two endpoints in the mesh without having to transit through a concentrator in a data center. Sessions between every pair of endpoints are encrypted end-to-end. If a bad actor can exploit a device, they only gain visibility into the traffic coming in and leaving that device, making lateral movement a challenge.\nZeroTier Network Hypervisor # ZeroTier\u0026rsquo;s network hypervisor is a custom protocol that implements a VXLAN like ethernet virtualization across two layers:\nVirtual Layer 1 (VL1): is the underlying peer-to-peer transport layer handling encryption, endpoint authentication, and credential verification Virtual Layer 2 (VL2): overlays boundaries, multicast, rule enforcement, and certificate-based access control on (VL1) Protocol Design Want to dig deeper into ZeroTier\u0026rsquo;s Network Hypervisor? Check out the Protocol Design Whitepaper for a transparent explanation of ZeroTier\u0026rsquo;s services, apps, and libraries. Remember, sufficiently advanced technology is like magic until you read the docs!\nWhat is OPNsense? # OPNsense OPNsense is an open-source security platform based on FreeBSD. I have used it historically as an entry point into various lab environments and as a customer-side appliance to terminate site-to-site VPNs for various public cloud providers. Since the platform offers LDAP and 2FA, extending access to others for collaboration is simple. OPNsense offers a ZeroTier Plugin and supports dynamic routing via FRR.\nWhy are we using a Firewall? You may be asking, why are we installing this on a firewall given the peer-to-peer architecture of ZeroTier? By installing ZeroTier on the firewall, it acts as a bridge of sorts between all the devices on my test network and the outside ZeroTier members I authorize. In doing this, I don\u0026rsquo;t have to install the ZeroTier client on each device I want to talk to when operating remotely.\nSetting up ZeroTier # First, we need to sign-up for a ZeroTier Account. Each VL2 network is identified by a 64-bit (16 hex digit) ID. Once our email is confirmed, we can create a network, add a new friendly name + description, and retrieve the Network ID.\nZT Network Installing ZeroTier Plugin # Next, let\u0026rsquo;s install the ZeroTier plugin on the OPNsense instance in front of the lab.\nPlugin Configuring OPNsense # Now, we need to configure the ZeroTier service on OPNsense. First, we must enable the service, and then we can add the network using the Network ID generated when we created the network in the ZeroTier portal.\nConfigure OPNsense Authorizing New Members # After we add the network in OPNsense, we can hop back to the ZeroTier portal and authorize it. Since I want to enable access for others to this lab environment, I\u0026rsquo;ll be setting up a managed route with the destination as the RFC1918 space of the lab with the OPNsense instance as the gateway. To prevent any future confusion, I configured Do Not Auto-Assign IPs when authorizing, and configured the managed IP statically.\nAuthorize Assigning the ZT Interface # If you navigate to Interfaces \u0026gt; Assignments, you should see a new interface populated. We need to add this interface and assign a static IPv4 address using the managed IP we set in the ZeroTier portal.\nAssign Interface Adding a Managed Route # To route traffic beyond the OPNsense instance, we need to add a managed route in ZeroTier. For the destination, we will use the RFC1918 space for the lab environment. For the gateway (Via), we will use the managed IP we set for OPNsense in the ZeroTier portal.\nManaged Routes Allowing Traffic on OPNsense # To allow communication from other ZeroTier members into our lab environment, we need to add a firewall rule on the ZeroTier interface in OPNsense. Of course, since this is a blog, I\u0026rsquo;m committing the cardinal sin of inserting an any-to-any rule. Avoid doing this as a general practice!\nRules Without the Firewall ZeroTier uses UDP hole punching, which essentially tricks the router into allowing access to a port on a device directly without establishing a TCP session. Since firewalls are stateful, they keep track of connections. Since UDP doesn\u0026rsquo;t establish a session, there is only a concept of inbound/outbound packets on the firewall. This means the firewall has to rely on limited data to let returning packets through (UDP 5-tuple data).\nConnecting from Client Devices # You can find the ZerTier One application on Google Play or the Apple Store. Once the app is downloaded, you can provide the network ID and click Join Network to connect. Downloads for other platforms can be found here.\nZeroTier SDK # Want to enable a mobile app to join a network without requiring special permissions on the phone or adding a tun/tap port? The ZeroTier SDK takes the network hypervisor and couples it with an embedded TCP/IP stack which you can link into an application library. This essentially moves the network stack into the application itself.\nConclusion # Mesh VPNs offer a thought-provoking alternative to traditional hub-and-spoke designs. It would be interesting to see what a large-scale deployment would look like in the typical risk-averse enterprise and how the migration path would play out. Tailscale seems to be the biggest competitor in this space and more or less does the same thing, just using different protocols. While ZeroTier\u0026rsquo;s protocol is proprietary, Tailscale uses Wireguard for its data plane. I look forward to seeing how remote work evolves over the next few years.\n","date":"10 July 2022","externalUrl":null,"permalink":"/posts/2022/exploring-zerotier-for-remote-access/","section":"Posts","summary":"Zero Trust is all the rage lately, and traditional VPNs are getting a lot of scrutiny since they essentially add and remove encryption at the firewall. This means bad actors can skip off into the sunset (laterally) and gain access to those legacy systems with less effort. Another challenge with using a traditional VPN is scaling with the dramatic shift to hybrid work. ZeroTier is an interesting solution that claims to combine the capabilities of VPN and SD-WAN, among other things. Let’s take it for a spin.\n","title":"Exploring ZeroTier For Remote Access","type":"posts"},{"content":"","date":"10 July 2022","externalUrl":null,"permalink":"/tags/zerotier/","section":"Tags","summary":"","title":"Zerotier","type":"tags"},{"content":"","date":"16 June 2022","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"AWS Community Day - Cruising Through Network Complexity","type":"talks"},{"content":"","date":"16 May 2022","externalUrl":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure","type":"tags"},{"content":"Anyone that has worked in tech knows that building greenfield is much easier than dragging along brownfield environments through a roller-coaster they aren\u0026rsquo;t ready for. Tools like Terraform make infrastructure-as-code a breeze, but what about all that infrastructure you already have provisioned? April Edwards, Cloud Advocate at Microsoft, recently posted a blog about Azure Terrafy, a new tool in preview which aims to simplify the process. You can find the original blog here. Let\u0026rsquo;s take it for a spin!\nIntro Can\u0026rsquo;t you already import resources into Terraform state? Terraform Import enables the importing of infrastructure today, but this only imports one resource at a time, and it\u0026rsquo;s on you to build the configuration files. Sounds time-consuming, right? When you are in the process of learning Terraform, having a tool that can import brownfield environments into state while also generating the HCL files, is extremely useful.\nInstalling Terrafy # I installed it via go install github.com/Azure/aztfy@latest.\nInstall Terrafy Building some Infra # To kick off this little experiment, I will build some infrastructure in Azure using Terraform and then create a different project directory to test Terrafy in. If things go south, I don\u0026rsquo;t have to remove everything manually.\nmain.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 locals { subnet_names = [\u0026#34;eeny\u0026#34;, \u0026#34;meeny\u0026#34;, \u0026#34;miny\u0026#34;, \u0026#34;moe\u0026#34;] vm_names = [\u0026#34;catch\u0026#34;, \u0026#34;tiger\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;toe\u0026#34;] subnet_prefixes = [\u0026#34;10.5.1.0/24\u0026#34;, \u0026#34;10.5.2.0/24\u0026#34;, \u0026#34;10.5.3.0/24\u0026#34;, \u0026#34;10.5.4.0/24\u0026#34;] } resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;rg\u0026#34; { name = \u0026#34;rg-terrafy-test\u0026#34; location = \u0026#34;eastus\u0026#34; } resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;vnet\u0026#34; { name = \u0026#34;vn-terrafy-test\u0026#34; address_space = [\u0026#34;10.5.0.0/20\u0026#34;] resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;subnet\u0026#34; { count = length(local.subnet_names) name = local.subnet_names[count.index] resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = [local.subnet_prefixes[count.index]] } resource \u0026#34;azurerm_network_interface\u0026#34; \u0026#34;interface\u0026#34; { count = length(local.subnet_names) name = \u0026#34;vnic-${local.subnet_names[count.index]}\u0026#34; location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name ip_configuration { name = \u0026#34;vnic-config\u0026#34; private_ip_address_allocation = \u0026#34;Dynamic\u0026#34; subnet_id = azurerm_subnet.subnet.*.id[count.index] } } resource \u0026#34;random_id\u0026#34; \u0026#34;id\u0026#34; { keepers = { resource_group = azurerm_resource_group.rg.name } byte_length = 8 } resource \u0026#34;random_string\u0026#34; \u0026#34;str\u0026#34; { length = 16 special = true override_special = \u0026#34;/@£$\u0026#34; } resource \u0026#34;azurerm_storage_account\u0026#34; \u0026#34;storage\u0026#34; { name = random_id.id.hex account_tier = \u0026#34;Standard\u0026#34; account_replication_type = \u0026#34;GRS\u0026#34; resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location } resource \u0026#34;azurerm_linux_virtual_machine\u0026#34; \u0026#34;vm\u0026#34; { count = length(local.vm_names) name = local.vm_names[count.index] admin_username = \u0026#34;ubuntu\u0026#34; admin_password = random_string.str.result resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location network_interface_ids = [azurerm_network_interface.interface.*.id[count.index]] size = \u0026#34;Standard_B1s\u0026#34; os_disk { caching = \u0026#34;ReadWrite\u0026#34; storage_account_type = \u0026#34;Standard_LRS\u0026#34; } source_image_reference { publisher = \u0026#34;Canonical\u0026#34; offer = \u0026#34;UbuntuServer\u0026#34; sku = \u0026#34;18.04-LTS\u0026#34; version = \u0026#34;latest\u0026#34; } disable_password_authentication = false boot_diagnostics { storage_account_uri = azurerm_storage_account.storage.primary_blob_endpoint } } Running Terrafy # I created an empty directory called terrafy-test for this exercise. Before running Terrafy, make sure azure-cli is installed and authenticated. If your running on macOS and use Homebrew, you can run brew update \u0026amp;\u0026amp; brew install azure-cli and then az login.\nRun Terrafy Missing Resources Like a boss, I ran with defaults and didn\u0026rsquo;t bother reading any of the well-crafted README. Most of the resources were translated fine, but I noticed the virtual machines and storage accounts didn\u0026rsquo;t make it. When going through the setup, you will be able to cycle through and view all the resources available to import. If you see (Skip), that resource will indeed get skipped unless you update the resource type accordingly.\nWas it Successful? # Let\u0026rsquo;s test a few things before we ride off into the sunset. I did a terraform plan -refresh only, and also updated some resources, and everything looks good!\nRefresh Some Thoughts # This is a great tool, especially in learning and adoption of infrastructure-as-code. The import process is scoped at the resource group level, which makes a lot of sense. Subscriptions would be the next level up, and that might get too messy. When Terrafy generates the infra-as-code files, it builds a provider.tf to source the azurerm details, and all the other configuration is generated in a monolithic main.tf file. This is the layout here:\nOutput More Organization # With infra-as-code, there is no easy button. To make things work at scale, things have to be organized. As a starting point, it would be neat to see Terrafy do more organization. Even something simple like breaking down infra into separate categories would be helpful.\nAn example of this would be taking resources like localNetworkGateways, networkSecurityGroups, and virtualNetworks and putting them into a separate file called network.tf as they all fall under Microsoft.Network when drilling down through azurerm.\nVirtual Machines and all compute-related components would fall under Microsoft.Compute and would be in a different file called compute.tf. Organizing things in categories like this would make it easy to sift through larger imports.\nSummary # Terrafy is a great way to bring existing infrastructure into Terraform state. Installation and usage are simple, and the documentation is good. I think this tool will gain enough momentum that new features will come quickly, hopefully on the organization side. To learn more, visit the GitHub page. Happy building!\n","date":"16 May 2022","externalUrl":null,"permalink":"/posts/2022/importing-infrastructure-with-azure-terrafy/","section":"Posts","summary":"Anyone that has worked in tech knows that building greenfield is much easier than dragging along brownfield environments through a roller-coaster they aren’t ready for. Tools like Terraform make infrastructure-as-code a breeze, but what about all that infrastructure you already have provisioned? April Edwards, Cloud Advocate at Microsoft, recently posted a blog about Azure Terrafy, a new tool in preview which aims to simplify the process. You can find the original blog here. Let’s take it for a spin!\n","title":"Importing Infrastructure With Azure Terrafy","type":"posts"},{"content":"","date":"23 February 2022","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"Alkira + HashiCorp Webinar: Modernized Network Delivered Through Fully Codified Infrastructure","type":"talks"},{"content":" ","date":"16 February 2022","externalUrl":null,"permalink":"/talks/2022/the-hedge-118/","section":"Talks \u0026 Media","summary":" ","title":"The Hedge: 118 - Integrating New Ideas with William Collins","type":"talks"},{"content":"The year is 2022, and Kubernetes is wreaking havoc on software delivery as we know it. Applications are going through modernization programs so they can be converted into microservices, but they are coming out the other end as distributed monoliths. Next thing you know, services exist across several clouds and even in your data centers. And, of course, the large majority of enterprises aren\u0026rsquo;t just greenfield.\nMany have a combination of technologies stacked over decades, including mainframes. Want some perspective? Check out this article on how COBOL still powers the Global Economy. In this blog, let\u0026rsquo;s examine the polarization of Kubernetes and microservices. Is it a zero-sum game, or is the real world destined to merge many different types of solutions in organized chaos forever? Before jumping into the world of the microservice, let\u0026rsquo;s visit our old friend, the monolith.\nMonolith : a single great stone often in the form of an obelisk or column : a massive structure : an organized whole that acts as a single unified powerful or influential force Merriam-Webster\nWhat Is Monolithic Architecture? # A monolithic application consists of a server-side application, database, and client-side interface, all of which are deployed as a single unit. You could even have a monolith in the cloud constructed with a cloud-native database, load balancer, and application running on virtual machines. The vital question to ask is, will rebuilding the application with a different architecture replace more problems than it is creating?\nMonolith When To Monolith # Of course, the answer to this is, it depends. Greenfield applications that need to scale horizontally can do so in full monolithic glory using a combination of infrastructure-as-code, image management tooling, and cloud-native services. Yes, fully automated, immutable, pipelines-on-pipelines, and reliability grandma would be proud of.\nAnd the best part about this is simplicity. In my experience, having something simple enough to match where an organization is at ends up being far more valuable than something too complicated to troubleshoot, understand, or upgrade. Technology affords us the means to simplify things, and in return, our brains tend to overthink and complicate it.\nA monolithic system\u0026rsquo;s initial simplicity can be appealing in many cases. Defining requirements, testing, and getting your MVP off the ground is quick. Some of the premiere companies built on microservices, like Netflix, started with a monolithic architecture. Check out this talk given by Ruslan Meshenberg on their journey:\nCriticism of Monolithic Architecture # Some of the everyday shade thrown towards monolithic architecture include:\nDiminished understanding over time of the application\u0026rsquo;s architecture as a whole Lack of modularity, thus limiting reuse or replacement of individual components Ability to make quick and reliable changes following increased growth + complexity Slower innovation coupled with a vastly competitive market; Risk of losing market share Scaling limitations; Entire application can scale, but not specific services Zooming Out Just because an application is monolithic doesn\u0026rsquo;t mean it\u0026rsquo;s legacy or even outdated. If you have a single monolith with clearly defined module boundaries, sound design, and the ability to scale to an organization\u0026rsquo;s needs, then why change anything at all?\nWhat Is Microservice Architecture? # Microservices position themselves as the paladin, leading the cause against the feared and long-lived tyrant, the infamous Monolith. The microservice architecture breaks up a given application into loosely coupled and independently deployable services. Each of these services has its own codebase, database, and management model. Kubernetes has emerged as the vehicle of choice for service orchestration. A common goal is to align those services to business capabilities with clearly defined context boundaries.\nMicroservice Context Boundaries In large enterprises, context boundaries are generally predicated on the domain-driven structures of the organization. Furthermore, these contexts forge the boundary for the size and scope of the service. Domain experts must collaborate with development and infrastructure teams to ensure success and cohesion across parallel domains, making this concept a big hurdle for many modernization efforts.\nWhen To Microservice # If a system can\u0026rsquo;t support exponential growth, thus failing to deliver on business goals, then microservices could be the answer (meaning they solve problems related to horizontal scale). In large organizations, new technologies can often be championed to solve human or organizational problems. This tends to snowball over time as the number of solutions and technical staff to operate them increases. Solving causal problems requires an accurate understanding of their causal structure. One of the best explanations on this I have heard came from Rob Brigham at re:Invent 2015:\nIn this talk, Rob recalls Amazon\u0026rsquo;s retail website back in 2001 and how it was a large architectural monolith. He then explains how this monolith became problematic and why breaking it into smaller pieces was the right solution. In Rob\u0026rsquo;s story, maintaining, testing, and deploying in smaller chunks solved the scaling challenges Amazon was facing.\nMicroservices Make Systems It should also be noted that although you can deploy microservices independently, this does not mean they are entirely decoupled. The goal is to produce a distributed system from a collection of services working harmoniously together for the good of the system as a whole. And although using microservice architecture helps manage and scale complex systems, it can also introduce many new and often unquantifiable complexities as a result.\nCriticism of Microservice Architecture # On GoTime#114, Kelsey Hightower talked at around 54:20 about one of his unpopular opinions which is, \u0026ldquo;Monoliths are the future\u0026rdquo;. You can read some of the excerpts here, but I would suggest going back and listening to the entire show to get the full context. Some of the common criticisms of microservice architecture include:\nTransitioning from monoliths require comprehensive understanding and refactoring Distributed nature adds complexity to testing; Increased overhead as services grow Time and cost associated with refactoring existing app portfolio to microservices Visibility and traceability challenges; New tooling required to manage sprawl Increased moving parts create a larger attack surface; New points of ingress/egress Relationship between services is often as efficient as collaboration between teams Is Something Broke? Enterprise Ops teams typically absorb a lot of data that drives metrics for the business. The real test comes when something is broken. How quickly can an operations team restore service? If it is not in their power to restore service, how fast can the fault be identified and escalated to the appropriate team that can take action? Debugging microservices often comes down to sifting through never-ending logs. Since many issues stem from failed integration between components, using methods like distributed tracing is necessary to understand the relationship between logs to identify issues between services.\nThings To Consider Before Changing Things Up # You get to work and sit down at your desk (or maybe roll out of bed and turn on your laptop). You attend your company\u0026rsquo;s Townhall presented by the CTO. This is the year of Digital Transformation. If Google can microservice on the kubernetes, so can we. It is time to modernize these legacy apps. What would I say if I had a few minutes to talk to this CTO?\nOrganizational Structure # Many enterprises today are still organized hierarchically, making the whole structure complex. This directly impacts the speed at which critical decisions are made, thus delaying work from being completed. In post-pandemic life, the ability to make quick and effective decisions is crucial at any rate, but especially when approaching modernized ways of delivering software. Finding ways to flatten the structure, clearly define the roles, and push empowerment for decision making lower can help with organizing for the future.\nDistributing The Monolith # In life, sometimes the human side of us tends to hammer a square peg into a round hole. Designed initially not to fit, we continue to take swings. If the hammer is big enough and the peg is as frail as many applications, you will get the peg into the hole. Now the operation of the object housing the peg may work, but not to its full potential.\nWhen it comes to transitioning monolithic applications into microservices, the result is often distributing the monolith. This type of application is deployed like a microservice but built like a traditional monolith. I have often seen this happen with applications hammered into the managed cloud provider kubernetes services. Are you in this boat? Start with these questions:\nAre any of our microservices sharing a datastore? Is our rock-star developer working across multiple microservices? Does making a change in one microservice require changes to others? Understanding The Cost # In this scenario, we retain some of the problems we were initially trying to escape. Changing a small chunk of the application still comes with redeploying the whole thing, the horizontal scale is not as efficient as it could be, and deployments are slow. Also, the compound interest begins to sting because we have the complexity of microservices.\nMany variables outside the application may impact the overall picture, including culture, infrastructure, adequate staffing, and technical talent. While no single solution can solve all problems, it stands to reason that a thorough understanding of where the culture and technology are should be the first step prior to planning any major transformational exercises. Attempting to modernize misunderstood and misrepresented applications with Kubernetes is not a strategy.\nConclusion # In my opinion, many legacy applications will continue to run in the enterprise space. Some will end up getting rehosted in the cloud, replatformed or decommissioned in favor of equivalent SaaS offerings. Others will go through large modernization initiatives coming out the other end as distributed monoliths running on kubernetes. Purely greenfield applications have a better chance at seeing real microservice architecture and reaping the benefits. Are any of these things wrong? I don\u0026rsquo;t think so.\nThe only constant is change. The ability to acknowledge that something isn\u0026rsquo;t working and promote the fail-fast mentality from the top will encourage new ideas, technologies, and practices that help keep an organization relevant. If there is a culture of fear, then innovative solutions from talented engineers will get sidelined. The monolith VS microservice conversation doesn\u0026rsquo;t matter. Delivering the best possible product to the customer with the resources available to you while delivering on business goals does.\nAcknowledgements # Big thanks to Kiran Dongara for taking the time to peer-review. Your wisdom and feedback are much appreciated!\n","date":"7 February 2022","externalUrl":null,"permalink":"/posts/2022/are-microservices-better-than-monoliths/","section":"Posts","summary":"The year is 2022, and Kubernetes is wreaking havoc on software delivery as we know it. Applications are going through modernization programs so they can be converted into microservices, but they are coming out the other end as distributed monoliths. Next thing you know, services exist across several clouds and even in your data centers. And, of course, the large majority of enterprises aren’t just greenfield.\n","title":"Are Microservices Better Than Monoliths?","type":"posts"},{"content":"","date":"7 February 2022","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"7 February 2022","externalUrl":null,"permalink":"/tags/microservices/","section":"Tags","summary":"","title":"Microservices","type":"tags"},{"content":"The public cloud continued to dominate spending in 2021. Gartner forecasts worldwide end-user spending for public cloud to reach $397.4 billion by 2022. With increased velocity; automation continues to be a critical business imperative for the enterprise. Getting automation right means getting all the appropriate teams pulled into the process early on (shift left). Lightlytics is a new SaaS product on the market that aims to make DevOps for cloud infrastructure as agile as software delivery.\nIntro What is Shifting Left? Shift left is the practice of getting adequate testing completed early, often, and with the right team engagement. Instead of cloud security practitioners being brought in to remediate an environment that is QA getting ready to go Prod, shifting-left would integrate security policy as early as initial development pull requests (or even commits) to version control. If you can\u0026rsquo;t merge risky infrastructure-as-code in the first place, then it will never make it to Prod!\nGetting Started # Lightlytics is a SaaS platform that empowers SRE + DevOps to automatically predict, pre-empt, and prevent failures, downtime, or business disruption caused by infrastructure. Getting started is pretty simple as they offer a 14-day free trial (no credit card required). Let\u0026rsquo;s take it for a spin!\nOverview Adding an AWS Account # To get started, you need to add an AWS account. This is done by simply providing your Account ID, choosing a Display Name, and selecting the AWS regions you have infrastructure deployed to. Lightlytics creates an IAM Role for Read Access using a CloudFormation Stack.\nTo keep your posture up-to-date in real-time, your account must have CloudTrail configured with a Management Events trail that applies to all regions. You can then enable real time collection of configuration events and updates (meaning Lightlytics stays up-to-date with infrastructure changes as they happen). This is enabled through an additional CloudFormation Stack.\nAccount A Common Scenario # Security-focused teams in enterprises generally want to decrease or eliminate direct internet exposure (especially from EC2 instances). Somehow, this still ends up happening, and reactive projects are scoped out to remediate. Let\u0026rsquo;s take this scenario through the paces with Lightlytics and see how it might prevent us from doing this.\nSome Risky Configuration # The following security group is referenced in my aws_network_interface configuration, which is attached directly to my EC2 instance. I also set up an aws_route with the destination CIDR of 0.0.0.0/0 to an IGW. This configuration will allow both ingress and egress internet traffic for the ec2 instance.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;allow-all\u0026#34; { name = \u0026#34;allow-all\u0026#34; description = \u0026#34;Allow all ingress/egress\u0026#34; vpc_id = aws_vpc.vpc.id ingress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] ipv6_cidr_blocks = [\u0026#34;::/0\u0026#34;] } } Security Today is Reactive One trend I\u0026rsquo;ve noticed with cloud security in the enterprise is, response to risky configuration seems to be reactive. Some bad configuration happens over a period of time followed by an audit. The audit yields actionable insights, and a remediation effort is planned. It is never easy remediating production infrastructure as an outage is usually incurred. How could we get security in the loop to get eyes on and understand the scope of the change before it ever gets any legs?\nSimulation with GitHub Actions Integration # This GitHub Actions workflow is slightly modified from the Lightlytics Documentation. Every time an attempt is made to merge changes to infrastructure into main, this workflow will kick-off and simulate the proposed changes in Lightlytics. This works by executing the Terraform Plan and sending the plan output to Lightlytics with each designated trigger (push/pull request). A link is generated directly to the simulation in the gitflow. In a subsequent release, Lightlytics plans to release the ability to automatically fail pull requests if a violation is detected.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 --- # Modified from https://docs.lightlytics.com/docs/github-action name: simulation on: push: branches: - main pull_request: jobs: terraform-simulation: runs-on: ubuntu-latest name: Lightlytics steps: - uses: actions/checkout@v2 - uses: hashicorp/setup-terraform@v1 with: terraform_wrapper: false # Additional step added to setup AWS credentials - name: Setup Credentials run: | mkdir -p ~/.aws echo \u0026#34;[default]\u0026#34; \u0026gt; ~/.aws/credentials echo \u0026#34;aws_access_key_id = ${{ secrets.ACCESS_KEY }}\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials echo \u0026#34;aws_secret_access_key = ${{ secrets.SECRET_KEY }}\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials - name: Terraform Plan run: | terraform init terraform plan -lock=false -out ./terraform.plan terraform show -json ./terraform.plan \u0026gt; ./plan.json # Converted collection-token to GH secret - uses: lightlytics/publisher@v1.1 id: ll-publisher with: plan-json: ./plan.json ll-hostname: ${{ secrets.LIGHTLYTICS_HOSTNAME }} collection-token: ${{ secrets.LIGHTLYTICS_TOKEN }} github-token: ${{ secrets.GH_TOKEN }} ... Entering DevSecOps # This is valuable because you could enforce default protections on every new git repository you create (complete with security approvers). The security practitioner could then click on the link and review an easy-to-follow simulation outlining the impact radius before approving and merging the changes. Can you say DevSecOps?\nActions Discovering Brownfield # Things have a habit of slipping through the cracks. Lightlytics Discovery decomposes all the dependencies between services, containers, or other common infrastructure you may have deployed. What if I need to find all EC2 instances that have internet ingress or egress in an unsupported design? I can narrow this criterion down using Search Paths in the Discovery dashboard. Using Internet as the source, I can set the Destination as a Resource Type (In this example, I use EC2).\nDiscovery Why use Lightlytics? # There are a lot of great open-source projects on the market that work by scanning Terraform HCL or Plan files that can accomplish most of what was outlined in this blog. Why would I turn to something like Lightlytics?\nBarrier to Entry # There is a reason SaaS is the largest market segment (public cloud services). Lightlytics requires minimal setup while making quick and easy work of integrating with version control and adding immediate value with thoughtful change review. The more open-source you go, the more customization is required to get the desired results. Furthermore, you must take steps to understand the maintainers of an open-source project, how it gets funded, and possibly assess the risk of it potentially being abandoned.\nGoing beyond Terraform State # Many products I\u0026rsquo;ve tested for static code analysis are effective only with infrastructure that is managed in Terraform State. Lightlytics takes this further by simulating changes against the entire infrastructure contained in the AWS account. If I have a brownfield environment deployed outside of Terraform State, I want to make sure the influx of new changes doesn\u0026rsquo;t negatively impact it.\nAgentless Approach # In reading my past blogs, you\u0026rsquo;ll know I\u0026rsquo;m partial to approaching as many problems as possible without agents or appliances, especially in the cloud. In the CSPM space, products generally use some combination of API calls, cloud logs, proprietary agents, or appliances. Lightlytics uses a combination of API calls along with integrations to cloud native features like CloudTrail and even VPC Flow Logs to add data-plane context.\nWith this approach, there is no reliance on scheduled or periodic scans of your infrastructure or git repositories. The posture is updated as changes happen. The following .gif was taken as I ran a terraform apply. Events were populating on the Lightlytics dashboard as they were completed in the Terraform plan output in real time. There is value here in that no gap exists between new infrastructure being provisioned and operational posture getting updated. This is what makes the Simulation piece compelling since you know that it will be running against a completely up-to-date picture of your entire infrastructure.\nPosture Why Agentless? In some circumstances, agents are a must. This holds true especially when an in-depth perspective into an asset\u0026rsquo;s OS, kernel, and processes is required. As the shift to immutable infrastructure continues, the need for this is minimized as resources like VMs are not long-lived. Leveraging cloud native API calls and logging provides a seamless union allowing for better integration and correlation with native provider automation and enforcement mechanisms.\nConclusion # For infrastructure-as-code, taking a proactive approach by shifting-left and catching things in the build pipeline is the ultimate security. Today, many products tend to be reactive which seems to be the modus operandi of security. Lightlytics has created a solid foundation that can provide value to CloudOps and Cloud SecOps teams that want to go fast without leaving availability and security behind.\nAs of writing this, Lightlytics has support for AWS but has multi-cloud on the roadmap. In addition to supporting additional clouds, the team is working to incorporate the ability to enforce custom-made, industry best practices, and business logic (architectural standards) as part of the GitOps flow. You can learn more about the vision and team here.\n","date":"4 January 2022","externalUrl":null,"permalink":"/posts/2022/shifting-left-with-lightlytics/","section":"Posts","summary":"The public cloud continued to dominate spending in 2021. Gartner forecasts worldwide end-user spending for public cloud to reach $397.4 billion by 2022. With increased velocity; automation continues to be a critical business imperative for the enterprise. Getting automation right means getting all the appropriate teams pulled into the process early on (shift left). Lightlytics is a new SaaS product on the market that aims to make DevOps for cloud infrastructure as agile as software delivery.\n","title":"Shifting Left with Lightlytics","type":"posts"},{"content":"","date":"9 November 2021","externalUrl":null,"permalink":"/tags/cryptocurrency/","section":"Tags","summary":"","title":"Cryptocurrency","type":"tags"},{"content":"New to cryptocurrency? Check this out for a quick primer. One of the giant debates, often a significant source of criticism, is Bitcoin\u0026rsquo;s energy efficiency (or lack thereof). This criticism may also be expanded to any other currency, or blockchain backed technology that leverages proof-of-work as a consensus mechanism. Innovation is inherent in technology. New ideas, methods, and optimizations come along to take something, add new features, and make it more efficient. One such optimization is proof-of-stake which brings a greener look to consensus. How much can it help?\nYour browser cannot play this video. Download video.\nBitcoin - Energy Consumption # The Digital Assets Programme (DAP) Team at The Cambridge Centre for Alternative Finance maintains a fantastic project called The Cambridge Bitcoin Electricity Consumption Index (CBECI) that estimates the power demand for the Bitcoin network. The material presented gets updated every 24-hours. Today, the estimated annualized consumption of the Bitcoin Network is \u0026gt; 111 TWh. A terawatt-hour is one trillion watts per hour, equal to 3.6x1015 Joules.\nInfo The CBECI hypothesizes the consumption of the Bitcoin Network based on this methodology. The actual Bitcoin electrical consumption is impossible to measure based on several external factors that the methodology cannot include.\nHistorical - Bitcoin Network Power Demand (Daily) # Demand Historical - Bitcoin Electricity Consumption (Monthly) # Consumption Bitcoin Mining Map (June-July 2021) # The following map visualizes monthly hashrate by country and region. Notice something different from June -\u0026gt; July 2021? To zoom in further, in September 2019, China accounted for 75% of the world\u0026rsquo;s Bitcoin mining energy consumption. Following China\u0026rsquo;s ban of trading and mining crypto, we see the U.S. become the number one hot spot.\nHashrate Comparison - Countries And Data Centers (Globally) # Let\u0026rsquo;s put Bitcoin\u0026rsquo;s energy footprint into perspective by comparing it to some other things that we have a better grasp on.\nName Population (Million) Annualized (TWh) China \u0026gt; 1,446 \u0026gt; 6,000 United States \u0026gt; 333 \u0026gt; 3,000 Data Centers (globally) - \u0026gt; 200 Bitcoin Network - \u0026gt; 115 Netherlands \u0026gt; 17 \u0026gt; 113 Aldous Huxley There are things known, and there are things unknown, and in between are the doors of perception.\nElon Musk Tweets # Nothing can move mountains and markets quite like Elon Musk tweets. The very purpose of Tesla\u0026rsquo;s existence is to accelerate the world\u0026rsquo;s transition to sustainable energy, as noted in the 2020 - Tesla Impact Report which is why this tweet comes as no surprise.\nView post on X (@elonmusk) Consensus Mechanisms # Blockchain backed technologies like Bitcoin are essentially distributed databases which are bound by the laws of CAP theorem. With blockchain, nodes must reach an agreement on the current state of the network by using a consensus mechanism.\nWhat Is Consensus With Blockchain? # Consensus means agreement. With blockchain, that agreement happens when 51% of the nodes on a network agree on its next global state. Consensus mechanisms (or algorithms) in a crypto-economic system enable networks to achieve this while preventing common attacks. A successful exploit, in theory, would compromise consensus by controlling \u0026gt;= 51% of the network. Consensus mechanisms exist to negate that 51% attack.\nProof-of-Work # Proof-of-Work (PoW), initially introduced in 1993 by Cynthia Dwork and Moni Naor to mitigate DDoS attacks, is the consensus mechanism used by many cryptocurrencies today like Bitcoin and Ethereum. This algorithm sets the difficulty and rules while the miners perform the actual work of adding valid blocks to the chain. Mining cryptocurrency requires specialized hardware that uses a substantial amount of electricity. Also, as difficulty increases, so does the computational power necessary to get the hash value. The penalty for bad actors submitting invalid blocks is losing computational power, energy, and time.\nReflection Zooming in further reveals that the mechanics of the underlying consensus mechanism underpins the need for the vast amount of energy required to power Bitcoin\u0026rsquo;s network (or any other currency leveraging proof-of-work). Also, as difficulty and computation increase, mining pools begin to emerge as a centralized authority of sorts.\nProof-of-Stake # Proof-of-Stake (PoS) was introduced in 2012 by Sunny King and Scott Nadal as an alternative to proof-of-work. At its core, proof-of-stake works to help reach consensus securely but does it in a fundamentally different way than proof-of-work. In a proof-of-stake system, miners don\u0026rsquo;t need to spend electricity on adding valid blocks to the chain, thus allowing the network to operate with substantially lower resource consumption. The penalty for bad actors on a proof-of-stake network is more severe. The validators\u0026rsquo; staked funds serve as the incentive to refrain from malicious intent. If a validator accepts a bad block, a portion of their staked funds will be slashed as a penalty.\nComparing PoW and PoS # Proof-of-Work Proof-of-Stake Miners compete with hardware to create new blocks full of processed transactions Validators are randomly selected based on their stake in the network (how many coins they hold) Rewards systems, like Pay Per Last N Shares (PPLNS) compensate miners Validators do not receive block rewards; Transaction fees are collected as compensation Additional currency created as rewards for miners; Contributes to price volatility Decreasing need to add coins to circulation; May contribute to price stability Adding a malicious block requires a node more powerful than 51% of the network Adding a malicious block would require owning at least 51% of all the cryptocurrency on the network Low transactions-per-second High transactions-per-second Susceptible to Tragedy of the Commons Susceptible to Nothing at Stake Trade-Offs? In a proof-of-work system, Tragedy of the Commons predicts a possible future in which the cost of mining becomes less lucrative when the block reward allowance becomes inconsequential. At this point, any rewards ultimately shift to transaction fees, thus opening the door to a potential 51% attack. In proof-of-stake systems, the Nothing at Stake problem, in theory, can disrupt consensus and make the system vulnerable to exploits like double-spend attacks.\nGoing Green # Depending on one\u0026rsquo;s definition of going green, that wording could be a mild stretch. One thing is for sure, optimizing to use less energy is a win. What if cryptocurrencies, like Ethereum, could use ~99.95% less energy by transitioning to proof-of-stake for consensus?\nEthereum - Energy Consumption And Scale # Ethereum, the second-largest coin by market capitalization, was launched in 2015 by Vitalik Buterin. While Bitcoin, long-term, could be viewed more as a store of value similar to gold, Ethereum offers more utility with its smart contracts driving the decentralized finance (DeFi) and NonFungible Token (NFT) arena. Today, Ethereum uses proof-of-work for consensus, which presents limitations at scale. Not only does it consume a country\u0026rsquo;s worth of power, but the blockchain also struggles to keep up due to popularity and demand. Smart Contracts, NFT minting + sales, and DeFi transactions increasing have caused a spike in fees.\nEthereum - Estimated TWh Today (Annualized) # TWh Comparison To Other Countries Today # Comparison Solving For Power And Scale With Ethereum 2.0 # Knowing the scalability and environmental challenges that come with proof-of-work, the Ethereum team has long-planned to upgrade to proof-of-stake consensus across 3 phases, the first of which, The Beacon Chain was implemented in Dec. 2020. The second phase, The Merge is slated for ~Q2 2022 at which point, staked Ether will take the place of mining. The last phase, Shard Chains, could potentially ship as early as late 2022 (depending on results of the merge), and greatly optimizes data and transactions.\nComparing PoW and PoS Power Requirements # A massive reduction in power can be observed when taking mining out of the picture. It is projected that Ethereum will use ~99.95% less energy post-merge. Therefore, instead of comparing the entire network\u0026rsquo;s consumption to a medium sized country, a more accurate comparison would be a small town of around 2,000 homes. Comparing PoW and PoS Scaling Transactions # Visa handles an average of 150 million transactions every day, or approximately 1,736 transactions-per-second. Today, Bitcoin handles somewhere between 4-7 transactions-per-second, with Ethereum coming in at around 13 transactions-per-second. The last phase of the shift to proof-of-stake introduces secure sharding, enabling Ethereum to create blocks simultaneously. According to Vitalik Buterin, this has the potential to scale up to ~100K transactions-per-second.\nView post on X (@VitalikButerin) Market Implications # Historically, precious metals were considered the go-to for hedging against inflation. Going into 2020, crypto has become an increasingly popular alternative. This is likely one of the contributing factors as to why Bitcoin outperformed all other major asset classes in 2020. How are things shaping up as we near the latter part of Q4 - 2021?\nQuote Inflation is when you pay fifteen dollars for the ten-dollar haircut you used to get for five dollars when you had hair.\n- Sam Ewing\nCrypto ROI VS Traditional Investments (2020-2021) # In 2020, cryptocurrency outperformed Gold and the S\u0026amp;P 500. Looking back over 2021, this gap has widened even further, with Ethereum starting to make big waves as it outperforms Bitcoin in price appreciation terms this year. Could this be resulting from the process and changes Ethereum is undergoing as it transitions to proof-of-stake?\nInvestments Wall Street Appeal # To me, it makes sense that Ethereum\u0026rsquo;s transition to proof-of-stake will garner increased interest from the financial establishment. I only say this because post upgrade, staked Eth will operate more like a digital bond. Instead of being a virtual commodity, it will be closer to a financial asset. Capital gets positioned, and then the yield VS price gets calculated. This makes it simple to regulate and even easier to tax (like a bond yield).\nConclusion # Cryptocurrency is still in its very early stages. Can proof-of-stake have an impact on the heavy energy demands of crypto today? From watching the news or reading any major publications, it would seem that the market is not so much fixated on functionality but on price. With Ethereum, transaction fees increase as the demand for Ethereum rises, which directly impacts the price. To me, this is a direct indication of its utility today, which drives the price higher.\nOne thing that remains unclear is, what happens to the second-largest coin by market capitalization as it completes this gigantic transition to proof-of-stake and takes on the persona of an interest paying bond or equity? Does this negatively impact the link between Ethereum price and transaction cost, or does increased utility and adoption along with significantly less energy demand drive Ethereum parabolic? The world will soon find out!\n","date":"9 November 2021","externalUrl":null,"permalink":"/posts/2021/crypto-energy-efficiency-and-proof-of-stake/","section":"Posts","summary":"New to cryptocurrency? Check this out for a quick primer. One of the giant debates, often a significant source of criticism, is Bitcoin’s energy efficiency (or lack thereof). This criticism may also be expanded to any other currency, or blockchain backed technology that leverages proof-of-work as a consensus mechanism. Innovation is inherent in technology. New ideas, methods, and optimizations come along to take something, add new features, and make it more efficient. One such optimization is proof-of-stake which brings a greener look to consensus. How much can it help?\n","title":"Cryptocurrency, Energy, and Proof-of-Stake","type":"posts"},{"content":"","date":"9 November 2021","externalUrl":null,"permalink":"/tags/proof-of-stake/","section":"Tags","summary":"","title":"Proof-of-Stake","type":"tags"},{"content":"","date":"9 November 2021","externalUrl":null,"permalink":"/series/understanding-cryptocurrency/","section":"Series","summary":"","title":"Understanding Cryptocurrency","type":"series"},{"content":"","date":"27 October 2021","externalUrl":null,"permalink":"/series/getting-started-with-alkira-and-terraform/","section":"Series","summary":"","title":"Getting Started With Alkira and Terraform","type":"series"},{"content":"In Part 1, we laid out our foundation. In Part 2 and Part 3 we connected various networks (both cloud and on-premises) and provisioned NGFWs that scale to real-time capacity. By default, networks connected to our corporate segment have full-mesh connectivity to each other. Let\u0026rsquo;s build some policies in code that can work with the groups we created to produce logical micro-segmentation that mirror a few real-world use cases.\nYour browser cannot play this video. Download video.\nScenario # Before moving on to policy requirements, let\u0026rsquo;s recap the infrastructure provisioned in the previous posts:\nDEV, TEST, STAGE, and PROD networks in AWS and Azure for cloud native applications Additional MIGRATION network in Azure for application migrations from on-premises DEV, TEST, and STAGE networks in GCP for a new product that is not yet in production Alkira Groups non-prod, prod, migration, internet, ipsec, and sdwan for micro-segmentation Internet Exit for users, sites, and clouds along with elastically scaled Palo Alto NGFWs Policy Requirements # Requirements I wanted these requirements to be grounded in reality. Segmenting application environments, isolating migration workloads, and selectively steering traffic to different services can be challenging when considering Hybrid Multi-Cloud. Defining these policies in code is a big step towards integrating NetSecOps into network and security workflows.\nWithout Terraform # Alkira makes it simple to define policies manually via the portal. A great benefit of doing it this way is to visualize the outcome of a given policy in real-time. To get a better idea of the process, let\u0026rsquo;s create a sample policy to allow communication from our Cisco SD-WAN mesh to the Azure migration zone:\nPolicy Building Policies With Terraform # Alkira uses a flexible policy-driven architecture for controlling traffic and enabling service insertion. These policies can be built and enforced without knowing the IP addresses or subnets that represent sites, cloud networks, or applications.\nResources # Name Type Description alkira_policy_prefix_list resource Manage Prefix Lists alkira_policy_rule resource Manage Rules alkira_policy_rule_list resource Manage Rule Lists alkira_policy resource Manage Policies Prefix Lists # Although defining subnets as resources isn\u0026rsquo;t required, it can make sense to categorize larger blocks of space together, like RFC1918 private address space.\nalkira_policy_prefix_list.tf\n1 2 3 4 5 6 7 8 resource \u0026#34;alkira_policy_prefix_list\u0026#34; \u0026#34;rfc1918\u0026#34; { // Prefix values name = \u0026#34;RFC1918\u0026#34; description = \u0026#34;Private Address Space\u0026#34; prefixes = [\u0026#34;10.0.0.0/8\u0026#34;, \u0026#34;172.16.0.0/12\u0026#34;, \u0026#34;192.168.0.0/16\u0026#34;] } Rules # Custom rules can be defined to control routing. For increased flexibility, rules allow matching on community, extended community, as path, and prefix list. You can also do service insertion by using service type to steer traffic through a given service. Palo Alto, CheckPoint, and ZScaler are supported services today.\nalkira_policy_rule.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 resource \u0026#34;alkira_policy_rule\u0026#34; \u0026#34;policy_rule\u0026#34; { // Policy rule values name = var.policy_rule_name rule_action = var.policy_rule_action src_prefix_list_id = var.src_prefix_list_id dst_prefix_list_id = var.dst_prefix_list_id src_ports = [var.src_ports] dst_ports = [var.dst_ports] dscp = var.dscp_value protocol = var.protocol rule_action_service_types = [var.service_types] } Selectively steering traffic to services like NGFWs in hybrid multi-cloud networks comes with many design considerations and challenges. In practice, this can often lead to physical and virtual firewall sprawl across CoLos, Public Clouds, and Sites. A great benefit of using Alkira is significantly limiting this firewall instance sprawl to only what is needed to meet capacity demands. Also, having a simple way of steering traffic as required saves time and simplifies design.\nPolicies # Policies contain a rule-list and get applied to connectors. Once an enforcement scaffolding is in place, newly added connectors will automatically inherit the intended policy defined with infrastructure-as-code.\nalkira_policy.tf\n1 2 3 4 5 6 7 8 9 10 11 12 resource \u0026#34;alkira_policy\u0026#34; \u0026#34;policy\u0026#34; { // Policy values name = var.policy_name description = var.policy_desc enabled = var.policy_status rule_list_id = var.rule_list_id from_groups = [var.src_group_id] to_groups = [var.dst_group_id] segment_ids = [var.segment_id] } Validation # I used Terraform Cloud for provisioning. From the design canvas, we can validate that each policy matches our original intent.\nValidation ONUG - Fall 2021 # I presented at ONUG - Fall 2021 on using infrastructure-as-code to build a production-grade hybrid multi-cloud network. The majority of what I covered in this blog series, I demonstrate in real-time using Github in tandem with Terraform Cloud\u0026rsquo;s Version Control Workflow. To get a better idea of how this looks, check out the session:\nConclusion # Automation is now a business imperative that underpins elasticity and intersects directly with business outcomes. In this blog series, we deployed a production-grade network spanning multiple clouds and sites with unified segmentation and security services, all via code and delivered using CI/CD. Keep a lookout for new content as I explore integrating Alkira with other tools and services!\nAcknowledgements # Big thanks to Ken Guo for taking the time to peer-review and offer such thoughtful insight.\n","date":"27 October 2021","externalUrl":null,"permalink":"/posts/2021/getting-started-with-alkira-and-terraform-part-4/","section":"Posts","summary":"In Part 1, we laid out our foundation. In Part 2 and Part 3 we connected various networks (both cloud and on-premises) and provisioned NGFWs that scale to real-time capacity. By default, networks connected to our corporate segment have full-mesh connectivity to each other. Let’s build some policies in code that can work with the groups we created to produce logical micro-segmentation that mirror a few real-world use cases.\n","title":"Getting Started With Alkira And Terraform (Part 4)","type":"posts"},{"content":"Check out Part 1 and Part 2 where we put together a scalable foundation and connect cloud networks from AWS, Azure, and GCP. For Part 3, we will bring on-premises back into the spotlight and connect some sites over Cisco SD-WAN and IPSEC.\nOn-premises remains a strong focus for many enterprises through 2021 going into 2022. Some workloads, as noted by Amazon CEO Andy Jassy, may never move to the cloud. While this may seem like a shocker, if continuing to run specific workloads on-premises makes sense from a cost or compliance standpoint, why should they move?\nScenario # Let\u0026rsquo;s expand on our network design layout from Part 2 and add some requirements for our on-premises networks. For this example, my SD-WAN fabric consists of two data centers + HQ in the East region which, will have connectivity extended to my East CXP. Three smaller sites will get connected directly via IPSEC.\nSites with \u0026lt;= 10 users will connect into Alkira\u0026rsquo;s CXP over IPSEC All other sites are on Cisco\u0026rsquo;s SD-WAN fabric and get extended into the CXP In the enterprise space, connectivity can be handled in numerous ways. For instance, many enterprises may want or already have AWS Direct Connect, specifically for its bandwidth and performance. Alkira makes it simple to leverage existing options to connect data centers and sites to Cloud Exchange Points meeting enterprise networking where it lives today and providing a path forward to elastic networks of the future.\nTopology Resources # Name Type Description alkira_credential_cisco_sdwan resource Provision Cisco SD-WAN Credential alkira_connector_cisco_sdwan resource Provision Cisco SD-WAN Connector alkira_connector_ipsec resource Provision IPSEC Connector Connecting On-Premises # A great deal of focus has been placed on the cloud over the past five years. However, for large enterprises, migrating and modernizing applications using the public cloud isn\u0026rsquo;t as simple as a project, program, or throwing lots of cash down to get it done in a year exercise. Playing the long-game is critical, and on-premises shouldn\u0026rsquo;t be ignored.\nTerraform Cloud # Like Part 1 and Part 2, I used Terraform Cloud for provisioning. I\u0026rsquo;m going to forgo explanation in this post to minimize repetition. To get a better understanding of how I organized things, check this out.\nIPSEC Connectors # IPSEC can be set up with static or dynamic routing via route based VPN mode. Connecting sites to Alkira consists of defining the connector along with each endpoint that should be associated with it.\nipsec_connector.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 resource \u0026#34;alkira_connector_ipsec\u0026#34; \u0026#34;connector\u0026#34; { // Connector values name = var.name cxp = var.region group = var.group size = var.size segment_id = var.segment_id vpn_mode = var.vpn_mode // Endpoint values endpoint { name = \u0026#34;Union Ave - ipsec\u0026#34; customer_gateway_ip = var.union_gw_ip } endpoint { name = \u0026#34;Summer St - ipsec\u0026#34; customer_gateway_ip = var.summer_gw_ip } endpoint { name = \u0026#34;Newport Ave - ipsec\u0026#34; customer_gateway_ip = var.newport_gw_ip } routing_options { type = var.routing_type customer_gateway_asn = var.cust_gw_asn } } Provision SD-WAN Connectors # Extending the Cisco SD-WAN fabric into Alkira consists of defining the connector, endpoints, and the bootstrap file from Cisco vManage. The SD-WAN fabric will then be extended into one or more Cloud Exchange Points, enabling regional hand-offs between the two.\ncisco_sdwan_connector.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 resource \u0026#34;alkira_credential_cisco_sdwan\u0026#34; \u0026#34;credential\u0026#34; { name = var.credential username = var.username password = var.password } resource \u0026#34;alkira_connector_cisco_sdwan\u0026#34; \u0026#34;connector\u0026#34; { // Connector values name = var.name cxp = var.region group = var.group size = var.size version = var.version // Endpoint values vedge { hostname = \u0026#34;hub\u0026#34; cloud_init_file = var.cloud_init credential_id = alkira_credential_cisco_sdwan.credential.id } vrf_segment_mapping { segment_id = data.alkira_segment.corporate.id vrf_id = data.alkira_segment.corporate.id } } Provision I used Cisco SD-WAN for this post. Today, all three node types (CSR, vEdge, and CAT8000v) are supported. Alkira continually adds new partner integrations, like Aruba EdgeConnect so keep a lookout for new product announcements\nConclusion # In Part 1, we built a scalable foundation, and in Part 2, we connected networks to that foundation from AWS, Azure, and GCP. This post brought data centers and remote offices into the picture over IPSEC and SD-WAN.\nNow that we have all of these networks connected, what about policy and service insertion? To be helpful to enterprises adopting cloud, limiting communication to and across clouds and on-premises is a given. Also, selectively steering specific traffic through VM-Series Firewalls should be a trivial task. In Part 4, we will define policies in code and use Alkira\u0026rsquo;s design canvas to validate.\n","date":"14 October 2021","externalUrl":null,"permalink":"/posts/2021/getting-started-with-alkira-and-terraform-part-3/","section":"Posts","summary":"Check out Part 1 and Part 2 where we put together a scalable foundation and connect cloud networks from AWS, Azure, and GCP. For Part 3, we will bring on-premises back into the spotlight and connect some sites over Cisco SD-WAN and IPSEC.\n","title":"Getting Started With Alkira And Terraform (Part 3)","type":"posts"},{"content":"In Part 1, we started with a scalable foundation that can adapt over time as the business grows and adjusts to changing markets. With Alkira\u0026rsquo;s Network Cloud, we take a cloud native approach in enabling our customer\u0026rsquo;s transformation. No appliances need to be provisioned in remote VPCs or VNets, and no agents need to be installed on workloads. Getting started is as easy as kicking off a build pipeline. For Part 2, let\u0026rsquo;s connect some networks from AWS, Azure, and GCP.\nScenario # In Part 1, we set up a hypothetical Line of Business called LoB - Digital, which has the following network requirements:\nCloud Native applications will be deployed in AWS; Application lifecycle requires DEV, TEST, STAGE, and PROD VPCs Azure gets the same network types as AWS for cloud native workloads; In addition, Azure will also get a MIGRATION VNet which will act as a landing zone for workloads being migrated from on-premises A new product surrounding data analytics is being established, and the product team wants to leverage GCP; The product is not production-ready, so only DEV, TEST, and STAGE VPCs are required Topology Since no appliances get installed inside cloud networks, how does Alkira interface with the cloud providers? Alkira takes the cloud native approach of using existing authentication methods in each cloud. For instance, in AWS, this would be IAM Policies and with Azure, Service Principals. Most enterprises are already interacting with the cloud this way today, so integrating with their existing automation + pipeline strategy is seamless.\nResources # We will be using the following Terraform Resources in this post:\nName Type Description alkira_credential data source Reference existing credential alkira_billing_tag data source Reference existing billing tag alkira_connector_aws_vpc resource Provision connector for AWS VPC alkira_connector_azure_vnet resource Provision connector for Azure VNet alkira_connector_gcp_vpc resource Provision connector for GCP VPC Connecting The Cloud # Alkira\u0026rsquo;s Terraform Provider does quick work of connecting cloud networks to our foundation. The following snippet will connect an Azure VNet, place it in the group we provide, and attach a billing tag.\nazure_connector.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // Source credential data \u0026#34;alkira_credential\u0026#34; \u0026#34;credential\u0026#34; { name = var.credential } // Source billing tag data \u0026#34;alkira_billing_tag\u0026#34; \u0026#34;tag\u0026#34; { name = var.billing_tag } // Provision connector resource \u0026#34;alkira_connector_azure_vnet\u0026#34; \u0026#34;connector\u0026#34; { // Azure values name = var.vnet azure_vnet_id = var.vnet_id azure_region = var.region // Alkira values cxp = var.cxp size = var.size group = var.group segment_id = var.segment_id billing_tag_ids = [data.alkira_billing_tag.tag.id] credential_id = data.alkira_credential.credential.id // Route table option routing_options = var.route_option } Organizing Things # HashiCorp recommends One Workspace Per Environment Per Terraform Configuration. Since we are provisioning and connecting so many networks + environments across all three cloud providers, I simplified a few things. With Azure, for example, repositories map to Workspaces like this:\nGit Provisioning # Like Part 1, we will use Terraform Cloud for provisioning. A successful merge to our main branch will automatically trigger a plan, and apply.\nProvision Validation # Twelve VPCs/VNets across three public clouds couldn\u0026rsquo;t be easier! By default, networks connected to our corporate segment have full-mesh connectivity to each other. Later in this series, we will build automated policies to work with our groups that produce some logical micro-segmentation.\nValidation Conclusion # For Part 1, we built a scalable foundation, and in this post, we connected networks from AWS, Azure, and GCP to it. One area where enterprises struggle is securely connecting their data center or remote offices to the cloud. This use-case often maps back to migrating workloads to the cloud or running Tiered Hybrid Workloads. In Part 3, we will connect a few on-premises networks into the mix to see how Alkira can help solve this problem.\n","date":"16 September 2021","externalUrl":null,"permalink":"/posts/2021/getting-started-with-alkira-and-terraform-part-2/","section":"Posts","summary":"In Part 1, we started with a scalable foundation that can adapt over time as the business grows and adjusts to changing markets. With Alkira’s Network Cloud, we take a cloud native approach in enabling our customer’s transformation. No appliances need to be provisioned in remote VPCs or VNets, and no agents need to be installed on workloads. Getting started is as easy as kicking off a build pipeline. For Part 2, let’s connect some networks from AWS, Azure, and GCP.\n","title":"Getting Started With Alkira And Terraform (Part 2)","type":"posts"},{"content":"HashiCorp\u0026rsquo;s Terraform needs no introduction. It is all but the de facto vehicle for delivering cloud infrastructure, and for a good reason. What Terraform did for Multi-Cloud Infrastructure as Code, is precisely what Alkira does for the network. What happens when you use these two platforms together to deliver networking in and across clouds? If providing network services in code faster than ever before sounds interesting, this multi-part series is for you. Need a quick primer on Alkira? You can read up here.\nIntro A Scalable Foundation # Troubleshooting networks that have been over-engineered can be tricky. Some of the most reliable networks I have worked in were also the most simple by design. Simple is easier to understand, automate, and scale. To begin this series, let\u0026rsquo;s deploy the following:\nA Segment for macro-segmentation; (We will deploy additional segments in a later post to demonstrate Partner and Acquisition connectivity scenarios) Groups for micro-segmentation; These will be used for policy enforcement once we start connecting cloud and on-premises networks A Billing Tag mapping to a hypothetical Line of Business (Let\u0026rsquo;s say Digital Transformation); Being able to bill specific app or product teams for network consumption is a game-changer An Internet Exit for users, sites, and clouds along with elastically scaled VM-Series Firewalls for our IPS, IDS, and ALG needs Segmentation Layout # Layout In Alkira, segments and groups are scaled globally, across clouds, and on-demand. To learn more about how Alkira handles segmentation, check out this design zone video.\nLet\u0026rsquo;s Build! # Segmentation # Alkira\u0026rsquo;s Terraform Provider makes it easy to build networking the DevOps way. The following snippet creates our segment and groups used for macro and micro segmentation.\nalkira_segments.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Groups locals { group = [\u0026#34;non-prod\u0026#34;, \u0026#34;prod\u0026#34;, \u0026#34;migration\u0026#34;, \u0026#34;ipsec\u0026#34;, \u0026#34;sdwan\u0026#34;, \u0026#34;internet\u0026#34;] } // Create segment resource \u0026#34;alkira_segment\u0026#34; \u0026#34;corp\u0026#34; { name = var.segment_name asn = var.segment_asn cidr = var.segment_cidr } // Create groups resource \u0026#34;alkira_group\u0026#34; \u0026#34;group\u0026#34; { count = length(local.group) name = local.group[count.index] description = \u0026#34;Group ${local.group[count.index]}\u0026#34; } // Create billing tag resource \u0026#34;alkira_billing_tag\u0026#34; \u0026#34;tag\u0026#34; { name = var.tag description = \u0026#34;Billing Tag ${var.tag}\u0026#34; } Internet Egress # The following code will provision an Internet Connector with VM-Series Firewalls. These scale to real-time capacity demand while symmetrically steering application traffic using intent-based policies.\nInternet, Intranet, and Private-Spoke security zones will be provisioned on the Firewalls with each zone mapping back to the corresponding Alkira Groups which provide the baseline for segmentation and policy\nalkira_internet.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 resource \u0026#34;alkira_connector_internet_exit\u0026#34; \u0026#34;connector\u0026#34; { // Connector values name = var.name cxp = var.cxp group = var.group size = var.size segment_id = var.segment_id } resource \u0026#34;alkira_service_pan\u0026#34; \u0026#34;ngfw\u0026#34; { // Service values name = var.service_name cxp = var.cxp size = var.size license_type = var.license_type type = var.pan_type version = var.pan_version management_segment_id = var.mgmt_segment_id segment_ids = [var.segment_id] credential_id = var.credential_id panorama_enabled = false max_instance_count = 1 // Instance values instance { name = var.instance_name credential_id = var.credential_id } // // Security zones // // All internet, untrusted zones_to_groups { segment_name = var.segment_name zone_name = \u0026#34;internet\u0026#34; groups = [\u0026#34;internet\u0026#34;] } // Branch, DC, CoLo (Private Network) zones_to_groups { segment_name = var.segment_name zone_name = \u0026#34;on-premises\u0026#34; groups = [\u0026#34;ipsec\u0026#34;, \u0026#34;sdwan\u0026#34;] } // Cloud Connectors zones_to_groups { segment_name = var.segment_name zone_name = \u0026#34;cloud\u0026#34; groups = [\u0026#34;non-prod\u0026#34;, \u0026#34;prod\u0026#34;, \u0026#34;migration\u0026#34;] } } Provision On Pull Request # I\u0026rsquo;m a big fan of Terraform Cloud. For this example, I have my infrastructure committed to Github with Terraform Cloud runs triggered automatically as changes get merged in version control.\nProvision Validation # Alkira has a very refined interface which serves as a great visual aid to validate configuration and policy. As we add more infrastructure later in this series, we will use the UI to gain a greater perspective into each aspect of the network.\nValidation Conclusion # The place where applications, data, and systems intersect is the network. Networks of the future must adapt and scale over time as businesses grow, markets change, and policies adapt. Elastic networking driven by automation is the future. In Part 2, we will connect a mix of AWS, Azure, and GCP networks to demonstrate just how easy Alkira can make the multi-cloud network experience, so stay tuned!\n","date":"10 September 2021","externalUrl":null,"permalink":"/posts/2021/getting-started-with-alkira-and-terraform-part-1/","section":"Posts","summary":"HashiCorp’s Terraform needs no introduction. It is all but the de facto vehicle for delivering cloud infrastructure, and for a good reason. What Terraform did for Multi-Cloud Infrastructure as Code, is precisely what Alkira does for the network. What happens when you use these two platforms together to deliver networking in and across clouds? If providing network services in code faster than ever before sounds interesting, this multi-part series is for you. Need a quick primer on Alkira? You can read up here.\n","title":"Getting Started With Alkira And Terraform - (Part 1)","type":"posts"},{"content":"Effectively automating infrastructure is no longer a luxury but a staple in the enterprise move through future transformation. I wrote a blog recently about using Terraform with Packer together, and wanted to take this thought further with breaking down Terraform Modules and getting well connected with Terraform Cloud. I recently put together a simple module for building base infrastructure in AWS for the purpose of testing Alkira Network Cloud. Let\u0026rsquo;s dive in!\nIntro What Is A Terraform Module? # If striving to build repeatable blocks of infrastructure that get provisioned consistently is the goal, then getting acquainted with Terraform Modules can help you get there. Deploying cloud infrastructure means deploying resources that depend on each other, are generally deployed together, and share the same lifecycle. This is what Terraform modules do - enable the packaging and management of common resources together, extending reuse and environmental consistency.\nModules are managed in a version control system like Github and published to Terraform Registry. Terraform\u0026rsquo;s Enterprise and Cloud variant have a private registry, making it an ideal vehicle for building, sharing, and managing internal modules for an organization. Furthermore, once a given module is published, it can be used in tandem with other modules to build purpose-based workspaces. For this example, I\u0026rsquo;m going to use the public registry.\nCreating A Custom Module # For my testing, I needed the flexibility to create a new AWS VPC in one or more regions, provide a dynamic list of subnets to be provisioned, and also create a lightweight EC2 instance per subnet. However, setting these environments up by hand takes time, and tearing down all the infrastructure manually takes even more time. Plus, I want to use this configuration in tandem with similar scenarios in other cloud providers while testing and demonstrating Alkira.\nVersion Control # The following repository was created to hold my work on Github. Module components include:\nmain.tf - Primary logic which describes the infrastructure I want to build variables.tf - Required input variables which must be set in the module block outputs.tf - Variables that can be exposed for other Terraform configurations to use; These act similar to return values in programming languages versions.tf - Acceptable versions of Terraform and the provider that work with my custom module Creating A Release # Terraform public and private registry expects release tags that can be used to identify module versions:\nRelease Tags Publishing A Module # Publishing the new module couldn\u0026rsquo;t be easier. When going through the setup, it will ask to connect your Github account to the registry. If uploading to the public registry, only access to your public repositories will be needed.\nPublish Terraform Cloud # Terraform Cloud makes provisioning easy. I set up the following repository to test out the new module. To run the new module, we need two files:\nmain.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-2\u0026#34; access_key = var.access_key secret_key = var.secret_key } module \u0026#34;infra\u0026#34; { source = \u0026#34;wcollins/infra/aws\u0026#34; vpc_name = \u0026#34;vpc-aws-east-2\u0026#34; vpc_prefix = \u0026#34;10.1.0.0/16\u0026#34; subnet_names = [\u0026#34;subnet-01\u0026#34;, \u0026#34;subnet-02\u0026#34;, \u0026#34;subnet-03\u0026#34;] subnet_prefixes = [\u0026#34;10.1.1.0/24\u0026#34;, \u0026#34;10.1.2.0/24\u0026#34;, \u0026#34;10.1.3.0/24\u0026#34;] instance_names = [\u0026#34;vm-east-01\u0026#34;, \u0026#34;vm-east-02\u0026#34;, \u0026#34;vm-east-03\u0026#34;] ssh_key = var.ssh_key } variables.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 variable \u0026#34;access_key\u0026#34; { description = \u0026#34;AWS - access key\u0026#34; type = string sensitive = true } variable \u0026#34;secret_key\u0026#34; { description = \u0026#34;AWS - secret key\u0026#34; type = string sensitive = true } variable \u0026#34;ssh_key\u0026#34; { description = \u0026#34;Public key data\u0026#34; type = string sensitive = true } Create A Workspace # Before applying any infrastructure, we must create a new Workspace in Terraform Cloud.\nWorkspaces Apply Infrastructure # After creating the Workspace and populating the appropriate variables, we can provision our desired infrastructure:\nApply Destroy Infrastructure # Running cloud infrastructure that isn\u0026rsquo;t being used is a great way to rack up unwanted costs. So when testing is completed, let\u0026rsquo;s destroy our infrastructure.\nDestroy Conclusion # Delivering automation in the context of complete environments deployed intact heralds a whole new world of possibilities. Terraform Modules simplify the building blocks of immutable infrastructure, and Terraform Cloud enhances the ability to deliver and iterate. Stay tuned for new content coming that showcases the power of Terraform driving Alkira Network Cloud.\n","date":"23 June 2021","externalUrl":null,"permalink":"/posts/2021/intro-to-terraform-modules-with-aws/","section":"Posts","summary":"Effectively automating infrastructure is no longer a luxury but a staple in the enterprise move through future transformation. I wrote a blog recently about using Terraform with Packer together, and wanted to take this thought further with breaking down Terraform Modules and getting well connected with Terraform Cloud. I recently put together a simple module for building base infrastructure in AWS for the purpose of testing Alkira Network Cloud. Let’s dive in!\n","title":"Intro To Terraform Modules With AWS","type":"posts"},{"content":"This is going to be a gigantic pivot from my usual topics of writing. If you are interested in learning something about cryptocurrency, stay put. The goal is to make this unbiased and thought-provoking while shedding light on what seems like a world of confusion and misinformation.\nIn the past month, I have heard various first-hand opinions about Bitcoin like, it is a pyramid scheme, basically gambling, will supersede the U.S. dollar, and that one should invest all or none of their money into it. Talk about a wide range of opinions here!\nIntro Gold Standard To Fiat # The United States operated under the Gold Standard from 1879 to 1933. All printed money officially became redeemable for a specific amount of gold in 1900 under the Gold Standard Act. Each country had its own monetary unit. For the U.S. it was the dollar; in the U.K., the pound. This unit then consisted of a specific number of troy grains. The mint price was coined to represent the ratio of unit to the quantity of gold. Being international in scope, exchange rates between currencies tied to gold were fixed for participating countries.\nThe Great Depression # Although the construction and automotive industries helped drive the post-war recovery for the United States after World War I, European countries had borrowed heavily and faced challenges with paying off debts. This caused a slowdown on demand for U.S. exports. The Wall Street Crash of 1929 and the bank failures of 1930 and 1931, caused a frightened public to hoard gold. In addition, the inability of countries to stimulate their economies by increasing the money supply caused severe economic hardships.\nTo alleviate the economic turmoil, FDR took the U.S. off the gold standard in 1931 (Two years after Britain). Given the circumstances, overcoming deflationary forces was viewed as the best way out of the Great Depression. In truth, the global financial system still operated on the gold standard, albeit more indirectly, as the dollar could still be converted to gold under the Bretton Woods System. In 1971 Richard Nixon ended dollar convertibility to gold completing the full transition to fiat money.\nTimeline What Is Fiat Money? # Fiat money carries no intrinsic value. Today\u0026rsquo;s dollar is solely based on one\u0026rsquo;s faith and the credit of the United States Government. Since there is no tangible thing underpinning the U.S. dollar, this means additional money can be pumped into circulation without creating any extra underlying value. This affords the central banks considerable control across economic variables like credit supply, liquidity, interest rates, and money in circulation.\nFiat Money Webster defines fiat money as \u0026quot;: money (such as paper currency) not convertible into coin or specie of equivalent value\u0026quot;.\nGreat, now a government can spend without increasing taxes or withdrawing valuable resources from a thriving economy. This means the government can also pay on debts or fund programs effortlessly. It doesn\u0026rsquo;t take much imagination to see where this goes. How do you pay for things not funded through taxation? Easy - The Federal Reserve can keep buying bonds, printing money, and spending. Take a look at the U.S. Debt Clock for some perspective.\nWhat Is Bitcoin # Bitcoin is a decentralized digital currency where transactions occur over the peer-to-peer Bitcoin Network. The ingenuity of Bitcoin is predicated on its integrity. A new kind of money that doesn\u0026rsquo;t require any central authority or bank to verify transactions, no entity can inflate or deflate its value, and it\u0026rsquo;s impervious to tampering or counterfeiting. The idea of Bitcoin, in many ways, is the polar opposite of fiat money. While fiat\u0026rsquo;s value is created from the inside-out, Bitcoin\u0026rsquo;s value is made from the outside-in.\nBitcoin Origins For a full technical deep-drive on Bitcoin, check out the original whitepaper written by Satoshi Nakamoto here.\nProof Of Work # Every time someone uses a portion of Bitcoin to make a purchase, a cryptographically secure transaction is created and broadcast to the network with the consensus being achieved by a proof of work system called mining. Reusable proof of work was pioneered by Hal Finney in 2004. Aside from maintaining the ledger of transactions, Bitcoin mining also produces new Bitcoin into circulation. This happens as miners add a new block of transactions to the blockchain. Therefore, proof of work (or any consensus algorithm) is critical in the operation of blockchain systems today.\nBlockchain # While blockchain-like protocols have been proposed in the past, the invention of the blockchain has been credited to Satoshi Nakamoto. Think of our typical everyday commerce. I decide to go to the local coffee shop for a tasty caramel latte. This transaction will hit a point-of-sales system, go through a financial institution, and spit out a physical receipt. Many things happen between the buyer and seller. With Bitcoin, blocks of transactions are added to a distributed ledger called blockchain. Each block contains the transaction data, timestamp, and the previous block\u0026rsquo;s cryptographic hash. No government, financial institution, or any other authority sits between buyer and seller.\nA lot of this article focuses on the currency aspect of Bitcoin. In understanding what makes Bitcoin compelling to diverse audiences, let\u0026rsquo;s decouple the underlying technology and view the benefits from a different lens. So, decentralization, security, immutability, and transparency all add to Bitcoin\u0026rsquo;s charm. Doing these things more efficiently and cheaper is critical to just about every industry if we frame the outputs differently. What about a digital voting system? Full transparency, decentralization while also being tamper-proof? Think of how complicated medical record-keeping is. How about a massive reduction in paperwork while making records more accurate? Or faster money transfers with lower fees from anywhere to anywhere? What about immutability in pharmaceutical trading?\nZooming Out One crucial point - Blockchain is to Bitcoin what a battery is to a Tesla. Of course, this example isn\u0026rsquo;t a perfect 1:1, but let\u0026rsquo;s tease this out a little bit. Tesla is a brand, an idea, and by the opinion of many, the first trailblazer into the modern EV space. Lithium-ion batteries, the driving power behind the Tesla, has many use cases beyond cars ranging from phones to weed eaters. Similarly, blockchain has many use cases beyond Bitcoin.\nAn Investment Play Or Currency? # Bitcoin is still very new and in its infancy stages. It has been difficult for many, even large crypto advocates, to categorize Bitcoin because it\u0026rsquo;s drastically different from other assets available on the market. The great and powerful Commodity Futures Trading Commission (CFTC) labels all virtual currencies as commodities under the Commodity Exchange Act (CEA). Many big names in finance dismissed Bitcoin as having no value and strongly advised investors to stay away. Many of those same titans of business also argued that nobody would use crypto as an actual currency. This has slowly changed over the past few years, especially coming into 2021.\nKevin O\u0026rsquo;Leary from Shark Tank likened Bitcoin to a giant nothing-burger not worth investing in. Fast forward a little into the future, and his stance has changed.\nView post on X (@kevinolearytv) Mark Cuban on the other hand, said he would rather have bananas than bitcoin. Now the billionaire entrepreneur and owner of the Dallas Mavericks is accepting Dogecoin as payment, on top of already accepting Bitcoin.\nView post on X (@forbes) JPMorgan CEO Jamie Dimon called Bitcoin a fraud but now JPMorgan seems bullish on Bitcoin even predicting Bitcoin\u0026rsquo;s volatility will converge with gold\u0026rsquo;s in the future. JPMorgan even launched its own digital coin - JPM Coin earlier this year.\nView post on X (@Hugh_Son) PayPal, a company that knows a thing or two about online payments, processed 3.47 billion transactions worth a combined value of $277 billion in Q4 2020 alone. PayPal accepts cryptocurrency as a funding source for digital commerce at its 26 million merchants.\nView post on X (@PayPal) What Does All This Mean? # Let\u0026rsquo;s start by making something clear: the goal of institutional investors is to drive profits for themselves and their clients. So the first thing to mind is, how high can the value be pushed, and what does the cash out look like? The last thing to mind is transparency, opaqueness, decentralization, and cutting out the middle party.\nA fear that many crypto purists have is, Wall Street leveraging their money and power to take cryptocurrency away from the populous (think of how other financial assets are handled). An example of this would be providing \u0026ldquo;guidance\u0026rdquo; to the government that forces the use of an \u0026ldquo;authorized entity\u0026rdquo; instead of your standard crypto wallet. Of course, this entity would have to meet stringent requirements, and guess who dots all the I\u0026rsquo;s and crosses all the T\u0026rsquo;s?\nHistory can be a great teacher. Usually, this falls in the category of not repeating it. Take, for instance The Telecommunications Act Of 1996, which ultimately led to six media giants owning 90% of what we read, watch, and listen to. Wall Street firms are skilled in the craft of buying up smaller fish. So why not buy up all the small crypto exchanges and popular wallet providers and integrate those wallets into their payment networks? What would stop Wall Street from persuading the government into handing over exclusive control of ICOs and IEOs? In that version of the future, the money and influence will find their way into marketing and lobbying. Sounds to me like the control and power being regulated right into their pockets.\nConclusion # To me, Bitcoin is not a pyramid scheme or gambling. Bitcoin\u0026rsquo;s underlying technology is highly innovative, and its full potential is not yet realized. Will it supersede the U.S. dollar? Probably not. Should someone invest all or none of their money into it?\nIt\u0026rsquo;s not for me to opine, but I am of the opinion that one shouldn\u0026rsquo;t invest in anything, not even the investment options within an employer-sponsored 401K, without first taking the time to understand precisely what one is investing in, level of risk, and potential for gains or losses.\nMost investment professionals would agree that diversification across several different asset classes such as stocks, bonds, real estate, gold, etc. can be one of the best ways to reduce risk and volatility in an investment portfolio. Owning crypto could be viewed as just another way to add diversification and depth to your overall investment strategy.\n","date":"14 June 2021","externalUrl":null,"permalink":"/posts/2021/understanding-the-world-of-bitcoin/","section":"Posts","summary":"This is going to be a gigantic pivot from my usual topics of writing. If you are interested in learning something about cryptocurrency, stay put. The goal is to make this unbiased and thought-provoking while shedding light on what seems like a world of confusion and misinformation.\n","title":"Understanding The World Of Bitcoin","type":"posts"},{"content":"","date":"28 April 2021","externalUrl":null,"permalink":"/tags/bgp/","section":"Tags","summary":"","title":"Bgp","type":"tags"},{"content":"","date":"28 April 2021","externalUrl":null,"permalink":"/series/exploring-azure-cloud-networking/","section":"Series","summary":"","title":"Exploring Azure Cloud Networking","type":"series"},{"content":"In Part 1, we went over some fundamentals. For Part 2, we will examine Azure network design patterns based on cloud maturity and organization size. The concept of design patterns was first introduced by Christopher Alexander and has profoundly influenced many technical disciplines.\nTo keep things simple, let\u0026rsquo;s define a design pattern as a reusable solution to a commonly occurring problem. Of course, you are not the first practitioner out there transitioning to cloud or growing to a new maturity model. Many trailblazers have already walked the walk and left behind beautiful blueprints to help you along the way.\nPatterns Overview # While every scenario may not fall under these categories, there are generally three phases that your cloud network may likely go through as your organization matures.\nIsland - Starting point or cloud native panacea; Disconnected from the enterprise network Hybrid - Often inevitable (integration requirements); Connected to the enterprise network Hub \u0026amp; Spoke - Stemmed by growth and need for centralized control; Increased governance Cloud Adoption Framework (CAF) # The Cloud Adoption Framework (CAF) has been crafted using essential feedback from partners and customers. This provides an excellent place to start as you begin planning. Of course, you don\u0026rsquo;t just land on the ideal design, but it helps to have a clear picture of where you are going.\nCAF The Microsoft Patterns \u0026amp; Practices site is another good Azure specific resource for understanding proven patterns and reference architectures.\nThe Island # Webster defines island as \u0026quot;:an isolated group or area\u0026quot;. There are a few variations of The Island, but one thing remains consistent; It has no connectivity to on-premises networks. This environment is completely isolated.\nIsland #1 - First Pass # Every good story has a beginning. Many enterprises started their cloud journey many years back by simply setting up an account, doing some sandboxing, and just familiarizing themselves with the Cloud. You never know, one day that sandbox application could turn Production overnight! (Yes, I\u0026rsquo;ve seen this happen)\nUsually, the first pass at a technology (especially as transformative as Cloud) isn\u0026rsquo;t going to be perfect. There are a lot of things you don\u0026rsquo;t know until you know. The main criteria that define our first pass at Cloud may include:\nSingle Virtual Network; All dependencies self-contained Enterprise Apps and Data live in Data Centers (Owned or Colocation) No connectivity to on-premises networks; Environment is completely isolated Cloud Environment may not be under centralized I.T. control; They slow things down, right? Island Island #2 - Cloud Native # We are getting this whole cloud computing thing down now. We are super mature around security and governance, and we want to take full advantage of Azure-specific services. In this scenario, we don\u0026rsquo;t have any hybrid dependencies and focus on reliable and scalable infrastructure leveraging cloud native constructs in a single cloud-only. As a set of outcomes, this pattern aims to:\nScale Out.and not up; Optimize based on consumption Multi-Region Active-Active design; Non-blocking asynchronous communication Embrace Stateless Application Design; Local state is considered an anti-pattern Use Internet as primary transport; State is shared between regions over cloud backbone Island - Cloud Native Our Cloud-Native Island is a best case scenario, but also unlikely at first for the traditional enterprise. This takes into account that your data lives in the Cloud, and you aren\u0026rsquo;t making any calls to backend services in your data centers. Data Gravity has proven to be a worthy antagonist in the quest to cloud native.\nHybrid # Firstly, my definition of Hybrid Cloud is: Any workload that combines public cloud with an on-premises data center environment. Meaning, if for some reason, this public Cloud no longer has connectivity back to the data center, then the application(s) will not work.\nAt some point, that Island is going to get a little lonely. How can you possibly migrate anything without a reliable connection to Azure? I will forego the whiplash that generally takes place at this point in the story. That beautiful moment we all remember when server, network, and security experts first get involved.\nSo, we need a little bit of hybrid now. This is probably where things get the most complicated. There are many variations of hybrid, a plethora of options/considerations, and numerous maturity levels.\nHybrid #1 - Forced Tunneling # Our first iteration of hybrid is met with much skepticism and minimal trust. This is why the obvious thing to do from the network and security space is to backhaul every packet to the data center to use existing routing, segmentation, deep packet inspection, file blocking, etc.\nThis practice is known as Forced Tunneling. It does precisely what is implied, e.g., it enables redirection of all traffic destined to the Internet back to your on-premises network.\nTo accomplish this with ExpressRoute - Private Peering, advertise a default route from your BGP speaker and all traffic from the associated virtual network will be routed to your on-premises network. Doing this will force traffic from services offered by Microsoft like Azure Storage back on-premises as well; You will have to account for return traffic via Microsoft Peering or the public Internet. If Service Endpoints is configured, traffic is not forced on-premises and remains on Azure\u0026rsquo;s backbone network. To accomplish Forced Tunneling over a Site-to-Site VPN, you would set up a User Defined Route to set the default route to the Azure VPN Gateway.\nForced Tunneling Hybrid #2 - Cloud DMZ # For workloads in the Cloud, it makes sense to reduce dependencies on-premises (especially Internet) and keep as much traffic as we can in Azure. Also, to continue meeting various security requirements, often deploying Network Virtual Appliances (NVAs) will be necessary.\nA common practice that accommodates future growth is routing egress and ingress traffic through separate scale sets. You can then use User Defined Routes for limited traffic engineering. For example, traffic destined to the Internet would use the private IP address of an internal load balancer as the next-hop.\nIn this scenario, Ingress represents inbound connections into the DMZ while Egress represents traffic destined to the internet. Another benefit of this design is additional throughput. VM-Series - Firewalls have come a long way but still have limitations that you must consider. You might also consider deploying an additional scale set for Transitive routing in substantial deployments.\nCloud DMZ NVAs must be deployed in self-contained subnets. Deploying an appliance in the same subnet as your workloads and attaching a UDR to that same subnet may cause a routing loop.\nHub \u0026amp; Spoke # Until now, our patterns have been simple; Only a single virtual network has been referenced. In larger enterprises, this can get a little tricky. Over time as the Cloud grows, additional business units, centralized services, security, and even finance may start to leverage the Cloud. Sound governance is critical in managing adoption.\nAs consumption increases, subscriptions will likely grow. It is generally a good practice to model these constructs to match your organization\u0026rsquo;s hierarchy. As you follow this path, other virtual networks will be created. How is this challenge designed around while taking scale and governance in mind?\nCentralized Hub # Network practitioners are going to be very familiar with Hub \u0026amp; Spoke. Think about it like data center and branch office routing. We don\u0026rsquo;t want prefixes to be visible randomly all across the network. Aggregation is our friend here.\nIt is much more efficient to have summarization of routing for each site at the edge. It also makes sense to centralize certain services at the edge rather than deploy them at every office. Deploying NGFWs at every site would become expensive and challenging to manage. Our cloud hub will follow this same pattern. Any shared services that will be used across all spokes will live here.\nHub \u0026amp; Spoke Spoke-To-Spoke Routing # Now that multiple virtual networks are in play, how does traffic flow? Remember from Part 1 that VNets are non-transitive. For example, let\u0026rsquo;s say we have Spoke-A and Spoke-B which are VNet peered back to our hub. There have been two common approaches to solving this in the past:\nOn-Premises / CoLo - Routing # This approach forwards traffic back to a physical device (That lives outside of Azure\u0026rsquo;s Cloud) that knows how to route packets. This approach is usually the first iteration as, if you already have hybrid connectivity, then these devices are already reachable.\nThe Hub virtual network is configured to allow gateway transit; This enables peered virtual networks to use any attached virtual network gateways Each spoke should be configured to use remote gateways; This is required for traffic to use a gateway living in the virtual network you are peered with The router on-premises will handle any spoke-to-spoke routing; The existing BGP which was set up for private peering handles dynamic routing Transit Routing - 1 Network Virtual Appliance (NVA) - Routing # Another option would be leveraging NVAs existing in the hub to route traffic between spokes. If we already have the Cloud DMZ design deployed, you would use UDRs to redirect traffic.\nEach subnet requires a UDR to steer traffic Destination matches the subnet prefix from the adjacent VNet Next Hop Type will be defined as Virtual Appliance (Next Hop Options) The Next Hop is the internal load balancer\u0026rsquo;s private IP (If deployed in a scale set) Transit Routing - 2 Conclusion # Why not just use a WAF for Inbound connections and Azure Firewall for Outbound? It all comes down to what requirements you are working with and the capabilities needed to meet them.\nNextGen Firewalls have more capabilities today and are also being used heavily in the enterprise. Many will opt to extend these capabilities into the Cloud. Microsoft currently has Azure Firewall - Premium in Public Preview, which should close the gap a little more.\nIf this material was valuable, check out Part 1 to learn more about the fundamentals. In Part 3, we will dig into ExpressRoute design along with breaking down a few things I wish I would have known before jumping in the deep-end.\nAcknowledgements # Big thanks to the wise and resourceful Steven Hawkins for taking the time to peer review this post. Your feedback is always insightful and appreciated.\n","date":"28 April 2021","externalUrl":null,"permalink":"/posts/2021/exploring-azure-cloud-networking-part-2/","section":"Posts","summary":"In Part 1, we went over some fundamentals. For Part 2, we will examine Azure network design patterns based on cloud maturity and organization size. The concept of design patterns was first introduced by Christopher Alexander and has profoundly influenced many technical disciplines.\n","title":"Exploring Azure Cloud Networking - (Part 2)","type":"posts"},{"content":"Is cloud networking complicated, or is it just different? In building your infrastructure in the cloud, end-to-end system complexity increases exponentially. As enterprise applications mature, the foundational infrastructure and networking used to host and transport them must evolve. One obvious crux of networking is blast radius - you cannot easily modify it without down-time.\nThis is Part 1 of a multi-part series that will explore Azure networking. To the best of my ability, this series will be written to articulate real-world scenarios and bring attention to specifics that are critical to an understanding before diving into cloud networking architecture. Even in working through basics, sometimes a simple thing can be overlooked, which carries long-lasting implications over time.\nOverview Core Concepts # New and shiny services like Virtual WAN and Route Server will continually be released as capabilities evolve. While new services and features are released to make life easier, some things rarely change. A few core network components are always required when deploying just about any application.\nCommon Components # In the cloud, you generally have a hierarchy in which logical components exist, which may contain additional logical components. With Azure, for instance, Management Groups include Subscriptions. Subscriptions hold Resource Groups, which is where Virtual Networks live. A VNet may be a shared resource across many apps and services in scope across many teams.\nResource Groups are containers that hold common objects; An example might be all resources and services which make up an application Virtual Networks (VNets) enable connectivity between your resources, forming the foundation you need to run applications in the cloud Subnets are created from the address space defined at the VNet level; This allows a VNet to be segmented as needed User-Defined Routes (UDRs) are simply static routes that override Azure\u0026rsquo;s default system routes or add additional routes to a subnet\u0026rsquo;s route table; A given subnet can have 0 or 1 route tables associated with it Components Basic Routing # Knowing how Azure routes traffic between virtual networks, on-premises, and over the Internet is good knowledge to have. Understanding how you can override certain default behaviors is excellent knowledge to have. Putting these things together to build a network that enables business outcomes should be the goal.\nSubnets # In Azure and AWS, VPCs and VNets are limited to a single Region. This is about where the similarities stop. With AWS, a subnet is limited to a single availability zone while Azure extends a single subnet across all availability zones.\nSystem Routes # System Routes are automatically created and assigned to each subnet. You cannot create system routes, nor can you remove them. A default routing table for a new subnet in Azure would look something like this:\nSource Address Prefixes Next Hop Type Default Address Space (assigned at provision time) Virtual Network Default 0.0.0.0/0 Internet Default 10.0.0.0/8 None Default 192.168.0.0/16 None Default 100.64.0.0/10 None Next Hop Types are self-explanatory. The type Virtual Network routes traffic between address ranges carved out from the VNet\u0026rsquo;s address space. For internet egress, type Internet is used. If you want to Null route traffic, type None is used; Traffic will be discarded.\nYou can validate effective routes via network interface settings or route tables.\nRoute Selection # How exactly does route selection in Azure work? With traffic exiting a given subnet, a decision must be made based on the destination IP Address. Azure makes this decision based on the longest prefix. If multiple routes contain the same address prefix, the tie is broken based on the following priority:\nUser Defined Routes (Custom) Border Gateway Protocol (BGP) System Route (Default) Longest Prefix System Routes for VNet peering is always a preferred route, even if BGP routes are more specific. This is a small but critical detail as a design evolves.\nIP Addressing # Some decisions age like fine wine, while others seem to age like sour milk. An area that comes to mind here is IP Address Planning. Taking some time to plan out your IP Addressing Scheme upfront can go a long way.\nIP Addressing Truths # There is no one size fits all here. A few things that are general truths include:\nIP Addressing Scheme should match the topology or vice versa VNet Address Space cannot be added, modified, or removed if a VNet peer is established Design dictates consumption; Good automation becomes difficult with poorly planned designs IP Addressing and Segmentation is a collaborative exercise; Align with the Security Overlords Planning Up Front # Let\u0026rsquo;s use Healthcare as an example. Privacy is critical, and HIPAA sets clear guidelines on the use and release of health records. If your business deals with protected information, then data classification is a great place to start. If you have a combination of applications (some leveraging protected information and some that do not), both categories can be treated fundamentally differently.\nAddressing Scheme The above example puts data classification at center stage. If you can categorize all of your protected data at a higher level, this simplifies the work required to ensure compliance. Think of it like this:\nProtected Data can only exist in the Production (With Protected Data) segment An alternative Production segment exists for workloads not requiring access to Protected Data Non-Production exists for all lower-stage environments across both Production segments; Synthetic Data enables rapid prototyping of heavily regulated data in lower-stage environments (Preventing the risk of accidental exposure) Virtual Network Peering # At some point, connectivity between two or more virtual networks is going to become a requirement. As you grow, chances are, your virtual network footprint will expand significantly. Virtual Network Peering enables connectivity between two or more virtual networks. The big caveat with peering is, virtual networks natively are none-transitive. This means, if you have 3 VNets, you can\u0026rsquo;t route from VNet: A to VNet: C through VNet: B.\nTransitive Routing Peering Gone Wild # Without giving network peering more thought, common sense may imply just peering VNets which require communication. So, we peer every virtual network with every other virtual network. What is the worst that could happen? I like spaghetti myself, but I don\u0026rsquo;t think of it as a design principle when building my networks!\nFull Mesh Is Peering Bad? # No, peering is not bad. Like any feature, you should think through how it is used to provide the outcomes you want. Also, try hard not to over-extend it to do things that it was not designed to do. If you are peering everything with everything, then maybe the problem that needs to be solved is transitive routing. In thinking through VNet peering, some obvious things come to mind:\nCost - Inbound/Outbound Charges Reliability - A failure occurs with a connection; How would redundancy or mitigation work? Operations - As new VNets are added, overhead and complexity grows exponentially Security - Least privilege is real; What can talk to what now shifts to UDRs and NSGs Conclusion # Now that we have a basic understanding of foundational network components, what\u0026rsquo;s next? For Part 2, we will go through the evolution of common network patterns ranging from my own little desert island, to modern hub-and-spoke designs that we see today in larger environments.\n","date":"5 April 2021","externalUrl":null,"permalink":"/posts/2021/exploring-azure-cloud-networking-part-1/","section":"Posts","summary":"Is cloud networking complicated, or is it just different? In building your infrastructure in the cloud, end-to-end system complexity increases exponentially. As enterprise applications mature, the foundational infrastructure and networking used to host and transport them must evolve. One obvious crux of networking is blast radius - you cannot easily modify it without down-time.\n","title":"Exploring Azure Cloud Networking - (Part 1)","type":"posts"},{"content":"Manually provisioning infrastructure slows down application delivery, isolates knowledge, can hamper operations teams, and doesn\u0026rsquo;t scale. Automating infrastructure provisioning can address these challenges by shifting manual process into code. Hashicorp has products spanning the infrastructure, security, and application stack that can unlock that cloud operating model and deliver applications faster.\nLet\u0026rsquo;s examine image lifecycle management and IaaS deployment. Both of these tasks are common challenges faced by the enterprise when moving to the cloud. Using Packer, we can automate the build process for images and then deploy common infrastructure and virtual machines with Terraform.\nIntro What Is Cloud Grade Automation? # I started using this expression as a catch phrase some time back, mainly in Powerpoint slides. The way I would describe its meaning goes well beyond a single tool, process, or methodology. To me, it embodies the core guiding principles for cloud and application delivery in today\u0026rsquo;s landscape. Let\u0026rsquo;s start with the outcomes we are aiming for:\nLoosely coupled application architecture; Cloud Native Infrastructure hosting a given application is delivered intact Infrastructure is never touched, changed, or otherwise modified in any way When changes are necessary, the environment is re-deployed from the newest artifact Rollback occurs in the same way except with an older versioned artifact The culture, tooling, process, and delivery that can achieve these outcomes, is my impression of what Cloud Grade Automation is. I\u0026rsquo;ll be focusing specifically on the tooling and delivery in this post.\nEvolving Automation # Automation can typically be handled at two different stages of a given workflow. Choosing which stage can ultimately drive operational decisions. As application architectures have evolved, the automation used to build the infrastructure has transformed. Complexity has largely shifted from runtime to build-time.\nRuntime - (Mutable) # When we think automation, folks that have been around a while typically think in terms of mutable. This means we deploy our server and then configure, update, or modify it in-place. I have seen anything from scripts (Shell, Perl, and Python) to configuration management tools like Ansible, Chef and Puppet used to accomplish this.\nWebster defines mutable as \u0026quot;:prone to change: INCONSTANT\u0026quot;.\nTo further expand on runtime configuration, let\u0026rsquo;s take the example of installing an agent-based solution. For the sake of demonstration, let\u0026rsquo;s say that each virtual machine provisioned in our infrastructure requires running ThousandEyes - Enterprise Agents for monitoring.\nA greenfield application is getting developed; New infrastructure is required (Develop) The new server (or multiple servers) are provisioned for the application to run on (Deploy) Config Management tooling applies the standard configuration, including the Agent (Configure) Mutable Mutable infrastructure is susceptible to configuration drift. As individual changes get applied on servers throughout their lifecycle, the configuration will significantly differ from the desired state and other servers across the environment.\nBuild-time - (Immutable) # Immutable infrastructure aims to reduce the number of moving pieces at runtime. Handling infrastructure this way speeds up delivery, eliminates configuration drift, increases environment consistency, optimizes rollback, and simplifies horizontal scaling.\nWebster defines immutable as \u0026quot;:not capable of or susceptible to change\u0026quot;.\nTo further expand on build-time configuration, let\u0026rsquo;s take the same example used above:\nA greenfield application is getting developed; New infrastructure is required (Develop) The standard configuration and Enterprise Agent get packaged at build-time (Configure) Infrastructure is provisioned using the machine template and deployed at runtime (Deploy) Immutable Building immutable infrastructure can be a significant undertaking when considering the transition of brownfield applications. One dependency ensures that the application layer remains stateless, including servers. Anything at this level should and will be consistently destroyed and rebuilt.\nGetting Started With Packer # Packer automates the creation of any machine image. Once the image is packaged, it can then be provisioned with Terraform. While this is the new, modern, cool, and cloud way to accomplish this, other methods have existed for a very long time. Back in 2014, I was doing non-interactive installs of various Linux distributions on Linux KVM with virsh and virt-install. Virt-install would use libvirt\u0026rsquo;s streaming API to upload the kernel and modified initrd to the remote host.\nPacker Building A Machine Image # As an example, let\u0026rsquo;s package an Ubuntu 18.04 image with a ThousandEyes - Agent. Although an overly simplistic example, it should demonstrate our desired intent and behavior. Demo code can be found here.\nPacker ubuntu-18.04-LTS.json\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \u0026#34;builders\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;azure-arm\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;{{user `client_id`}}\u0026#34;, \u0026#34;client_secret\u0026#34;: \u0026#34;{{user `client_secret`}}\u0026#34;, \u0026#34;tenant_id\u0026#34;: \u0026#34;{{user `tenant_id`}}\u0026#34;, \u0026#34;subscription_id\u0026#34;: \u0026#34;{{user `subscription_id`}}\u0026#34;, \u0026#34;managed_image_resource_group_name\u0026#34;: \u0026#34;demo-eastus-rg\u0026#34;, \u0026#34;managed_image_name\u0026#34;: \u0026#34;ubuntu-18.04-template\u0026#34;, \u0026#34;os_type\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;image_publisher\u0026#34;: \u0026#34;Canonical\u0026#34;, \u0026#34;image_offer\u0026#34;: \u0026#34;UbuntuServer\u0026#34;, \u0026#34;image_sku\u0026#34;: \u0026#34;18.04-LTS\u0026#34;, \u0026#34;azure_tags\u0026#34;: { \u0026#34;budget_id\u0026#34;: \u0026#34;br12345\u0026#34;, \u0026#34;business_unit\u0026#34;: \u0026#34;Engineering\u0026#34;, \u0026#34;environment\u0026#34;: \u0026#34;Non-Prod\u0026#34;, \u0026#34;support_group\u0026#34;: \u0026#34;OS Lifecycle Mgmt\u0026#34; }, \u0026#34;location\u0026#34;: \u0026#34;East US\u0026#34;, \u0026#34;vm_size\u0026#34;: \u0026#34;Standard_B1s\u0026#34; }], \u0026#34;provisioners\u0026#34;: [{ \u0026#34;execute_command\u0026#34;: \u0026#34;chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh \u0026#39;{{ .Path }}\u0026#39;\u0026#34;, \u0026#34;inline\u0026#34;: [ \u0026#34;apt-get update\u0026#34;, \u0026#34;apt-get -y upgrade\u0026#34;, \u0026#34;apt-get -y install jq\u0026#34;, \u0026#34;apt-get -y install ntp\u0026#34;, \u0026#34;curl -Os https://downloads.thousandeyes.com/agent/install_thousandeyes.sh\u0026#34;, \u0026#34;chmod +x install_thousandeyes.sh\u0026#34;, \u0026#34;./install_thousandeyes.sh {{user `token`}}\u0026#34;, \u0026#34;/usr/sbin/waagent -force -deprovision+user \u0026amp;\u0026amp; export HISTSIZE=0 \u0026amp;\u0026amp; sync\u0026#34; ], \u0026#34;inline_shebang\u0026#34;: \u0026#34;/bin/sh -x\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34; }] } Instead of using the shell provisioner to deploy the agent (see line 35 in the template), a more elegant method would call the \u0026ldquo;type\u0026rdquo;: \u0026ldquo;ansible\u0026rdquo; provisioner pointing to a playbook.yml file in the repository. This would allow you to separate packages into separate playbooks managed in version control.\nGetting Started With Terraform # Terraform Terraform codifies cloud APIs into declarative configuration files. If you want to create reproducible infrastructure for consistent testing, staging, and production environments with the same configuration, look no further! While it\u0026rsquo;s generally not that simple, Terraform does seem to be the defacto tool these days for Infrastructure as Code in the cloud.\nDeploying Some Infrastructure # Let\u0026rsquo;s deploy some foundational infrastructure along with a virtual machine provisioned from the machine image we built above with Packer. Demo code can be found here.\nDeploy main.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 provider \u0026#34;azurerm\u0026#34; { features {} subscription_id = var.subscription_id client_id = var.client_id client_secret = var.client_secret tenant_id = var.tenant_id } data \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;rg\u0026#34; { name = var.resource_group_name } resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;vnet\u0026#34; { name = var.vnet_name address_space = [var.address_space] resource_group_name = data.azurerm_resource_group.rg.name location = data.azurerm_resource_group.rg.location tags = data.azurerm_resource_group.rg.tags } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;snet\u0026#34; { name = var.snet_name address_prefixes = [var.snet_prefixes] resource_group_name = data.azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name } resource \u0026#34;azurerm_network_security_group\u0026#34; \u0026#34;nsg\u0026#34; { name = var.nsg_name resource_group_name = data.azurerm_resource_group.rg.name location = data.azurerm_resource_group.rg.location tags = data.azurerm_resource_group.rg.tags security_rule { name = \u0026#34;SSH\u0026#34; priority = 1001 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = \u0026#34;22\u0026#34; source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } } resource \u0026#34;azurerm_public_ip\u0026#34; \u0026#34;pip\u0026#34; { name = var.pip_name allocation_method = var.allocation_method resource_group_name = data.azurerm_resource_group.rg.name location = data.azurerm_resource_group.rg.location tags = data.azurerm_resource_group.rg.tags } resource \u0026#34;random_id\u0026#34; \u0026#34;id\u0026#34; { keepers = { resource_group = data.azurerm_resource_group.rg.name } byte_length = 8 } resource \u0026#34;azurerm_storage_account\u0026#34; \u0026#34;storage\u0026#34; { name = random_id.id.hex account_tier = var.account_tier account_replication_type = var.account_replication_type resource_group_name = data.azurerm_resource_group.rg.name location = data.azurerm_resource_group.rg.location tags = data.azurerm_resource_group.rg.tags } resource \u0026#34;azurerm_network_interface\u0026#34; \u0026#34;nic\u0026#34; { name = var.interface_name resource_group_name = data.azurerm_resource_group.rg.name location = data.azurerm_resource_group.rg.location tags = data.azurerm_resource_group.rg.tags ip_configuration { name = \u0026#34;internal\u0026#34; private_ip_address_allocation = \u0026#34;Dynamic\u0026#34; subnet_id = azurerm_subnet.snet.id public_ip_address_id = azurerm_public_ip.pip.id } } data \u0026#34;azurerm_image\u0026#34; \u0026#34;packer_image\u0026#34; { name = var.managed_disk_name resource_group_name = data.azurerm_resource_group.rg.name } resource \u0026#34;azurerm_linux_virtual_machine\u0026#34; \u0026#34;linux_vm\u0026#34; { name = var.vm_name size = var.vm_size admin_username = var.vm_default_user resource_group_name = data.azurerm_resource_group.rg.name location = data.azurerm_resource_group.rg.location tags = data.azurerm_resource_group.rg.tags network_interface_ids = [ azurerm_network_interface.nic.id, ] admin_ssh_key { username = var.vm_default_user public_key = var.public_key } os_disk { caching = \u0026#34;ReadWrite\u0026#34; storage_account_type = \u0026#34;Standard_LRS\u0026#34; } source_image_id = data.azurerm_image.packer_image.id } This template is an example. In practice, this would probably be broken up into at least two distinct modules (network and compute). If specific groups of resources have a similar life cycle, you would probably create a module for them. An example of this might be core network components like VNets, Subnets, and UDRs.\nConclusion # Learning to use Packer or writing Terraform templates is a tiny piece of a vast puzzle. If the desired outcome is Continuous Delivery then application architecture, cultural philosophies, and release management all play equally important roles.\nEnterprise Leaders all want DevOps for the benefits of automation, quicker releases, reduced downtime, reliable recovery from disasters, and cost benefits in the cloud (That last one makes me chuckle). The real question is, will they do what is necessary to buck traditional culture and adopt truly immutable infrastructure?\n","date":"8 February 2021","externalUrl":null,"permalink":"/posts/2021/cloud-grade-automation-with-packer-and-terraform/","section":"Posts","summary":"Manually provisioning infrastructure slows down application delivery, isolates knowledge, can hamper operations teams, and doesn’t scale. Automating infrastructure provisioning can address these challenges by shifting manual process into code. Hashicorp has products spanning the infrastructure, security, and application stack that can unlock that cloud operating model and deliver applications faster.\n","title":"Cloud Grade Automation With Packer and Terraform","type":"posts"},{"content":"","date":"8 February 2021","externalUrl":null,"permalink":"/tags/immutable-infrastructure/","section":"Tags","summary":"","title":"Immutable-Infrastructure","type":"tags"},{"content":"","date":"8 February 2021","externalUrl":null,"permalink":"/tags/packer/","section":"Tags","summary":"","title":"Packer","type":"tags"},{"content":"Azure Private Link enables access to hosted customer and partner services over a private endpoint in an Azure virtual network. This means private connectivity over your own RFC1918 address space to any supported PaaS service while limiting the need for additional gateways, NAT appliances, public IP addresses, or ExpressRoute (Microsoft Peering).\nHold on, wasn\u0026rsquo;t the point of Public Cloud to leverage services offered by third-party providers over the public internet? Why, then, would we want to contain traffic in our private IP space, which is likely routable across our on-premises network? Let\u0026rsquo;s examine this a little deeper to understand the problem we are aiming to solve, solutions available today, some complexities introduced with Private Link, and why we wouldn\u0026rsquo;t just use PaaS services as is.\nMicrosoft announced Private Link (Preview) in 2019 becoming generally available in early 2020. AWS released their flavor of Private Link back in 2017. Google Cloud released Private Service Connect in 2020.\nOverview The Problem # Leveraging PaaS services is kind of like owning a home. Except in that home, you don\u0026rsquo;t have to fix cracks in the foundation, patch that leaky roof, clean the garage, or even mow the lawn. PaaS services significantly reduce infrastructure management and increase agility while simplifying your ability to scale. Security-focused teams will usually identify the following as problems when talking PaaS:\nThe risk of data leakage / exfiltration increases as PaaS integration with IaaS and on-premises environments increase Decreasing or eliminating internet exposure is preferred; Azure PaaS services were originally available via public IP addresses only Anything that runs over a network that isn\u0026rsquo;t your own private WAN is risky and must be avoided While I understand and share concerns with risk here, these are generally obstacles I have run into with various security teams throughout my time in tech. I can look back on some tough conversations in the past when trying to advocate for SD-WAN. Since it went over the internet, it couldn\u0026rsquo;t be reliable and introduced significant risk to the business. Fast-forward to 2021 especially in the current state of the COVID-19 pandemic; most large enterprises now declare internet as the new network, public cloud as the new data center, and identity as the new perimeter.\nExamining Cloud Security Models # Our scope here is security in the cloud, not security of the cloud. Security considerations differ in the cloud by service model. In descending through each service model\u0026rsquo;s different layers, the responsibility for security shifts between Customer and Provider.\nFor example, with SaaS, the layers a customer is responsible for is relatively small; data and access/identity are considered. As you move into PaaS, the application layer, along with some platform configuration, comes into play. Moving into IaaS, the operating system is now customer responsibility, along with all the layers above.\nSecurity Models When you have a nice mixture of the three cloud layers and the full gamut of all things data center, thinking through the security domain becomes complicated quickly. Add some Multi-Cloud to this venue, and you are in for a real party.\nRisky Patterns # In the Digital Era, privacy is at the top of the list. This is especially true when considering specific regulations like GDPR or HIPAA. A data leak can cause a significant loss in revenue and long-term damage to your brand. Specific patterns can expose the number of porous areas, contributing to possible data leakage if not appropriately hardened.\nIdentity \u0026amp; Access Management (IAM) - Access based on identity authentication and authorization controls; The ground floor for establishing Zero Trust in the public cloud Cloud Endpoints to On-Premises - Services in public cloud provider\u0026rsquo;s managed network need to make calls to on-premises private networks On-Premises to Cloud Endpoints - Exact opposite of the first pattern; Services in public cloud provider\u0026rsquo;s managed network need to be called from an on-premises private network Internet Egress - Any point of internet exit; A sinister concoction of data center places, properly designed cloud places, shadow IT oops cloud places, and\u0026hellip; plenty of other venues just south of anyone\u0026rsquo;s radar A Word On Provider Architectures # For public cloud to achieve the massive scale-out which accommodates customer growth, cloud services are built on large pools of resources (network, compute, and storage). This large pool of resources is managed by a centralized control plane. As demand increases, additional resources are added to the pool to increase capacity.\nService Behavior # There are ultimately two ways in which services can be categorized based on networking behavior. Since this write-up is Azure focused, I\u0026rsquo;ll map Azure specific constructs into the examples:\nPublic Services is typically what we think of when referencing PaaS. Public Services are consumed over the internet, reachable via public IP addresses, and are not provisioned in the customer\u0026rsquo;s VNet. An example of a public service would be Azure Storage.\nPrivate Services by default, are not reachable over the internet. Services are assigned RFC1918 addresses and are deployed directly inside a customer\u0026rsquo;s virtual network. An example of a private service would be an Azure Virtual Machine.\nResource Allocation # When customers provision a service, the control plane will create an instance of the service. Resources are then allocated accordingly. There are generally two ways in which a service can allocate resources to an instance:\nShared Services - Resources are allocated to more than one service instance; Each instance consumes resources simultaneously.\nDedicated Services - Resources are allocated to a single instance; These resources remain dedicated for the instance\u0026rsquo;s entire lifecycle.\nService Allocation Integration # Suppose you are going through the cloud migration journey. In that case, you may have some strategy that includes a combination of rehost, replatform, and refactor. At some point, you will probably be faced with integrating cloud PaaS services with IaaS and even back on-premises.\nBefore Service Endpoints # Azure SQL DB is a great way to migrate your SQL Server databases without changing your apps, making it extremely popular. Before Service Endpoints came along, how would a virtual machine communicate to Azure SQL?\nAzure SQL DB is available via public IP address and listens on port 1433 The virtual machine has direct connectivity to the internet via NAT\u0026rsquo;ed IP address The virtual machine leverages this for internet egress to reach Azure SQL service Azure SQL service sees the NAT\u0026rsquo;ed IP address, not the private IP of the virtual machine No restrictions can be enforced to limit service ingress to your private IP addresses only Why Is This Risky? # To filter traffic in this scenario, the virtual machine would need a public IP address. This presents the following complications:\nVirtual Machine now allows ingress directly from the internet (This is never a good practice) The subnet that hosts the virtual machine would require a NAT gateway; More configuration with potential performance implications Azure SQL would need to be open to clients on any network; In the event credentials are leaked, anyone on the internet could gain access Tradeoffs # Aside from being inflexible in controlling ingress and egress traffic, the route is not optimal. Traffic is technically going out to the internet, but this is probably, in reality, routing through Azure\u0026rsquo;s external backbone and coming back in. The service is reachable via public IP address only. Considering the design and various integrations required to make it work, there could be significant opportunities for data leakage or exfiltration.\nVNet Integration # VNet Integration is leveraged for services whose design matches the dedicated services architecture. Also called VNet Injection, this method deploys a given service directly in your VNet. As a solution, it enables better continuity for workloads operating in tiered hybrid design.\nVNet Integration Problems Solved By VNet Integration # Injected services are exposed over RFC1918 addresses directly from your VNet As a dedicated platform, none of the components are shared across other customers; This could be a requirement in some cases At the network layer, the behavior would be similar to that of a virtual machine in the same VNet Communication to virtual machines in same VNet and in peered VNets is bidirectional Communication between on-premises environments over ExpressRoute - Private Peering or VPNs is bidirectional Inbound connections from routable IP addresses outside of VNet address space is possible when exposed behind public IP address; Best to use an external load balancer / front-end IP address VNet Source NAT can be used to initiate connections to internet routable IP addresses VNet Integration Tradeoffs # Although VNet Integration was created to deploy App Services privately, is all of it really private and exclusive to only your VNet? While the service itself is dedicated, the control plane managing the services is not. Integrated services require inbound and outbound connections to platform managed public IP addresses, which facilitate interaction to the control plane. Whether this is a tradeoff is debatable.\nIntegrated services are deployed into an isolated subnet; Services contain sizing guidlines Each service identifies the required dependencies for control plane communication; For example, App Service Environment dependencies can be found here Increased complexity in managing NSGs and UDRs; Service Tags should be used to reduce overhead as the mapping between tag and addresses are automatically managed by the platform Ultimately, each service will exhibit different behavior and likely has different requirements; Always consult the official documentation Service Endpoints # Service Endpoints as a solution can be applied to select PaaS services with a shared architecture. A route is created from your PaaS service into the desired VNets.\nService Endpoints Problems Solved By Service Endpoints # After enabling a service endpoint in the subnet where the Application Gateway is deployed, source IP addresses switch from public to private IP addresses Traffic from a given virtual machine in a private subnet can now talk directly to Azure SQL without requiring internet egress; Without a public IP address, bad actors can\u0026rsquo;t scan a virtual machine\u0026rsquo;s open ports for vulnerabilities, thus limiting application downtime and data theft Service Endpoints ensure traffic egressing a given subnet is tagged with the subnet ID; Traffic inbound to Azure SQL can then be identified and blocked except for your desired subnet IDs (like the subnet which hosts the service endpoint) An NSG can then restrict all egress traffic from the subnet to the internet, thus locking down communication both ways Connecting to Azure Services from your VNet happens with an optimized route over Azure\u0026rsquo;s internal backbone network; This reduces network hops, which increases performance (In most cases) Setting Up A Service Endpoint # When provisioning a compatible service like Azure Keyvault, you will have an option to use Public Endpoint (Selected Networks) during the setup process. It will then walk you through enabling service endpoints on your desired virtual network and subnet.\nSetup DNS Behavior With Service Endpoints # DNS behavior remains as-is when leveraging service endpoints. DNS of a given resource will always resolve the resource\u0026rsquo;s public IP address regardless of where the traffic originates. This is because the service endpoint doesn\u0026rsquo;t change the network interface IP address of the resource it was added to. In doing a DNS lookup from an Azure virtual machine, we get a CNAME to cloudapp.net which gives us 20.185.217.251. This lookup is identical when being executed both from the virtual machine living in the VNet and on my local machine at home.\nDNS Query Service Endpoint Tradeoffs # Leveraging service endpoints is an excellent way to harden your VNet and service communication; As with everything in technology, it has tradeoffs to consider.\nConnection initiation with service endpoints is unidirectional; The initiator must be inside the subnet that holds the integration From a VNet\u0026rsquo;s vantage point, service endpoints provide access to the entire PaaS service; You cannot target specific instances of a PaaS service Once enabled on a subnet, clients in the subnet have network level access to all instances of that particular service (including those belonging to other users); To mitigate the risk of leakage or data exfiltration, Service Endpoint Policies should be used Service Endpoint Policies enable VNet owners to control precisely which instances of a service type can be accessed from their VNet\nThe source is now a private IP address, but the destination is still the service resource\u0026rsquo;s public IP address; Traffic is still leaving your virtual network (Whether this is a tradeoff is debatable) Service Endpoints override any BGP or UDR routes for the address prefix match of any Azure service; While this isn\u0026rsquo;t necessarily a problem, it introduces another traffic pattern that must be accounted for When a service endpoint is created, routes for all the public prefixes used by the service type get added to the subnet\u0026rsquo;s route table. The next-hop is set to a value of VirtualNetworkServiceEndpoint. All packets that match that next hop are encapsulated in outer packets, which carry information about the identity of their source virtual network.\nService Endpoints do not work across Azure AD Tenants; This is not generally a problem in smaller environments but may require a workaround in larger or non-standard environments Endpoints can\u0026rsquo;t be used for traffic originating from on-premises networks When service endpoints launched, the recommended solution for connecting from on-premises was to set up Microsoft Peering, which comes with additional complexity and considerations. This would involve identifying the Azure service resource side\u0026rsquo;s public IP addresses and allowing via resource IP firewall and the ExpressRoute / On-Premises firewall. Why not just use internet at this point?\nPrivate Link # To address additional customer demand surrounding integration and security, Private Link was born. It is the latest solution to integrate services with a shared architecture. It addresses limitations of service endpoints by exposing PaaS services via private IP addresses taken from a VNets RFC1918 space.\nPrivate Link Breaking Down The Components # Virtual Networks (VNets) are created in a subscription with private address space and provide network-level containment of resources with no traffic allowed by default between any two virtual networks. Any communication between virtual networks needs to be explicitly provisioned. Private Endpoint establishes a logical relationship between a public service instance and a NIC attached to the VNet where the service is exposed. Private Link Service enables you to access Azure PaaS Services over a private endpoint. Services supported today can be found here. Problems Solved By Private Link # Using a private endpoint enables flexible PaaS integration with your own private address space With no public IP addresses exposed, this means network-based access restrictions are not so complicated or not required at all; This simplifies network design as a whole Private Link enables on-premises clients to use existing ExpressRoute Private Peering or an existing VPN to consume PaaS services via private IP addresses A private endpoint is mapped to an instance of a PaaS resource instead of the full service meaning consumers can only connect to that specific resource; This provides additional built-in protection against data leakage Connectivity from a designated VNet to a partner (inside or outside) organization\u0026rsquo;s VNet is now possible; This creates new opportunities for B2B (Business-to-Business) which can leverage Azure native connections without the necessity for public endpoints Setting Up A Private Endpoint # Using the Azure Keyvault example again, you will have an option to use Private Endpoint during the setup process. It will then walk you through setting up all the necessary configuration for a private endpoint and optionally creating a Private DNS Zone. Following the defaults, it will also create the A Record for you.\nSetup DNS Behavior With Private Endpoints # One of the more significant differences between service endpoints and private endpoints is with DNS. In doing a DNS lookup from an Azure virtual machine, we directly resolve the private IP address 10.5.1.4. If we did this same lookup from a machine outside of our VNet or Azure, then we would get a CNAME to cloudapp.net which would then give us the public IP address of the service.\nDNS Query Private Link Tradeoffs # Connection initiation with Private Link is unidirectional; The initiator can be multiple VNets or on-premises networks As of today, NSGs are not supported on private endpoints; While subnets containing the private endpoint can have an NSG associated, the rules will not be effective on traffic being processed DNS integration is a major part of Private Link and brings additional complexity; Mapping design and configuration to outcomes requires significant planning Transitioning infrastructure from service endpoints to private endpoints will require additional planning and downtime Expanding On DNS Complexity # In the most simple of scenarios, an instance of a service that supports Private Link can be accessed concurrently through the service\u0026rsquo;s public IP address and private endpoint (across multiple VNets). During provisioning, each instance of a public service is assigned a unique and publicly resolvable FQDN. With a service operating in shared architecture, multiple instances run on the same set of resources sharing the same IP address. To handle resolution, CNAME records map the FQDNs of each instance back to the same address.\nPrivate Link, by design, requires DNS behavior to change based on origin - e.g., inside/outside and also based on the existence of a private endpoint for a given instance. Since, by default, the FQDN of the service resolves to a public IP, you would need to configure DNS accordingly to map to the private IP address allocated from your VNet. This is a topic that should be planned out prior to leveraging this service. One consistent truth I have learned working in technology is, Some of the most bizarre problems leading to colossal time waste turn out to be misconfigured DNS. A great way to get ahead of these challenges is to educate on how name resolution for resources in Azure works and also know your options for integrating with on-premises DNS.\nConclusion # Service Endpoints as a solution is easier to set up and manage, although integration with on-premises can be a challenge. Private Link is much more complex and will differ across environments. Deploying Private Link will require much more up-front discovery and planning around existing design, which also segues into infrastructure as code and possibly incorporating automation outside of Azure - (If leveraging existing DNS solution).\nBenjamin Franklin once said, \u0026ldquo;By failing to prepare, you are preparing to fail.\u0026rdquo; By taking that extra preparation, Private Link can offer much more granular control with how PaaS services are integrated into an environment. In the state of Cybersecurity post-pandemic, I predict that all the major cloud providers will prioritize the privatization of all PaaS endpoints.\nAcknowledgements # Big thanks to the wise and insightful Steven Hawkins for taking the time to peer review this post.\n","date":"11 January 2021","externalUrl":null,"permalink":"/posts/2021/cloud-security-and-azure-private-link/","section":"Posts","summary":"Azure Private Link enables access to hosted customer and partner services over a private endpoint in an Azure virtual network. This means private connectivity over your own RFC1918 address space to any supported PaaS service while limiting the need for additional gateways, NAT appliances, public IP addresses, or ExpressRoute (Microsoft Peering).\n","title":"Cloud Security And Azure Private Link","type":"posts"},{"content":"","date":"11 January 2021","externalUrl":null,"permalink":"/tags/dns/","section":"Tags","summary":"","title":"Dns","type":"tags"},{"content":"","date":"11 January 2021","externalUrl":null,"permalink":"/tags/private-link/","section":"Tags","summary":"","title":"Private-Link","type":"tags"},{"content":"","date":"13 October 2020","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"AnsibleFest AnsibleFest 2020, like most conferences this year, took place completely virtual. I presented on Automating IPAM In Cloud: Ansible + Netbox. You can find the slides along with the demonstration code in this git repo. In this post, I\u0026rsquo;m going to expand a little further on the content I presented.\nWhat is IPAM? # IP Address Management (IPAM) is the critical component that organizes your IP addresses and networks in one place. Responsible management of IP addressing drives efficient, repeatable, and reliable network automation. It is also a dependency for many other types of automation. Think of all the things that require IP addresses to communicate?\nIPAM The Problem # If you are hybrid multi-cloud, it probably means you share the same RFC1918 space across clouds and traditional networking. If this is the case, it means you probably want to keep track of allocation across VNets, Subnets, and VMs.\nBoundaries Are Blurred # As Hybrid Multi-Cloud becomes a reality, private IP space becomes shared across cloud(s) and on-premises Developers leverage CI/CD as they deploy, maintain, and migrate applications in the cloud; network can\u0026rsquo;t keep up Some developers do not have a good understanding of how IP addressing works and how to consume it responsibly Problem Inconsistency In Data Domains # Data required for efficient automation is often dispersed across many tools and platforms Frequently, overlap can exist between these tools and platforms for a given data domain Tools and platforms may be owned and managed by different teams with different directions Good automation is dependent on data accuracy, consistency, and ability to be consumed Data Domains What Is Our Desired Outcome? # Aside from the idea that we want to automate IP address management, the main focus is user experience. This encompasses the overall experience for those designing for the future, supporting current, and consuming cloud networking services. The following considerations are critical:\nMinimal human intervention Repeatable with other cloud providers Compatible across traditional + cloud networking End-to-end network automation Why Automate Here? # If Tesla can make self-driving cars and my coffee maker turns itself on in the morning, we can automate IPAM across the hybrid multi-cloud network, right? Also, the cloud demands agility. If you want agility, you need design patterns to build something repeatable against. If you manage IPAM in the cloud with any Microsoft Office tool, you are doing it wrong.\nWhy Ansible and Netbox? # Leveraging something like Ansible begins to make sense when the goal is automation at scale for networking across multiple vendors and environments. Netbox can serve as a Source of Truth intended to represent the desired state of a network versus its operational state. The API is very flexible, and the functionality can drive many automation use cases well beyond IPAM.\nPrerequisites # This post will not detail how to deploy Ansible or Netbox as there is plenty of examples out there already. There is excellent documentation for setting up both of these platforms via Docker. Instructions for setting up Ansible AWX can be found here and Netbox here.\nTo authenticate to an Azure subscription, you will need to create a Service Principal. Once a Service Principal is created, you will need to assign a role so that you can access the resources in that subscription. Detailed instructions for completing these steps can be found here.\nWhat Are We Automating? # Let\u0026rsquo;s examine the cloud environment so we can begin formulating our approach for how we want to automate.\nThe Technology # The Tech Digging Down Into The Environments # In the cloud, you generally have a hierarchy in which logical components exist, which may contain additional logical components. With Azure, for instance, management groups include subscriptions. Subscriptions hold Resource Groups, which include VNets. A VNet may be a shared resource across many apps and services in scope across many teams.\nCloud Environment Colocation Data Centers like Equinix or Megaport are being used more frequently as demand for cloud services increase. We will need to make some changes to a few physical Cisco ASRs, so packets can route.\nCloud Design Drives Automation # Thinking through a given design is a crucial element for how you approach the automation. If our goal is end-to-end network automation at scale, that means we must automate across multiple vendors and environments.\nStandard Workflows # To automate both the foundational (shared) and service-oriented (app-specific) components, leveraging two distinct workflows makes sense.\nStandard Workflows A Scalable Tagging System # If you want to do cloud right, cost governance should be in your considerations. This means a well-designed and consistently applied tagging convention, which compliments lifecycle management, automation, and visibility in reporting. In treating our pets like cattle, we need a source of truth with standardized identification across all networking components.\nResource Tagging Touchpoints # Between both of our workflows, there are numerous touchpoints:\nnetwork_cli for traditional networking Ansible Cloud Modules via Azure Resource Manager Netbox Collection leveraging the Netbox API Touchpoints Breaking Down The Logic # A great benefit of using Ansible is flexibility. This can also cause significant confusion as there are many ways in which we can structure things. It is beneficial to understand the structure, logic flow, and inheritance.\nProject Structure # Thinking through your project structure can make future work more manageable. The following guide - Best Practices - Content Organization in Ansible\u0026rsquo;s documentation is a good starting point. However, this is not a one size fits all scenario, and experimenting, reevaluating, and tweaking will probably be necessary. The following approach has served me well in the past.\nProject Structure Ansible Collections # Ansible Collections are pretty slick. My only recommendation here is, manage your collections with a requirements.yml file. When it comes to testing new versions of a collection, all you need do is create a new git branch, set the new version in your requirements.yml file, and point Tower to this branch. Ansible has documented collections usage pretty thoroughly here.\nansible-cloud-ipam/collections/requirements.yml\n1 2 3 4 5 6 7 8 --- collections: - name: netbox.netbox version: 1.0.0 source: https://galaxy.ansible.com ... I would stress never pulling down collections from Galaxy and committing them directly into the source control repository for your project. Friends do not let friends engage in this type of behavior!\nHigh-Level Plays # The playbooks sitting in our root project folder are pretty simple. They are primarily used as the entry point for Ansible and execute based on a specific condition.\nansible-cloud-ipam/play.azure_snet_add.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 --- - hosts: all connection: local hosts: localhost gather_facts: False vars: play_action: netbox_reserve_prefix_snet roles: - netbox ... Functional Roles # Roles for this demo are organized by either a given platform or OS type. Each role\u0026rsquo;s main file will include specific tasks to use based on the condition defined in the root level play.\nansible-cloud-ipam/roles/netbox/tasks/main.yml\n1 2 3 4 5 6 7 8 9 --- - include_tasks: task.netbox_reserve_prefix_vnet.yml when: play_action == \u0026#39;netbox_reserve_prefix_vnet\u0026#39; - include_tasks: task.netbox_reserve_prefix_snet.yml when: play_action == \u0026#39;netbox_reserve_prefix_snet\u0026#39; ... Purpose Built Tasks # The actual logic is handled inside a given role\u0026rsquo;s tasks. Each task is purpose-built and does a very intentional thing. Also, the idea is to keep things DRY (Don\u0026rsquo;t repeat yourself) so that we can easily reuse things. This means we don\u0026rsquo;t want to set values here statically. The smaller a thing is, the less it does. If something does less, it makes it easier to repeat.\nansible-cloud-ipam/roles/netbox/tasks/task.netbox_reserve_prefix_snet.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 --- - name: Add next prefix in Netbox netbox.netbox.netbox_prefix: netbox_url: \u0026#34;{{ netbox_url }}\u0026#34; netbox_token: \u0026#34;{{ netbox_token }}\u0026#34; data: # Define criteria for prefix parent: \u0026#34;{{ prefix_parent }}\u0026#34; prefix_length: \u0026#34;{{ prefix_length }}\u0026#34; # Define criteria for site tenant: \u0026#34;{{ tenant_name }}\u0026#34; site: \u0026#34;{{ azure_location }}\u0026#34; # Metadata prefix_role: \u0026#34;{{ prefix_role }}\u0026#34; description: \u0026#34;{{ prefix_desc }}\u0026#34; # Tags tags: - \u0026#34;{{ app_name }}\u0026#34; - \u0026#34;{{ app_tier }}\u0026#34; - \u0026#34;{{ app_env }}\u0026#34; - \u0026#34;{{ billing_id }}\u0026#34; - \u0026#34;{{ business_unit }}\u0026#34; - \u0026#34;{{ support_group }}\u0026#34; # State state: present first_available: yes # Register output register: first_available # Set stat for prefix string - name: Set stats set_stats: data: net_prefix: \u0026#34;{{ first_available.prefix.prefix }}\u0026#34; ... The Physical Network # Out of the three platforms we have interacted with here, the Cisco ASR is the only one that isn\u0026rsquo;t API driven. For automating this beauty, I\u0026rsquo;ll be using the ios_config module for configuration.\nThere are more modern ways to do this today, but this is probably the reality for most in practice. This is a straightforward use case, so this type of execution serves its purpose for a demo\u0026rsquo;s scope. Each time the Service Workflow is run, the subnet it reserves will be appended to a prefix-list living on the ASR.\nIn a real world cloud scenario, this prefix-list could be used to identify and filter traffic. It could then be used inside a route-map to enable and enforce policy criteria beyond the routing table. When doing BGP to the cloud, this can be important as you may want to enforce specific policies on specific neighbors to particular clouds.\nansible-cloud-ipam/roles/cisco-ios/tasks/task.ios_prefix_list_append.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 --- # Block - Append prefix-list on IOS - name: Append prefix-list on IOS block: # Create temp staging directory - name: Stage config directory file: path: \u0026#34;staging/{{ inventory_hostname }}\u0026#34; state: directory mode: \u0026#39;0777\u0026#39; # Stage configuration for IOS prefix-list - name: Stage configuration for IOS prefix-list template: src: prefix_list_append.j2 dest: \u0026#34;staging/{{ inventory_hostname }}/prefix_list_append.cfg\u0026#34; # Push configuration to IOS - name: Push configuration to IOS ios_config: src: \u0026#34;staging/{{ inventory_hostname }}/prefix_list_append.cfg\u0026#34; # Clean up temp staging directory - name: Clean staging directory file: path: \u0026#34;staging/{{ inventory_hostname }}\u0026#34; state: absent ... Putting It All Together # Ansible Tower offers two flavors of templates. Job Templates are used to execute a single task many times, while Workflow Templates stitch together multiple Job Templates.\nThe value here aims at getting the same outcome that your typical Build/Release pipelines would offer for Infrastructure as Code. When thinking of infrastructure as Code, I generally think immutability and cloud exclusivity.\nIn my opinion, this approach lends itself more to remaining flexible and accommodating for a multitude of disparate and traditional infrastructure while also providing the ability to integrate with new technologies.\nTemplates Job Templates # In the spirit of building for repeatability, I created Job Templates for each unique task. This allows me to reuse them over time in new Workflow Templates as new use cases present themselves.\nJob Template Workflow Templates # I then assembled my two Workflow Templates outlined above. The Base Workflow in practice would be run far less frequently than the Service Workflow.\nWorkflow Visualizer # Stitching your workflow together is pretty simple with Tower\u0026rsquo;s workflow visualizer. Once you create a blanket workflow, you can add and arrange jobs.\nWorkflow Visualizer Using Surveys # Surveys are a great way to populate variables at runtime interactively. Once a source of truth in a given data domain is populated, and the logic is more mature, this will probably be used less.\nSurveys Workflows In Action # Now that our workflow is populated with the right variables let\u0026rsquo;s take it for a spin!\nRunning Workflows A Little Manual Validation # Purely for the sake of delightful visuals, let\u0026rsquo;s validate a few things.\nNetbox Prefix Reservation: We can see here that the next available prefix was reserved, configuration criteria are correct, and our enforced tags are in place.\nNetbox Validation Cisco ASR 1002-HX Configuration: The prefix reserved above has now been added to the cloud-allowed-prefixes list on our router. If the list does not exist, the job will create it. From there, any new prefixes reserved will then be appended.\nASR Validation Conclusion # Building a logical way for developers to consume IPAM at the speed of cloud is WINNING! Remember, go fast, but go responsibly!\nGo Fast! ","date":"13 October 2020","externalUrl":null,"permalink":"/posts/2020/automating-cloud-ipam/","section":"Posts","summary":"AnsibleFest AnsibleFest 2020, like most conferences this year, took place completely virtual. I presented on Automating IPAM In Cloud: Ansible + Netbox. You can find the slides along with the demonstration code in this git repo. In this post, I’m going to expand a little further on the content I presented.\n","title":"AnsibleFest 2020 - Automating IPAM In Cloud","type":"posts"},{"content":"","date":"13 October 2020","externalUrl":null,"permalink":"","section":"Talks \u0026 Media","summary":"","title":"AnsibleFest 2020: Automating IPAM in Cloud","type":"talks"},{"content":" Introduction # Multi-Cloud is making its rounds. Network and Security engineers face increasing challenges with managing complexity and risk as they work to react with more agility to enable business outcomes. At the start, enterprises didn\u0026rsquo;t just decide they would be multi-cloud. They started with a single cloud, likely Amazon Web Services and tailored their strategy around that cloud\u0026rsquo;s architecture and features.\nA little time passes, and now those engineers that are still evolving to handle AWS are tasked with adopting Microsoft Azure. Those handcrafted design patterns, features, and core architectures in scope for AWS it turns out, don\u0026rsquo;t port over to Azure. At this point, those engineers are starting to get overwhelmed. Just as they finish getting the initial design in place for Azure, there is now a push to begin setting up the foundational structure and connectivity for Google Cloud.\nIn May, Alkira exited stealth with the first unified multi-cloud network delivered as-a-service which aims to solve problems like this. Alkira is brought to us by Amir Khan and Atif Khan who also created and co-founded Viptela SD-WAN (Viptela was acquired by Cisco in 2017 for 610 million). Listen to Packet Pushers Podcast - Heavy Networking 520 as I talk through the challenges of multi-cloud and how Alkira can help you solve them.\nIntro By The Numbers # Back when multi-cloud started to gather momentum, I admittedly thought it was a sham. Some of the biggest names in the cloud game seem to choose a single cloud, innovate, optimize, and scale. Might have heard of that streaming giant Netflix, no? They are using AWS exclusively. What about Spotify, who went from 20 million premium subscribers in 2015 to a staggering 130 million in 2020? Well, they are all in on Google Cloud.\nWhile efficiently managing 1 cloud may seem hard enough, it looks like enterprises and SMBs are taking a different approach. According to Flexera 2020 - State of the Cloud Report, 93 percent of enterprises have a multi-cloud strategy; 87 percent have a hybrid-cloud strategy. On average, organizations are actively using 3.4 clouds and experimenting with another 1.5 clouds (private and public). The numbers tell a story.\nHow Did We Become Multi-Cloud Anyway? # For some, it all started with the words Digital and Transformation. Cloud is the new foundation for our agile business, right? We proudly setup our VPN to AWS and thought, that was easy. I can cloud with the best of em! Then things kind of snowballed from there. While no organization has the exact same story to tell, we can look at a hypothetical example in which many can probably relate.\nMulti-Cloud Analyzing Multi-Cloud Network Design Patterns # The concept of design patterns was first introduced by Christopher Alexander and has profoundly influenced software engineering. Simply put, a design pattern is just a reusable solution to a commonly occurring problem. This applies to any discipline and is of critical importance when architecting for the cloud.\nThe network is not just limited to public cloud. In the instance of Healthcare, you may have existing data centers, corporate offices, hospitals, and a growing remote workforce which require fast, reliable, and secure transport. Examining the prominent building blocks of cloud computing - IaaS, PaaS, and SaaS can add color as to how the future network may look.\nCloud Computing Pre-Cloud Design - (Data Center Backhaul) # You can\u0026rsquo;t adopt the cloud without taking that first step. Most enterprises and SMBs took that step by merely setting up accounts in AWS and sandboxing. Since network and security folks generally slow things down, this was typically started without much oversight.\nPre-Cloud Design Pattern Evolution # Cloud has caused disruption across all technical disciplines, and networking is no exception. Although not a one-to-one match to the evolution of cloud computing models, we can definitely draw some comparisons.\nNetwork 1 - Design Pattern - (Colocation) # Once network performance for cloud becomes critical, adoption of Direct Connect, ExpressRoute, and Interconnect become essential for connecting your existing infrastructure. These links are critical for migrations and tiered hybrid deployments (Backend application reuse, with frontend components being migrated case by case).\nColocation data centers have many qualities that can be attractive to a business seeking to get well connected. One major value add would be, getting you closer to the cloud providers, applications, and services, which are also hosted in a respective CoLo. This solution isn\u0026rsquo;t really centralized or decentralized. I would say it fits somewhere in the middle, maybe call it regionalized?\nColocation 2 - Design Pattern - (Cloud Transits) # To centralize or decentralize, that is the question. This seems to be the proverbial pendulum swing in action. We got somewhere in the middle with CoLos, but now with SD-WAN making us transport agnostic, why not wholly decentralize? Every cloud, data center, and remote for itself, easy right?\nCloud Transits 3 - Design Pattern - (As a Service) # Each of the previous patterns solves specific problems but also leaves new ones in its wake. Alkira aims to tackle this in a disruptive fashion similar to how the cloud computing models transformed application delivery. By shifting to a service based consumption, you gain a lot of the benefits of cloud applied to the full domain of networking.\nAlthough their solution is marketed as-a-service with consumption-based billing, I will attempt to expand on this notion. In the world of cloud, as-a-service implies that you don\u0026rsquo;t technically do anything but consume. I would argue that Alkira is more like a SaaS solution that enables you to operate components of your network in a PaaS like fashion.\nAs-a-Service Examining Complexity In Multi-Cloud Today # Complexity can be a tricky thing. Often times, adding complexity to a network to either increase availability or make it more secure can backfire. If the person responsible for a given design has to think about it for a length of time when something breaks or has difficulty explaining it, then it may be overly complicated. This means it will be far more difficult for operational teams to manage over time.\nThe CoLo Approach # In the journey to get well connected, many organizations are leveraging global colocation data centers like Equinix or Megaport for interconnection services. In many reference architectures, this is referred to as performance hubs which are distributed in proximity to your data centers, remotes, customers, and physical location of public cloud data centers.\nKey Drivers\nRegional private WAN egress to the Internet Low latency private connections to Cloud Providers Localized security stack per CoLo (no backhaul to data centers) Leverage competition between carriers (cost consideration) CoLos have a lot to offer in terms of value, but they also significantly increase complexity and operational overhead. A lot of times when CoLos are discussed, I often hear some variation of the question - \u0026ldquo;Aren\u0026rsquo;t we going to cloud so we can get out of the business of managing our own data centers\u0026rdquo;? The idea is to avoid additional spend on hardware, costly refreshes, and additional staffing to manage it all.\nPhysical Hardware Leads The Way # In building out CoLos, physical hardware is used to terminate connections, route traffic, and perform security functions. While services exist like Equinix Cloud Exchange Fabric, this merely enables you to provision multiple logical circuits on-top of a single physical port. You are still on the hook for terminating bare-metal connections and routing traffic.\nLayer 2 Across Clouds # As we start setting up our fast on-ramps to cloud, we must first setup the physical hardware to terminate each cloud\u0026rsquo;s variation of direct connection. For increased SLA and reliability, these are sold in pairs, so our physical hardware must be setup in an Active/Standby configuration.\nWe will have a cross connect from Switch-A going to DirectConnect-A and a cross connect additionally from Switch-B to DirectConnect-B. This will be required for each Cloud provider.\nCoLo - Layer: 2 Layer 3 Across Clouds # Now that we\u0026rsquo;re switching those frames, it\u0026rsquo;s time to route some packets. Aside from some nice enterprise-grade switches, we are probably going to need some nice shiny routers to see our reflection in. Although we could have simply terminated connections on the same device we are routing with, as we expand the hardware topology to NGFWs, the separate switches make sense.\nCoLo - Layer: 3 Layer 3, 4, and 7 Across Clouds # The Cisco Annual Internet Report makes the staggering prediction that by 2021, 85% of network traffic will be east-west. This creates a compelling case for putting time and effort into understanding what, how, and why specific traffic is communicating inside and across clouds.\nIf I am an attacker, I want sensitive information. When organizations are migrating to the cloud, the raw data may not exist in the cloud, but network connectivity is required to the cloud to plan and orchestrate migrations.\nThis opens the door for cross-cloud attacks in which a bad actor might piggyback on address-based controls and protocols that are missed by non-modernized toolkits or lack of visibility. The bad actor can then propagate malware across clouds, or to on-premises data centers.\nRegionally placing security gear in CoLos can help alleviate some of these concerns as you can force all of your traffic through Next-Gen appliances. When you start going multi-cloud however, your traffic flows, routing, and segmentation complexity begin to increase exponentially.\nCoLo - Layer: 3-7 Segmentation # Cloud Security should be similar to a very large, plain-looking, yet complex onion. This onion has several layers, and each layer you peel back has a unique but intentional function. In network security, macro-segmentation is a higher-level and less granular method in which we can group categories of things, traditionally geared towards north-south communication.\nAs we get more granular, we leverage micro-segmentation. This expands the looking glass to encompass east-west traffic with the intent of providing seamless protection for workloads spanning data centers and multiple clouds.\nIn getting started, we intend to keep this as simple as possible while still meeting security-focused requirements. We can use the tried and true (but also traditional) building blocks of networking to meet some of our macro-segmentation requirements. This works well since; technically, we are just using these performance hubs as lightning-fast vehicles to cloud.\nComplexity Segmentation Overload # As complexity increases and requirements change, the problem statement changes as well. What if a specific workload needs to talk outside of an AWS VRF across cloud to a VRF that drives connectivity for a specific Azure VNet? Well, since these VRFs were scoped out on a per-cloud basis, we need to incorporate more segmentation to keep this traffic separated at a workload level.\nSo, I can define traffic in prefix-lists. I can then use those prefix-lists to dictate policy in route-maps from which I can apply to cloud-specific VRFs. Then, the original way we might have segmented things five years ago changes dramatically because BANG - multi-cloud. Now I am leaking routes from VRFs, routing between firewall contexts, and my own snowflakes are now turning into a full-on blizzard.\nAlso, how do we define what a Macro Segment is? How do we determine what a Micro Segment is? Who decides how granular we should get on the network, and why doesn\u0026rsquo;t this complexity live up in the cloud anyways?\nIt seems to me; segmentation should have a very unique and intentional strategy across all environments. When developers are driving the way infrastructure looks and behaves in the cloud, how can we keep end-to-end control of the network while still helping forward progress and enabling the business?\nThere are a lot of products hitting the market today to help solve these problems. To get a look at how other vendors are tackling this, checkout NSX Cloud from VMware, Cisco - Secure Agile Exchange (SAE), and Aviatrix - Cloud Network Platform. Some of these solutions may require you to install a given vendor\u0026rsquo;s appliances in the cloud provider VPCs / VNets or setup additional physical hardware on-premises or in CoLos.\nAlkira - CSX # Alkira has coined their platform Cloud Services Exchange (CSX). The sales pitch: Provision a global, fully routed multi-cloud network with integrated network services, visibility, and governance in minutes. All delivered as-a-service. Inside CSX, their unified platform enables the customer to manage products and services. As new features are released over time, they will become available via the CSX for consumption.\nAlkira Cloud Exchange Points (CXPs) # If CSX is the unified platform, then Cloud Exchange Points (CXPs) are the interconnected brain. You can think of CXPs as globally distributed multi-cloud points of presence, which provide a full routing stack and network services. They provide elastic and highly available regional entry points for all the things you want to connect.\nCXPs Tackling Segmentation With Alkira # Alkira takes a unique approach to network segmentation. To bridge the gap for segmentation inside the cloud and out, they pull in VPC and VNet policy from a given cloud provider into their platform. Not having to install and manage additional appliances in each cloud provider is a major plus. Creating segments is painless. In addition to segments, you can define logical groups of things in which you build policy against.\nThe sweet spot here is, all of those segments, groups, and policies can be used across all of your clouds, remotes, data centers, or any other network you choose to connect through their platform. The experience with creating and applying these policies is unlike any other. You click and drag your pieces into place as you build your intended design.\nWhen doing this, my mind automatically began thinking, \u0026ldquo;Wow, this click and drag was a few days worth of planning and change windows\u0026rdquo;. Once you get all of your desired policy in place, no changes are executed until you click their Provision button.\nSegments Services Marketplace # What good is a cloud offering without the ability to inject 3rd party services? The three giants AWS, Azure, and GCP all have their own flavor of a marketplace. I would assume since Alkira is primarily focused on reinventing the network, that their focus on a marketplace would be based in network based appliances and services. As of today, the marketplace is limited to adding firewalls from Palo Alto.\nPalo Alto VM-Series Firewalls # Just like segments and groups, a Palo Alto Firewall can be setup and used across all of your clouds and sites. These can be deployed to your existing instance of Panorama, and configured to auto-scale as load increases. This really beats managing individual scale-sets in each cloud provider.\nFirewalls The Server Admin Evolves # In the pre-cloud era, the server admin stayed pretty busy. Not just with solving problems directly tied to the business, but with mundane tasks that held up other teams from executing work.\nServer Admin Pre-Cloud # Racking a server isn\u0026rsquo;t as easy as just racking a server. This may sound like a quick and trivial task, but gets complicated relatively quickly depending on an environment. Some of my longest nights have been spent either installing, upgrading, or troubleshooting hardware inside data centers.\nMaybe it is a pod-based solution? Is compute, network, virtualization, and storage all baked into the same rack? Now we are going beyond the simple racking of a server to understanding some of the upstream networking components in order to ensure proper failover in outage scenarios.\nOnce the Layer 1 stuff is done, we have to install and configure a hypervisor. Setting up Out-of-Band Management is also a must (more networking stuff, I know). If something breaks, you have to have an alternate method of troubleshooting beyond driving into the data center and rolling over the crash-cart.\nNot to mention - in smaller to medium-sized shops, these same individuals are probably solely responsible for OS lifecycle management too. Let\u0026rsquo;s not forget installing, patching, and troubleshooting VMs? All of this overhead, and we haven\u0026rsquo;t even got to the application or load balancer yet.\nAdmin Pre-Cloud Server Admin Post-Cloud # In the 1930s, an industrial engineer - Allen F. Morgenstern coined the phrase Work smarter, not harder. This phrase is the first thing that comes to my mind when thinking of today\u0026rsquo;s Server Admin. Once you remove a lot of the busy work, the cycles are then focused on design, efficiency, automation, and outcomes.\nHypothetical Server Build - Today # Changes are committed to source control Azure DevOps builds and packages application Release artifact is produced with associated trigger Trigger envokes Packer to build a Linux Image Ansible installs middleware + application Infrastructure is provisioned with Terraform Admin Post-Cloud What About Networking? # With the technology, automation, and abstraction available to us today, can\u0026rsquo;t we solve some of the painful network problems like cloud did for compute? Depending on the industry, institutional knowledge, and available talent, it behooves oneself to make a design only as complicated as requirements mandate. When inserting state into a network - (think NAT or stateful firewalls), enforcing traffic symmetry at that point in the network is critical. In multi-cloud, this usually means complicated hop-by-hop configuration + Policy Based Routing (PBR) to steer traffic.\nIn 2020, in the era of Cloud, it doesn\u0026rsquo;t feel like this is where network engineering cycles should be placed. Network engineers should be more focused and intune with intent and driving outcomes for the business. I think this is where Alkira aims to take networking. Abstracting some of the larger pain points away while still enabling the network team\u0026rsquo;s ability to leverage all the nerd knobs (maintaining control and visibility).\nFor instance, say the business requires AWS-Prod communication to GCP-Prod. Or maybe Product-A in Azure needs limited communication with Product-B in GCP. This type of policy is easily definable in Alkira. Your intent is described, and their platform handles the rest. No complicated policy based routing, traffic steering through a NGFW, or HA + autoscale per cloud.\nConclusion # Alkira provides a flexible, adaptable, innovative, and fresh spin on how the network can be used as a utility. A solution which leverages additional abstractions that ultimately lead to a reduction in time-to-value while accelerating speed-to-market.\nThe landscape is changing faster than ever before. The need for hardware is rapidly decreasing, and it no longer makes sense to back-haul anything back to data centers. User experience is rightfully taking center stage.\nHaving the ability to deliver unified network and security services, remain location agnostic, and provide low-latency from the cloud edge is a bare minimum for the future. Getting this right has a lot of challenges.\nAny solution that doesn\u0026rsquo;t integrate seamlessly into public cloud, or where application delivery happens, will further impede network and security professionals with design and operational setbacks. Alkira seems to have found the sweet spot to achieve outcomes in this space\n","date":"19 August 2020","externalUrl":null,"permalink":"/posts/2020/alkira-multicloud-networking/","section":"Posts","summary":"Introduction # Multi-Cloud is making its rounds. Network and Security engineers face increasing challenges with managing complexity and risk as they work to react with more agility to enable business outcomes. At the start, enterprises didn’t just decide they would be multi-cloud. They started with a single cloud, likely Amazon Web Services and tailored their strategy around that cloud’s architecture and features.\n","title":"Multi-Cloud Networking With Alkira","type":"posts"},{"content":"A lot of the work I do professionally involves transforming traditional network culture, practices, and technology. Just as DevOps transformed application delivery, NetDevOps is here to make sure the network can keep up. This post will cover how I deploy this blog with Hugo to GitHub Pages using GitHub Actions to completely automate the workflow. This is very similar to how I approached modernizing network documentation for my current employer. Listen to Day Two Cloud Podcast as I talk through this transformation in more detail.\nWhy Docs-as-Code? # Docs-as-Code refers to the philosophy that you should be writing documentation with the same tools as code. It is also an easy and non-intrusive way to get traditional engineers started using tools like git which have practical use cases far beyond documentation. Some of the benefits include:\nVersion Control (Git) Incorporating Automation (CICD) Ensure Docs evolve with code (Linked work items) Adopt DevOps practices in network engineering (Priceless) Static Site Generation # Static sites are making a comeback! In today\u0026rsquo;s world, speed and security are kind of a big deal. The more complex something is, the more difficult it is to secure. Static site generators are becoming popular as they bridge the gap between static sites of old, and dynamic sites of today. Benefits of static webpages include:\nCost (Fast delivery, low up front cost) Simplicity (Hosting, scaling, and automation) Speed (No backend database calls or dynamic content generation) Security (Less exposure to the internet) A request can only be made for files that contain HTML, CSS, JavaScript, Images, Audio, or Video. Since there is no database to breach and no server-side platform or CMS, the threat vector doesn\u0026rsquo;t exist to maliciously control an application or exploit a database.\nHugo - Static Site Generator # About GitHub Hugo is written in GoLang and claims to be the world\u0026rsquo;s fastest framework for building websites. At \u0026lt;1 ms per page, the average site builds in less than a second. It doesn\u0026rsquo;t take many cycles to get acclimated with writing in Markdown, and Hugo\u0026rsquo;s Shortcodes help you extend functionality to new heights.\nGitHub - Source + Build + Release # About GitHub Since everyone knows what GitHub is and are probably already experts at using git, this doesn\u0026rsquo;t require much explanation. If you are not an expert, then have a look here to educate yourself. You may not know as much about GitHub Pages and GitHub Actions.\nGitHub Pages # GitHub Pages allows you to host a webpage directly from your GitHub repository. This is a great way to showcase your blog, portfolio, project, documentation, or anything else you are interested in sharing with the world. With no databases to setup and no servers to build, you can focus more on content and less on management.\nAll your host provider has to do is serve static assets. Since they don\u0026rsquo;t need to support a specific programming language or framework, this makes it simple to host, manage, and even migrate to another hosting provider. Professionally, I am using Pivotal Cloud Foundry on Azure to host a NetDevOps DocSite.\nGitHub Actions # About Actions GitHub Actions enable you to automate your workflow. I have been using Azure Pipelines quite a bit recently for continuous integration and continuous delivery (CI/CD) . With the exception of a few things, GitHub Actions worked in a very similar way and is a breeze to setup. I would imagine GitHub Actions will be leveraged largely by the open source community as GitHub is geared for open source projects.\nPutting it Together # There are a few different ways you can deploy a static site using Hugo on GitHub Pages. I like the idea of keeping my source code in a dedicated repository and generating the contents of the site to an external repository. For me, this seems much cleaner than building the content inside the same repository and lends itself more to to a logical branching strategy.\nSetup Repositories # User/Org or Project repo setup - Instructions here Hugo source repo setup (This repo can be private, and the naming convention doesn\u0026rsquo;t matter) Hugo site committed to source repo (To develop or other integration branch) Generate SSH Keys # To enable the ability to push from source to destination repo, we need to generate SSH keys.\n1 2 3 # Private key added as a secret in Hugo source repo # Public key added as deploy key in GitHub Pages repo ssh-keygen -t rsa -C \u0026#34;hugo_deploy\u0026#34; -b 4096 -f ~/.ssh/hugo_deploy Add Source Repo - Secret # Navigate to settings on the source repo and create a new secret with the name ACTIONS_DEPLOY_KEY. Use your private key contents as the Value for the secret.\nSource Repo Add Destination Repo - Deploy Key # On the GitHub Pages repo, we need to create a Deploy Key. The naming convention doesn\u0026rsquo;t matter, but should be descriptive like - Deploy Key Pub. Use the public key contents as the Key.\nDestination Repo Workflow Overview # Since I am the only one contributing to this blog, the branching is pretty simple. I create a branch for a given post and merge back with develop (integration) branch. Once I am ready to release a new post, I create a pull request and merge back with master. The workflow looks something like this:\nBlog Workflow Workflow File # Add .github/workflows/deploy_hugo_site.yml to the Hugo source repo. Substitute username/destination-repo with the appropriate GitHub Pages / Destination repo.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # .github/workflows/deploy_hugo_site.yml name: Deploy Hugo Site on: push: branches: - master jobs: deploy: # build agent runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v2 # install / setup hugo # https://github.com/peaceiris/actions-hugo - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.68.3\u0026#39; # build site - name: Build run: hugo --minify # deploy to destination repo # https://github.com/peaceiris/actions-gh-pages - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} external_repository: username/destination-repo publish_branch: master publish_dir: ./public Validation # Once the code is committed and the workflow gets triggered, you can track the progress under Actions in the hugo source repo. The first time a site is deployed to GitHub Pages, you will have to wait a few minutes for it to be reachable. Subsequent releases should update within a few seconds.\nConclusion # If you want to check out some popular alternatives to Hugo, check out Jekyll and Gatsby. In integrating these types of tools into a technical workflow, you are not force-fitting writer-centric tools onto engineers but rather fitting the documentation into developer-centric tooling.\n","date":"21 June 2020","externalUrl":null,"permalink":"/posts/2020/automate-blog-deployment/","section":"Posts","summary":"A lot of the work I do professionally involves transforming traditional network culture, practices, and technology. Just as DevOps transformed application delivery, NetDevOps is here to make sure the network can keep up. This post will cover how I deploy this blog with Hugo to GitHub Pages using GitHub Actions to completely automate the workflow. This is very similar to how I approached modernizing network documentation for my current employer. Listen to Day Two Cloud Podcast as I talk through this transformation in more detail.\n","title":"Automating Blog Releases: Hugo + GitHub Actions","type":"posts"},{"content":"","date":"21 June 2020","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":" ","date":"2 June 2020","externalUrl":null,"permalink":"/talks/2020/heavy-networking-520/","section":"Talks \u0026 Media","summary":" ","title":"PacketPushers Heavy Networking: 520 - Multi-Cloud Network Adoption with Alkira","type":"talks"},{"content":"","date":"28 May 2020","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"28 May 2020","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":" Ansible Tower VS AWX # The best way to learn is by constructing your own knowledge, not passively absorbing information. A lot of great projects are open source these days, including The AWX Project. At zero cost, you can build your automation skill set, increasing your chances at landing that next big promotion.\nThis post will cover how to setup Ansible AWX on Debian. I use a combination of Ansible and Terraform in the NetDevOps toolchain. These tools play a huge role in keeping my knowledge relevant as Tech progresses.\nTower VS AWX What is AWX? # So, maybe you have heard of this Linux Distro called Fedora. This is the fast moving upstream project for Red Hat Enterprise Linux (RHEL). Red Hat also funds CentOS which is essentially a clone of RHEL minus all Red Hat branding. This is kind of how AWX works upstream for the official Ansible Tower product.\nRed Hat also announced CentOS Stream which sits between bleeding edge Fedora, and rock solid RHEL. Sitting somewhere in the middle, this would allow developers an easier path for getting their packages in RHEL.\nHow does it differ from Ansible Tower? # The AWX Project\nFast iterating with frequent releases No paid enterprise support available Direct in-place upgrades are not supported Not recommended for use in Production Licensed under Apache 2.0 AWX is a fantastic tool for the lab, testing, and demos. If you want to dip a toe into the Ansible Tower world without forking over the cash, look no further.\nAnsible Tower\nCommercial derivative of AWX Longer release cycle aimed at long-term supportability Fully supported and license based solution In-place upgrade to latest version (from up to two major releases behind) Recommended for use in Production Ansible Tower is the reliable solution you want for the enterprise. Deployment may differ since you will likely make use of scale-out clustering for added redundancy and capacity.\nDeployment Options # AWX runs as a containerized application using Docker images deployed to either OpenShift, Kubernetes, or Docker Engine.\nStandalone Deployment with Debian + Docker # In the spirit of keeping this simple, I use Docker running on Debian Linux in the lab. I also run everything on virtual machines as snapshots are a beautiful thing. Any hypervisor should work - VMware Workstation, Oracle VirtualBox, ESXi, or even KVM. For this setup, I\u0026rsquo;ll be using VMware Workstation.\nAWX Design Why Debian? # My love for Linux started back in the 90s with Slackware. At some point or another, I tested out Debian and my wish for the perfect operating system was granted. For me it strikes the right balance between ease of use, stability, and airy minimalism. Using the Debian netinstall, you only install what you need. I have AWX running smoothly with:\nProcessor: 1 Memory: 2 GB Hard Disk: 12 GB Prerequisites # The official prerequisites for AWX can be found here. I\u0026rsquo;ll be using Debian 10 - Buster however any release \u0026gt;= 10 will work. At this point, Debian should be installed, running, and reachable.\nIf running in a virtual machine, a snapshot should be taken now. In the event anything goes wrong, this is an easy way to revert back to our beginning state.\nGetting Started # Disclaimer: Downloading and executing scripts from the internet may potentially harm your machine. Make sure to review the source code before execution so that you have a good understanding of what is going to happen.\nThe Installer Script # The installer is pretty basic. It will make sure we are up to date, install some dependencies, clone the projects, set a few parameters, and start the install.\nawx_install.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 #! /usr/bin/env bash set -o pipefail function update_debian() { # +----------------+ # | Update Debian | # +----------------+ cmd_list=\u0026#34;update upgrade dist-upgrade autoremove clean\u0026#34; # Run! for cmd in $cmd_list do sudo apt-get -y $cmd done } function install_depend() { # +-----------------------+ # | Install Dependencies | # +-----------------------+ pkg_list=\u0026#34;git python3-pip docker docker-compose net-tools dnsutils vim\u0026#34; for pkg in $pkg_list do sudo apt-get install -y $pkg done # upgrade pip3 sudo pip3 install --upgrade pip # install ansible sudo pip3 install ansible # allow docker to run as non-root user sudo usermod -aG docker $awx_usr } function install_awx() { # +--------------+ # | Install AWX | # +--------------+ # create docker dir mkdir -p ~/docker # clone awx + branding cd ~/docker \\ \u0026amp;\u0026amp; git clone https://github.com/ansible/awx \\ \u0026amp;\u0026amp; git clone https://github.com/ansible/awx-logos # backup default inv and set params cd ./awx/installer \\ \u0026amp;\u0026amp; cp inventory default_inventory \\ \u0026amp;\u0026amp; sed -i \u0026#34;s|# awx_official=false|awx_official=true|g\u0026#34; inventory \\ \u0026amp;\u0026amp; sed -i \u0026#34;s|create_preload_data=True|create_preload_data=False|g\u0026#34; inventory \\ \u0026amp;\u0026amp; sed -i \u0026#34;s|#project_data_dir=/var/lib/awx/projects|project_data_dir=~/.awx/projects|g\u0026#34; inventory # start install sudo ansible-playbook install.yml -i inventory } function main() { awx_usr=$USER update_debian install_depend $awx_usr install_awx } main \u0026#34;$@\u0026#34; Running The Installer # 1 2 3 4 5 6 7 8 # Go home! cd ~/ # Install cURL sudo apt-get install -y curl # Run the installer curl -sSL https://gist.githubusercontent.com/wcollins/caca9aa1c416d05379ab3638804e3dd6/raw/479e5e8d6d0f5a93db9f3ee0a6cd46d8ef310e6e/awx_install.sh | bash Depending on host machine horsepower, this may take a few minutes. You should see progress as the installer deploys the containers.\nStart Containers Validation + Login # If using VMware Workstation with network adapter type: NAT, you can see what IP Address it gave you with /sbin/ifconfig. You should then be able to reach this IP from your host machine.\nOn the permanent instance I have up and running in the lab, I\u0026rsquo;m using GitHub Actions to build and release after each AWX software release. I use Linux KVM as a Type-1 hypervisor with a static IP of my choice assigned.\nLaunch AWX We should now be able to login with default credentials for AWX. There are many best practice considerations we would work through for handling authentication in a production environment but for the lab, we just want to roll up our sleeves and dive in, right?\nAWX Login Conclusion # Don\u0026rsquo;t wait to get started with automation. If you wait, you may miss out on a great opportunity. With so much open source software available, there is no upfront cost other than a laptop powerful enough to run a few virtual machines.\nIn the land of network engineering, getting your feet wet with public cloud and automation can help you make that next big step in your career. There is currently a high-demand with low-supply for these skills in the network engineering space.\n","date":"28 May 2020","externalUrl":null,"permalink":"/posts/2020/setup-awx-on-debian/","section":"Posts","summary":"Ansible Tower VS AWX # The best way to learn is by constructing your own knowledge, not passively absorbing information. A lot of great projects are open source these days, including The AWX Project. At zero cost, you can build your automation skill set, increasing your chances at landing that next big promotion.\n","title":"Setting Up Ansible (AWX) On Debian","type":"posts"},{"content":" ","date":"5 February 2020","externalUrl":null,"permalink":"/talks/2021/day-two-cloud-034/","section":"Talks \u0026 Media","summary":" ","title":"PacketPushers Day Two Cloud: 034 - Everything-as-Code","type":"talks"},{"content":" ","date":"13 December 2019","externalUrl":null,"permalink":"/talks/2019/heavy-networking-494/","section":"Talks \u0026 Media","summary":" ","title":"PacketPushers Heavy Networking: 494 - Hybrid Cloud Networking","type":"talks"},{"content":" William Collins is a strategic thinker and catalyst for innovation. Over his career, he has helped enterprises build large-scale networks, driven modernization through cloud adoption, and excels at optimizing complex environments through good design practices and automation.\nWhat I Do # Today, William works as Director of Technical Evangelism for Itential, where he focuses on evangelizing the Itential Platform, fostering strong relationships with customers to fully realize their goals, engaging with community, and advocating for the successful future of network, security, and automation infrastructure.\nAs a content creator, William hosts The Cloud Gambit Podcast with Eyvonne Sharp, a show that unravels the state of cloud computing, markets, strategy, and emerging trends with industry experts. He is also a LinkedIn Learning Instructor (Automation, Cloud, and Network Engineering Content), AWS Community Builder (Network \u0026amp; Content Delivery), and a group organizer for the USNUA - Kentucky User Group (KYNUG).\nBackground # Prior to Itential, William worked as a Principal Cloud Architect and Director of Technical Evangelism for Alkira where he helped grow the company from lean beginnings to being ranked 25th Fastest-Growing Company in North America and 6th in the Bay Area on the 2024 Deloitte Technology Fast 500. He also held various senior technical roles across the enterprise space in Financial Services and Healthcare, most recently at Humana as Director of Cloud Architecture.\nBeyond Tech # Outside of tech, his time is spent with family, woodworking, ice hockey, and guitar.\nOpinions expressed here are solely his own and do not express the views or opinions of his employer.\n","externalUrl":null,"permalink":"/about/","section":"William Collins","summary":" William Collins is a strategic thinker and catalyst for innovation. Over his career, he has helped enterprises build large-scale networks, driven modernization through cloud adoption, and excels at optimizing complex environments through good design practices and automation.\n","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/go/","section":"Tags","summary":"","title":"Go","type":"tags"},{"content":"","externalUrl":null,"permalink":"/projects/gridctl/","section":"Projects","summary":"","title":"gridctl","type":"projects"},{"content":" Connect # LinkedIn GitHub X (Twitter) YouTube TikTok Instagram Content # The Cloud Gambit Podcast LinkedIn Learning RSS Feed Open Source # gridctl ","externalUrl":null,"permalink":"/links/","section":"William Collins","summary":" Connect # LinkedIn GitHub X (Twitter) YouTube TikTok Instagram Content # The Cloud Gambit Podcast LinkedIn Learning RSS Feed Open Source # gridctl ","title":"Links","type":"page"},{"content":"","externalUrl":null,"permalink":"/tags/mcp/","section":"Tags","summary":"","title":"Mcp","type":"tags"},{"content":"","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"}]